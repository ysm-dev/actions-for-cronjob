<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[tech.ssut]]></title><description><![CDATA[Node, Go, and Python enthusiast.]]></description><link>https://tech.ssut.me/</link><image><url>https://tech.ssut.me/favicon.png</url><title>tech.ssut</title><link>https://tech.ssut.me/</link></image><generator>Ghost 2.15</generator><lastBuildDate>Sun, 12 May 2019 09:42:54 GMT</lastBuildDate><atom:link href="https://tech.ssut.me/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[NodeJS에서 async_hooks을 이용해 요청의 고유한 context 사용하기]]></title><description><![CDATA[<blockquote>이 글은 <a href="https://twitter.com/geekuillaume">Guillaume Bession</a>의 개인 블로그에 올라와있는 <a href="https://blog.besson.co/nodejs_async_hooks_to_get_per_request_context/">Getting per-request context in NodeJS with async_hooks</a> 글의 번역글입니다.</blockquote><p>저는 최근 NodeJS로 HTTP 서버를 개발하던 도중 문제를 맞닥트리게 되었습니다. 저는 제 코드베이스의 많은 부분에서 로깅을 하고 있었고, 각 요청별로 유니크ID를 가졌습니다. 저는 이 유니크한 ID를 각 요청 안에서 무슨 일이 일어나고</p>]]></description><link>https://tech.ssut.me/getting-per-request-context-in-nodejs-with-async_hooks/</link><guid isPermaLink="false">5c8129496bd2b46d8ab48fe3</guid><category><![CDATA[nodejs]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Mon, 25 Mar 2019 00:44:30 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2019/03/1-WvQ_cZPu8DaHTx2StbpPTA.jpeg" medium="image"/><content:encoded><![CDATA[<blockquote>이 글은 <a href="https://twitter.com/geekuillaume">Guillaume Bession</a>의 개인 블로그에 올라와있는 <a href="https://blog.besson.co/nodejs_async_hooks_to_get_per_request_context/">Getting per-request context in NodeJS with async_hooks</a> 글의 번역글입니다.</blockquote><img src="https://tech.ssut.me/content/images/2019/03/1-WvQ_cZPu8DaHTx2StbpPTA.jpeg" alt="NodeJS에서 async_hooks을 이용해 요청의 고유한 context 사용하기"><p>저는 최근 NodeJS로 HTTP 서버를 개발하던 도중 문제를 맞닥트리게 되었습니다. 저는 제 코드베이스의 많은 부분에서 로깅을 하고 있었고, 각 요청별로 유니크ID를 가졌습니다. 저는 이 유니크한 ID를 각 요청 안에서 무슨 일이 일어나고 있는지 추적하기 위해 로그 메시지에 붙이고 싶었습니다. 이걸 어떻게 하면 효율적으로 할 수 있을까요?</p><p>자아 간단한 방법은 모든 함수에 이 요청 ID를 넘겨 처리하는 방법입니다. 하지만 이런 방식으로 문제를 해결하면 코드가 유지보수하기 쉽지 않다는 문제가 발생합니다: 만약 제가 함수 안에서 함수를 호출하는 식으로 5개 스택의 함수를 호출하게 되면, 저는 여기에 해당하는 모든 함수 인자로 이 ID를 받는 부분을 추가해야만 합니다.</p><p>"context" 객체를 모든 함수에 넘겨 처리하는 방법도 있겠지만 여전히 문제는 존재합니다. 오래 걸리는 쿼리가 감지되었을 때 함수를 실행할 수 있게 해주는 SQL 라이브러리를 사용하는데 이 함수는 오직 query string만 받아 사용할 수 있습니다. 제가 원하는 요청 context는 넘길 수 없죠.</p><p>만약 당신이 실험적인 NodeJS 기능을 사용하는 데 충분한 준비가 되었다면, 저는 이 글에서 <a href="https://nodejs.org/api/async_hooks.html">async_hooks</a>라는 매우 훌륭한 해결책을 소개하고 싶습니다.</p><h2 id="async_hooks-">async_hooks에 관한 몇가지 이론</h2><p><code>async_hooks</code> 모듈은 비동기 리소스를 추적할 수 있는 함수를 제공합니다. 이 훅은 <code>setTimeout</code>, 서버 리스너 또는 다른 비동기 작업이 만들어지거나 시작되거나 끝나거나 제거될 때 호출됩니다.</p><p>비동기 리소스가 생성될 때, 새로운 <code>asyncId</code>가 해당 비동기 리소스에 할당되게 되고, 우리의 <code>init</code> 훅이 이 id와 부모 리소스의 <code>asyncId</code>를 넘겨받아 실행됩니다. 또한 이 모듈은 현재 실행되고 있는 함수의 <code>asyncId</code> 를 반환하는 <code>executeAsyncId()</code> 라는 매우 유용한 메소드를 제공합니다.</p><p>여기에 어떻게 우리가 훅이 실행될 때 메시지를 로깅했는지 확인할 수 있습니다:</p><pre><code>const fs = require('fs');
const async_hooks = require('async_hooks');

const log = (str) =&gt; fs.writeSync(1, `${str}\n`);

async_hooks.createHook({
  init(asyncId, type, triggerAsyncId) {
    log(`INIT: asyncId: ${asyncId} / type: ${type} / trigger: ${triggerAsyncId}`);
  },
  destroy(asyncId) {
    log(`DESTROY: asyncId: ${asyncId}`);
  },
}).enable();</code></pre><blockquote>이 코드에서 <code>console.log</code> 를 사용하지 않았다는 것을 확인할 수 있을텐데, 이는 <code>console.log</code> 는 비동기 작업이고, 이가 의미하는 것은 <code>console.log</code> 가 실행될 때 비동기 훅이 또(무한정으로) 실행된다는 사실입니다. 이곳에서의 해결책은 동기 함수인 <code>fs.writeSync</code> 를 이용하고 우리가 만든 훅을 트리거하지 않는 것입니다.</blockquote><p>간단하게 <code>setTimeout</code> 을 활용한 훅을 이용해 확인해봅시다:</p><pre><code>log(`&gt;&gt; Calling setTimeout: asyncId: ${async_hooks.executionAsyncId()}`);
setTimeout(() =&gt; {
  log(`&gt;&gt; Inside setTimeout callback: asyncId: ${async_hooks.executionAsyncId()}`);
}, 0);
log(`&gt;&gt; Called setTimeout: asyncId: ${async_hooks.executionAsyncId()}`);</code></pre><p>위 코드를 실행하면 다음과 같은 결과를 확인할 수 있습니다:</p><pre><code>&gt;&gt; Calling setTimeout: asyncId: 1
INIT: asyncId: 2 / type: Timeout / trigger: 1
&gt;&gt; Called setTimeout: asyncId: 1
&gt;&gt; Inside setTimeout callback: asyncId: 2
DESTROY: asyncId: 2</code></pre><p>어떤 일이 일어났는지 확인해봅시다:</p><ul><li>asyncId 1로 시작했고 setTimeout을 호출했습니다.</li><li><code>Timeout</code>이라는 비동기 리소스가 생성됐고 우리가 만든 <code>init</code> 훅이 새로 생성된 asyncId 2와 부모 asyncId 1을 받아 실행됐습니다.</li><li>프로그램 끝에 남긴 로그에서 asyncId는 여전히 1로 확인됩니다.</li><li><code>Timeout</code> 리소스 콜백이 호출됐고 asyncId 2가 기록되었습니다.</li><li>`Timeout` 리소스가 제거됐고 우리가 만든 <code>destroy</code> 훅이 실행됐습니다.</li></ul><p>이외에도 <code>before</code> 과 <code>after</code> 라는 훅이 존재합니다. 이 훅들은 외부 HTTP 요청 또는 SQL 쿼리와 같은 비동기 리소스에서 시간을 재는 데 사용될 수 있습니다.</p><h2 id="-">알겠는데, 그래서 요점은요?</h2><p><code>executionAsyncId()</code> 와 <code>init</code> 와 함께 (비동기라 더라도)우리가 만든 함수의 스택을 다시 만들 수 있습니다.</p><p>여기에 와닿는 실제 예제가 있습니다. 우리는 HTTP 서버를 만들 거고, 이 서버는 각 요청마다 <code>test.txt</code> 파일의 내용을 읽고 보내는 서버입니다.</p><pre><code>const fs = require('fs');
const async_hooks = require('async_hooks');
const http = require('http');

const log = (str) =&gt; fs.writeSync(1, `${str}\n`);

async_hooks.createHook({
  init(asyncId, type, triggerAsyncId) {
    log(`asyncId: ${asyncId} / trigger: ${triggerAsyncId}`);
  },
}).enable();

const readAndSendFile = (res) =&gt; {
  fs.readFile('./test.txt', (err, file) =&gt; {
    log(`&gt;&gt; Inside readAndSendFile: execution: ${async_hooks.executionAsyncId()}`);
    res.end(file);
  });
}

const requestHandler = (req, res) =&gt; {
  log(`&gt;&gt; Inside request: execution: ${async_hooks.executionAsyncId()}`);
  readAndSendFile(res);
}

const server = http.createServer(requestHandler);

server.listen(8080);</code></pre><p> 이 코드를 실행하고 두 개의 요청을 보내봅시다. 결과에서 불필요한 부분은 지웠습니다.</p><pre><code>&gt;&gt; Inside request: execution: 6
asyncId: 9 / trigger: 6
asyncId: 11 / trigger: 9
asyncId: 12 / trigger: 11
asyncId: 13 / trigger: 12
&gt;&gt; Inside readAndSendFile: execution: 13
[...]
&gt;&gt; Inside request: execution: 31
asyncId: 34 / trigger: 31
asyncId: 36 / trigger: 34
asyncId: 37 / trigger: 36
asyncId: 38 / trigger: 37
&gt;&gt; Inside readAndSendFile: execution: 38</code></pre><p>두 요청에 두 개의 asyncId (6, 31)가 할당된 것을 확인할 수 있습니다. 파일을 읽는 건 새로운 비동기 리소스를 만들고, <code>readAndSendFile</code> 은 두 개의 asyncId (13, 38)이 기록되었습니다.</p><p><code>readAndSendFile</code> 함수에서 "async path"를 추적하여 원래 요청의 <code>asyncId</code> 를 확인할 수 있는데, 예를 들어 첫번째 요청의 asyncId는 13이었고 이를 역추적하여 13 -&gt; 12 -&gt; 11 -&gt; 9 -&gt; 6을 얻을 수 있습니다.</p><h2 id="--1">쓸만한 것으로 만들기</h2><p>위 모든 것들을 이용해 각 요청마다 context 객체를 만들고 가져오는 함수를 만들 수 있습니다. 이는 HTTP 서버 뿐 아니라 다른 곳에서도 사용될 수 있겠죠.</p><pre><code>const async_hooks = require('async_hooks');

const contexts = {};

async_hooks.createHook({
  init: (asyncId, type, triggerAsyncId) =&gt; {
    // 새로운 비동기 리소스가 생성되었을 때
    // 만약 부모 asyncId가 이미 context 객체를 가지고 있다면
    // 해당 context 객체를 현재 리소스 asyncId에 할당합니다
    if (contexts[triggerAsyncId]) {
      contexts[asyncId] = contexts[triggerAsyncId];
    }
  },
  destroy: (asyncId) =&gt; {
    // 메모리 누수를 방지하기 위해 정리하는 작업을 합니다
    delete contexts[asyncId];
  },
}).enable();


function initContext(fn) {
  // 새로운 비동기 리소스의 초기화를 강제합니다
  const asyncResource = new async_hooks.AsyncResource('REQUEST_CONTEXT');
  return asyncResource.runInAsyncScope(() =&gt; {
    // 새로운 asyncId를 갖게 되었습니다
    const asyncId = async_hooks.executionAsyncId();
    // 빈 객체를 asyncId의 context 객체로 할당합니다
    contexts[asyncId] = {}
    return fn(contexts[asyncId]);
  });
}

function getContext() {
  const asyncId = async_hooks.executionAsyncId();
  // 현재 asyncId에 연결된 context 객체를 가져옵니다
  // 아무 것도 없다면 빈 객체를 줍니다
  return contexts[asyncId] || {};
};

module.exports = {
  initContext,
  getContext,
};</code></pre><p>이제 잘 작동하나 확인해보는 간단한 코드를 짜봅시다.</p><pre><code>const {initContext, getContext} = require('./context.js');

const logId = () =&gt; {
  const context = getContext();
  console.log(`My context id is: ${context.id}`);
}

initContext((context) =&gt; {
  context.id = 1;
  setTimeout(logId, 100);
  setTimeout(logId, 300);
});

initContext((context) =&gt; {
  context.id = 2;
  setTimeout(logId, 200);
  setTimeout(logId, 400);
});</code></pre><p>이 코드를 실행하면 다음과 같은 결과를 얻을 수 있습니다:</p><pre><code>My context id is: 1
My context id is: 2
My context id is: 1
My context id is: 2</code></pre><h2 id="--2">다음은 뭘까요?</h2><p>여기서 구현한 두 개의 함수는 매우 간단하지만, 데이터를 담아둘 context 객체를 만들고 이를 모든 함수에 넘길 필요가 없게끔 하는데 유용하게 사용할 수 있습니다.</p><p>실제 활용으로는 HTTP 요청마다 context를 만들고 요청 ID를 발급하고 로깅하는 함수에서 이 ID를 가져와야 하는 상황이 될 수 있습니다. 저는 또한 하나의 HTTP 요청에서모든 데이터베이스 요청을 모아서 동일 트랙잭션 내에서 실행하는 데에도 사용했습니다.</p><p>context를 만들어 데이터를 담을 때, 얼마나 많은 데이터를 담을지에 대해 조심할 필요가 있습니다. 이는 버그를 만들 수도 있는 엣지 케이스를 방지하기 위해 프로그램의 데이터 흐름에서 최대한 간단하게 유지되어야 합니다. 또한 TypeScript와 같은 도구를 사용할 때에 이런 context를 사용하면 현재 context가 무엇인지 확인할 수 없을 수도 있습니다.</p><p><a href="https://nodejs.org/api/async_hooks.html"><code>async_hooks</code> </a>은 아직 실험적인 기능이란 것을 알아두시기 바랍니다. 하지만 모험적인 것을 좋아한다면 한 번 사용해보세요!</p>]]></content:encoded></item><item><title><![CDATA[2018년에 크롬은 정말로 빠를까요?]]></title><description><![CDATA[<p>개인적으로 맥을 사용할 때면 크롬 대신 사파리를 사용하고 있습니다. 원래는 크롬을 사용했지만 성능, 배터리(= 전력 소모) 면에서 사파리가 크롬보다 월등히 뛰어나다는 것을 몸소 느끼고 있기때문에 크롬은 웹 개발을 할 때를 제외하고는 켜지 않고 있습니다.</p><p>그러다 갑자기 궁금증이 생겼습니다. 크롬 vs 사파리 구도에서 크롬이 훨씬 무겁고, 메모리를 더 많이 먹으며, 불필요하게</p>]]></description><link>https://tech.ssut.me/apple-safari-12-versus-google-chrome-72-in-2018/</link><guid isPermaLink="false">5bf02f2c0e761335a6390a3b</guid><category><![CDATA[web browser]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Sat, 17 Nov 2018 15:39:00 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/11/Safari-Vs-Chrome.png" medium="image"/><content:encoded><![CDATA[<img src="https://tech.ssut.me/content/images/2018/11/Safari-Vs-Chrome.png" alt="2018년에 크롬은 정말로 빠를까요?"><p>개인적으로 맥을 사용할 때면 크롬 대신 사파리를 사용하고 있습니다. 원래는 크롬을 사용했지만 성능, 배터리(= 전력 소모) 면에서 사파리가 크롬보다 월등히 뛰어나다는 것을 몸소 느끼고 있기때문에 크롬은 웹 개발을 할 때를 제외하고는 켜지 않고 있습니다.</p><p>그러다 갑자기 궁금증이 생겼습니다. 크롬 vs 사파리 구도에서 크롬이 훨씬 무겁고, 메모리를 더 많이 먹으며, 불필요하게 CPU를 많이 사용하고, 많은 성능(전력)을 요구한다는 사실을 밝혀낸 수많은 자료가 있지만 대부분 2017년 자료이거나 2018년 초 자료인 경우가 다수였습니다. (Safari 11, Chrome 68)</p><p>그래서 직접 벤치마크를 해봤습니다.</p><h1 id="-">벤치마크</h1><h2 id="--1">환경</h2><p>벤치마크 환경은 2018 맥북 프로 15인치 (i7-8850H, 32GB of RAM, 512GB SSD), MacOS Mojave 10.14.1에서 진행하였습니다.</p><p>벤치마크에 사용된 웹 브라우저의 버전은 다음과 같습니다.</p><ul><li>Google Chrome 72.0.3610.2 (V8 7.2.317)</li><li>Safari 12.0.1 (JavaScriptCore)</li></ul><p>확장 프로그램의 간섭을 받지 않도록 두 브라우저 모두 Incognito Mode에서 벤치마크를 진행하였습니다.</p><h2 id="--2">결과</h2><h3 id="ares6">ARES6</h3><figure class="kg-card kg-image-card"><img src="https://tech.ssut.me/content/images/2018/11/image.png" class="kg-image" alt="2018년에 크롬은 정말로 빠를까요?"></figure><p><a href="http://browserbench.org/ARES-6/">Ares-6</a>는 다양한 JS의 새로운 기능을 이용하는 벤치마크입니다. <strong>사파리는 크롬보다 약 11% 빠른 스코어를 보여주었습니다.</strong></p><h3 id="kraken">Kraken</h3><figure class="kg-card kg-image-card"><img src="https://tech.ssut.me/content/images/2018/11/image-1.png" class="kg-image" alt="2018년에 크롬은 정말로 빠를까요?"></figure><p><a href="https://krakenbenchmark.mozilla.org/">Kraken</a>은 Mozilla가 만든 자바스크립트 벤치마크 도구입니다. <strong>사파리는 크롬보다 약 26% 빠른 스코어를 보여주었습니다.</strong></p><h3 id="motionmark">MotionMark</h3><figure class="kg-card kg-image-card"><img src="https://tech.ssut.me/content/images/2018/11/image-2.png" class="kg-image" alt="2018년에 크롬은 정말로 빠를까요?"></figure><p><a href="http://browserbench.org/MotionMark/">MotionMark</a>는 다양하고 복잡한 애니메이션(그래픽) 성능을 테스트하는 벤치마크 도구입니다. <strong>사파리는 크롬보다 6배 이상 빠른 스코어를 보여주었습니다.</strong></p><h3 id="octane">Octane</h3><figure class="kg-card kg-image-card"><img src="https://tech.ssut.me/content/images/2018/11/image-5.png" class="kg-image" alt="2018년에 크롬은 정말로 빠를까요?"></figure><p><a href="http://chromium.github.io/octane/">Octane</a>은 구글 Chromium 팀이 개발한 종합 JS 벤치마크 도구입니다. 구글이 2017년에 벤치마크 항목에 높은 점수가 나오도록 웹 브라우저가 개발되는 것을 우려하여 <a href="https://v8project.blogspot.com/2017/04/retiring-octane.html">개발 중단을 선언</a>하였으나 아직 벤치마크는 가능합니다. <strong>이 벤치마크에서는 사파리가 크롬보다 2% 정도 빠른 스코어를 보여주었습니다.</strong></p><h3 id="speedometer">Speedometer</h3><figure class="kg-card kg-image-card"><img src="https://tech.ssut.me/content/images/2018/11/image-6.png" class="kg-image" alt="2018년에 크롬은 정말로 빠를까요?"></figure><p><a href="http://browserbench.org/Speedometer/">Speedometer</a>는 웹 브라우저에서 애플리케이션이 얼마나 빨리 반응하는지를 측정하는 브라우저 벤치마크 도구입니다. 실제 웹 브라우저 체감 성능과 가장 근접한 벤치마크이므로 웹 앱을 자주 사용한다면 이 벤치마크 결과를 주시할 필요가 있습니다. <strong>사파리는 크롬보다 14% 정도 더 빠른 스코어를 보여주었습니다.</strong></p><h3 id="jetstream">JetStream</h3><figure class="kg-card kg-image-card"><img src="https://tech.ssut.me/content/images/2018/11/image-7.png" class="kg-image" alt="2018년에 크롬은 정말로 빠를까요?"></figure><p><a href="https://browserbench.org/JetStream/">JetStream</a>은 종합적인 JS 성능을 측정하는, 가장 넓은 범위를 커버하는 벤치마크 도구입니다. SunSpider 1.0.2 및 Octane 2 벤치마크 suite를 포함하고 있습니다. <strong>사파리는 크롬보다 54% 정도 더 빠른 스코어를 보여주었습니다.</strong> </p><h2 id="--3">정리</h2><figure class="kg-card kg-image-card"><img src="https://tech.ssut.me/content/images/2018/11/Untitled-5.png" class="kg-image" alt="2018년에 크롬은 정말로 빠를까요?"></figure><p>전체적으로 사파리가 크롬보다 훨씬 좋은 성능을 보여주고 있으며, 특히 실제 페이지 렌더링 성능을 보여주는 MotionMark에서는 넘사벽의 성능차를 보여주고 있습니다.</p><p>사파리가 '불편'해서 사용하지 못한다는 분들도 많이 계십니다만, 사파리도 나와있을 확장 프로그램은 거의 다 나와있고 userscript를 통한 확장도 가능하여 웹 브라우징 목적으로는 부족하지 않게 사용할 수 있는 상태입니다. 또한 iOS가 설치된 기기를 사용한다면 Keychain을 공유해서 사용할 수 있다는 점은 사용성 면에서 굉장히 매력적인 포인트입니다. 특히나 배터리 상태로 맥을 사용한다면 가뜩이나 느린 크롬은 배터리까지 더 잡아먹기 때문에, 더더욱이 크롬을 사용할 이유가 없습니다. 느리고, 메모리도 많이 사용하고, 배터리도 많이 사용하는 크롬 대신 사파리를 사용해 보세요.<br>(비슷한 비교로 Windows에서는 Edge가 더 빠릅니다..)</p><hr><p><strong>결론: 맥에서는 사파리 씁시다. 끝.</strong></p>]]></content:encoded></item><item><title><![CDATA[HTTPS is faster than HTTP]]></title><description><![CDATA[For those who believe that HTTP is ever faster than HTTPS in these days. Chromium Dev has announced that from the 62 version of Chrome they'll label HTTP sites as 'Not Secure'. It is widely known that HTTP is vulnerable to leaks and attacks. No need to complicated MITM, and it's hard to find whether it is compromised data or not when entering the website, which is not DNSSEC signed.]]></description><link>https://tech.ssut.me/https-is-faster-than-http-en/</link><guid isPermaLink="false">5b585aa4e2a96873632ed4b0</guid><category><![CDATA[http]]></category><category><![CDATA[https]]></category><category><![CDATA[http2]]></category><category><![CDATA[security]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Wed, 08 Aug 2018 17:05:30 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/08/RZKScM0.png" medium="image"/><content:encoded><![CDATA[<img src="https://tech.ssut.me/content/images/2018/08/RZKScM0.png" alt="HTTPS is faster than HTTP"><p><strong>For those who believe that HTTP is ever faster than HTTPS in these days.</strong></p>
<blockquote>
<p>Note: This is an article that I translated from <a href="https://tech.ssut.me/2017/05/07/https-is-faster-than-http/">my original article written in Korean</a> (2017-05).<br>
<em>I'm not an English native.</em></p>
</blockquote>
<p><img src="https://lh6.googleusercontent.com/l8_RG7ZnHQgTh6taxEgVoN2IDE_7KUfF6evEGp91wScTk8xHcNgS7M8x-SvF9UwBN7f15F0WNBVHGY7L10Cgd12ZJs30AmiNW20zxLfabaNogrwC9ioZxaaBiLZDoPEyldb9almk" alt="HTTPS is faster than HTTP"></p>
<p>Chromium Dev has announced that from the 62 version of Chrome they'll label HTTP sites as 'Not Secure'.</p>
<p><img src="https://lh4.googleusercontent.com/5kwSTROagZ5eV6ljampQwA19Sa458XGtmNhzLTlPRvSatTQ3f85mciuza1H0ocUSBBWvVMqvQbr4L1Z_YFOMhkhOGNl_vrjuWM3dgKljhaVJ3xk2h3rTuL52_mdBmwc_FNFKB9y9" alt="HTTPS is faster than HTTP"></p>
<p>It is widely known that HTTP is vulnerable to leaks and attacks. No need to complicated MITM, and it's hard to find whether it is compromised data or not when entering the website, which is not DNSSEC signed. In order to treat these problems, HTTPS was introduced and implemented by the SSL (Secure Socket Layer). However, this simple implementation has also introduced some problems for both clients and servers.</p>
<h2 id="encryptioncosts">Encryption costs</h2>
<p>Before the communication starts, HTTPS needs time to do the handshake like the following: <em>For more details see <a href="http://www.moserware.com/2009/06/first-few-milliseconds-of-https.html">http://www.moserware.com/2009/06/first-few-milliseconds-of-https.html</a></em></p>
<p><img src="https://i.stack.imgur.com/Rcq1a.png" alt="HTTPS is faster than HTTP"></p>
<p>The data can be securely transferred after both server and client exchange encryption algorithms and corresponding keys. These days, DHE-RSA (RSA: signature, Diffie-Hellman: key exchange) is mainly used, and also there is a mechanism called OCSP(Online Certificate Status Protocol) which queries the issuing authority for the validity of that certificate.</p>
<p>The good thing is that the communication will be fast once successfully completed the above steps. And Keep-Alive can reduce the costs because it keeps the session.</p>
<p>We can say &quot;it is cryptographically secure&quot; when the time taken to process the encryption is greater. But the performance matters. If every request requires this handshake stuff in order to communicate, we cannot say &quot;it requires a low cost&quot;. So far (as of July 2017) this have been an obstacle to serving all pages in HTTPS, so this was the fundamental reason why people think &quot;HTTPS is so slow and requires high-performance machines&quot;.</p>
<h2 id="httpsinproductions">HTTPS in productions</h2>
<p>But the computer world has changed. The performance of machines these days are way better than before while there are no more new things with TLS for HTTPS encryption. See the following benchmarks doing <code>OpenSSL speed RSA</code> without <code>multi</code>:</p>
<p><strong>Intel XEON 5130 (Mac Pro 2007, similar to i5-3570K)</strong></p>
<pre><code>                  sign    verify    sign/s verify/s
rsa  512 bits 0.000148s 0.000010s   6778.9  96769.7
rsa 1024 bits 0.000499s 0.000029s   2005.4  34371.6
rsa 2048 bits 0.003225s 0.000100s    310.1  10036.3
rsa 4096 bits 0.023140s 0.000367s     43.2   2727.1
</code></pre>
<p><strong>Intel Core i7-2600 (2011)</strong></p>
<pre><code>sign    verify    sign/s verify/s
rsa  512 bits 0.000054s 0.000004s  18556.8 267823.0
rsa 1024 bits 0.000158s 0.000010s   6317.2  99584.9
rsa 2048 bits 0.001123s 0.000034s    890.4  29142.7
rsa 4096 bits 0.008168s 0.000124s    122.4   8035.9
</code></pre>
<p>CPU performance has got really better in the last few years, this has proven to be of great benefit to the encryption. From the companies around:</p>
<blockquote>
<p>On our production frontend machines, SSL/TLS accounts for less than 1% of the CPU load, less than 10 KB of memory per connection and less than 2% of network overhead. Many people believe that SSL/TLS takes a lot of CPU time and we hope the preceding numbers will help to dispel that. <strong>-- Adam Langley, Google</strong></p>
</blockquote>
<blockquote>
<p>We have deployed TLS at a large scale using both hardware and software load balancers. We have found that modern software-based TLS implementations running on commodity CPUs are fast enough to handle heavy HTTPS traffic load without needing to resort to dedicated cryptographic hardware. <strong>-- Dough Beaver, Facebook</strong></p>
<p>Elliptic Curve Diffie-Hellman (ECDHE) is only a little more expensive than RSA for an equivalent security level… In practical deployment, we found that enabling and prioritizing ECDHE cipher suites actually caused negligible increase in CPU usage. HTTP keepalives and session resumption mean that most requests do not require a full handshake, so handshake operations do not dominate our CPU usage. We find 75% of Twitter’s client requests are sent over connections established using ECDHE. The remaining 25% consists mostly of older clients that don’t yet support the ECDHE cipher suites. <strong>-- Jacob, Hoffman-Andrews, Twitter</strong></p>
</blockquote>
<blockquote>
<p>**SSL/TLS is not computationally expensive any more.</p>
</blockquote>
<h2 id="encryptioncostsonlargefiletransfers">Encryption costs on large file transfers</h2>
<p>Many people consider sending file hashes and then serving files in HTTP because they think serving files in HTTPS requires high-performance machines, but we need to think more. As on &quot;<a href="https://hackernoon.com/yes-python-is-slow-and-i-dont-care-13763980b5a1">Yes, python is Slow, and I Don't Care</a>&quot; article shows us that CPU is way faster than networks or disks. Well, let's see. Most server-class network adapters can handle up to 10Gbps, even for large &quot;single&quot; instances. What this number means is that it doesn't matter when you really need to send large files to clients, since the physical devices like SSDs and network adapters can't handle those amount of demands. So with those ideas, it does not increase both the number of traffics a server can process and CPU usages, even if you have planned to use high-performance parts like 10Gbps PCI Express SFP+ NIC or PCI Express SSD. This is so simple: it is only &quot;100 connections&quot; with 10Gbps network adapter and 100 connections with 100Mbps bandwidth, by some estimates.</p>
<p>On the other hand, sending file hashes and then serving files in HTTP is inefficient and considered a bad thing because it needs to establish 2 TCP connections under different protocols. You may think that it needs 4 connections in real since TCP establishes a two-way connection between a server and a single client. What this number means is that TCP establishment overhead is 2x compared to an HTTPS connection, requires 2x RTT. In the paragraph above, I said 100 simultaneous download requests, so let's see, we can say it needs 200 TCP connections, so 600 RTTs.</p>
<p>No one ever said about network bandwidth and disk performance, most people are all focused on CPU performance, when talking on large file transfers. It does not seem to make sense to say that it requires high loads because not only large file transfers but also CPU should do that.<br>
Most of cloud and CDN service providers such as Google Drive, Dropbox, iCloud, OneDrive, Cloudflare, etc has already been serving files from small to large in HTTPS, so that means that CPU does not matter because the real reason is in the network bandwidth.</p>
<h2 id="googlespdyandhttp2">Google, SPDY, and HTTP/2</h2>
<p>In 2012, Google announced project SPeeDY (SPDY), which uses multiplexing and prioritizing under the TCP connection, so makes the web faster. After a few years (as of March 2015), HTTP/2 implementation specification was officially <a href="https://tools.ietf.org/html/rfc7540">standardized</a> in response to Google's HTTP-compatible SPDY protocol.</p>
<p>As the name of HTTP/2 implies, in the beginning, it was also designed for HTTP, but most major web browsers have added support for HTTP/2 in only secured connection, this connotes that HTTP/2 is designed in order to power HTTPS.</p>
<p>HTTP/2, for the faster web, really seems to be designed for HTTPS. HTTP/2 is binary transfer protocol, whereas HTTP/1.1 is plaintext transfer protocol, if we think about this carefully, communicating on a single connection bring great benefits to HTTPS, not much for HTTP.</p>
<p>In the real, there have been many web changes since HTTP/2 has become more popular, like the number of websites supporting HTTPS, based on <a href="http://w3techs.com/technologies/details/ce-http2/all/all">W3Techs</a> statistics in August 2018, HTTP/2 is used by 29.0% of all the websites. HTTP/2 has great growth potential, and because many CDN services around the world such as CloudFlare, MaxCDN, and Fastly support HTTP/2, many people around the world can also use HTTP/2 connection.</p>
<h2 id="httpssecurityproblem">HTTP's security problem</h2>
<p>In Feb 2018, the biggest Korean web portal NAVER has marked as &quot;Not Secure&quot; website because they have been served their website as the only HTTP. In response to this, they said, &quot;Our website's landing page does not contain any kind of private data, only public data, and we are serving authentication and search engine page as HTTPS&quot;. Let's talk about the problem on here.</p>
<p>This is also one reason why DNS matters: DNS was designed in a simple way like when client saying &quot;I need IP addresses v4(A) for xxx.xxx&quot;, the server responds &quot;The IP addresses v4(A) for xxx.xxx are [x.x.x.x, x.x.x.x, ...]&quot;. Because of this simple way, the security layer on the DNS was not considered to design, and because of the implementation, DNS produced a bunch of security-related problems. Nowadays, there is a technology called <a href="https://en.wikipedia.org/wiki/Domain_Name_System_Security_Extensions">DNSSEC</a>, which verifies that a domain name points to the correct web server, however, fundamentally, the problem has not been solved yet.</p>
<p>So, let's think: why we need an encrypted connection even though we do not transfer privacy data? this refers to the internet censorship, which was almost universally censored. Korea, for example, has already censored HTTP connections and checks the Host header by using DPI(Deep Packet Inspection), in order to block illegal websites in the country. The problem starts here, the government can surveil connections, from that they can know the internet activities and user's behavior. For example, if a person requested news.naver.com and then requested a subpage subsequently, they can mark a user and check what page that user has visited. This also applies to DNS. So, the real problem is internet censorship, all users' have the right to be protected and it is not good if there is any way for some to see it.</p>
<p><img src="https://cdn.comparitech.com/wp-content/uploads/2015/11/wireshark-7.png" alt="HTTPS is faster than HTTP"></p>
<p>Let's back to the first paragraph and I'd like to say: for these reasons, this is why we must use HTTPS for every website we make. they, NAVER, said their landing page does not contain any sensitive data, however, not only landing page but also all subpages from their landing page is served with HTTP, so this affects to internet censorship by giving hints. Further, there is a chance that a hacker can manipulate the page in order to compromise.</p>
<h3 id="dnsproblem">DNS problem</h3>
<p>As I mentioned above, DNS itself is not encrypted by default, so everyone who can monitor your traffic. In order to help to mitigate these situations, there are some projects that can secure all the way to DNS servers:</p>
<ul>
<li><a href="https://dnscrypt.org/">DNSCrypt</a></li>
<li><a href="https://developers.google.com/speed/public-dns/docs/dns-over-https">DNS-over-HTTPS</a> from Google Public DNS</li>
</ul>
<h2 id="certificatesarefree">Certificates are free</h2>
<p>Certificates have been much costed, and this was one reason people don't use HTTPS. In the past, we had to use Trusted root authorities like Comodo, VeriSign, Symantec, GeoTrust, Thawte, etc, but many internet service corporations like Cisco, Google, Mozilla, Fastly, etc have thought that they would want people to use HTTPS everywhere, and so they launched <a href="https://letsencrypt.org/">Let's Encrypt</a>. Let's Encrypt issues free certificates for everyone, so now every dev and ops can get a certificate for free, and moreover, this is now the biggest effect to HTTPS deployments.</p>
<p><img src="https://letsencrypt.org/images/letsencrypt-logo-horizontal.svg" alt="HTTPS is faster than HTTP"></p>
<p>And because <a href="https://github.com/ietf-wg-acme/acme/">ACME(Automatic Certificate Management Environment)</a> has announced, it also delivered some automated certificate issue projects like <a href="https://certbot.eff.org/">certbot</a>, or <a href="https://github.com/golang/go/issues/17053">Golang's internal library</a>. In order words, ACME makes it easy for all users to issue HTTPS certificates, rather than traditional complex and troublesome HTTPS certificate issuing methods, which makes people force not to say like &quot;Issuing HTTPS certificate is so hard&quot; or &quot;the cost of the certificate is so high&quot;.</p>
<h2 id="movetohttps">Move to HTTPS</h2>
<p><img src="https://i.imgur.com/RZKScM0.png" alt="HTTPS is faster than HTTP"></p>
<p>I think not a lot of people can believe what this article is saying about. This is the test of HTTP vs HTTPS by Let's Encrypt: <a href="https://www.httpvshttps.com/">https://www.httpvshttps.com/</a>. If you are using <a href="http://caniuse.com/#search=http2">web browser that supports HTTP/2</a> I guarantee you can see HTTPS is way faster than HTTP. The difference increases when you refresh that page because of the session resumption feature of HTTP/2. There is also a benchmark on the official Go website: <a href="https://http2.golang.org/gophertiles">https://http2.golang.org/gophertiles</a>. In this page, you might see a different kind of test. Like the first one, you can see HTTPS is way faster as well. Right, these tests in real are not about HTTP vs HTTPS, it is HTTP/1.1 vs HTTP/2. <strong>However, please be reminded that <em>on every web browser</em>, HTTP/2 is ONLY supported on the top of SSL/TLS.</strong></p>
<p>There is no big deal while moving to HTTPS. But you just have to note that some requests from HTTPS to HTTP are prohibited by the web browser policy, like when you load a script, which is from HTTP page, from an HTTPS page. To move gracefully, the movement operation must be done after a thorough review and staging testing. As of 2018, most major web servers support HTTP/2: Apache is supported by mod_h2 module from 2.4.12, nginx from 1.9.5 (backed by SPDY in older version), and node.js from 5.0.</p>
<p>If you are using major CDNs such as Akamai, Cloudflare, AWS CloudFront, Fastly, etc you don't have to do anything. yay!</p>
<hr>
<p>HTTPS(TLS) is not an issue related to the performance. It is mandatory, not optional. The times are changing fast. Not only Google, Facebook, and Twitter, but also websites, that don't have to worry about leaking personal information when not logged in, like Github, Booking.com, and YouTube have HTTPS connections by default.</p>
<blockquote>
<p>In January this year (2010), Gmail switched to using HTTPS for everything by default. Previously it had been introduced as an option, but now all of our users use HTTPS to secure their email between their browsers and Google, all the time. In order to do this, we had to deploy <em>no additional machines</em> and <em>no special hardware</em>.</p>
<p>-- <a href="https://www.imperialviolet.org/2010/06/25/overclocking-ssl.html">Overclocking SSL</a>, 25 Jun 2010, Google</p>
</blockquote>
<p>References:</p>
<ul>
<li><a href="https://istlsfastyet.com/">https://istlsfastyet.com/</a></li>
<li><a href="https://www.maxcdn.com/blog/ssl-performance-myth/">https://www.maxcdn.com/blog/ssl-performance-myth/</a></li>
<li><a href="https://arstechnica.com/business/2011/03/https-is-more-secure-so-why-isnt-the-web-using-it/">https://arstechnica.com/business/2011/03/https-is-more-secure-so-why-isnt-the-web-using-it/</a></li>
<li><a href="https://www.keycdn.com/blog/https-performance-overhead/">https://www.keycdn.com/blog/https-performance-overhead/</a></li>
</ul>
]]></content:encoded></item><item><title><![CDATA[[번역] OpenSSH의 기본 키 암호화는 평문보다 못합니다]]></title><description><![CDATA[<blockquote>
<p>이 글은 Latacora에서 작성한 <a href="https://latacora.singles/2018/08/03/the-default-openssh.html">The default OpenSSH key encryption is worse than plaintext</a>의 번역글입니다. 암호화에 대한 지식이 깊지 않고 영문 번역 전문가가 아니기 때문에 오역이 많을 수 있습니다. (<em>많을 거라 확신합니다.</em>) 이에 대해서는 개인 이메일로 문의주시면 바로 수정하겠습니다. 감사합니다.</p>
</blockquote>
<p>최근에 eslint-scope npm 패키지가 해커의 손에 넘어가 npm 사용자의 홈</p>]]></description><link>https://tech.ssut.me/the-default-openssh-key-encryption-is-worse-than-plaintext/</link><guid isPermaLink="false">5b697d35e2a96873632ed4c6</guid><category><![CDATA[security]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Tue, 07 Aug 2018 12:11:13 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/08/810px-AES-SubBytes.svg.png" medium="image"/><content:encoded><![CDATA[<blockquote>
<img src="https://tech.ssut.me/content/images/2018/08/810px-AES-SubBytes.svg.png" alt="[번역] OpenSSH의 기본 키 암호화는 평문보다 못합니다"><p>이 글은 Latacora에서 작성한 <a href="https://latacora.singles/2018/08/03/the-default-openssh.html">The default OpenSSH key encryption is worse than plaintext</a>의 번역글입니다. 암호화에 대한 지식이 깊지 않고 영문 번역 전문가가 아니기 때문에 오역이 많을 수 있습니다. (<em>많을 거라 확신합니다.</em>) 이에 대해서는 개인 이메일로 문의주시면 바로 수정하겠습니다. 감사합니다.</p>
</blockquote>
<p>최근에 eslint-scope npm 패키지가 해커의 손에 넘어가 npm 사용자의 홈 디렉토리로부터 npm 인증 정보를 훔쳐갔던 일이 있었습니다. 우리는 차례대로 짚어보면서 어떤 것들이 문제였고 어떻게 위험을 줄일 수 있는지 알아봤습니다.</p>
<p>많은 사람들이 RSA SSH 키를 갖고 있습니다. 이 SSH키는 대부분의 권한을 갖고 있습니다: production 및 GitHub에 액세스 할 권한 같은 것들 말이죠. npm 인증 정보와는 반대로 SSH 키는 암호화됩니다, 자 그럼 이게 유출되어도 안전할까요? 한번 짚어보죠!</p>
<pre><code class="language-sh">user@work /tmp $ ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/home/user/.ssh/id_rsa): mykey
...
user@work /tmp $ head -n 5 mykey  
-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: AES-128-CBC,CB973D5520E952B8D5A6B86716C6223F

+5ZVNE65kl8kwZ808e4+Y7Pr8IFstgoArpZJ/bkOs7rB9eAfYrx2CLBqLATk1RT/
</code></pre>
<p>뭐 암호화가 된 것처럼 보이니까 암호화 됐다고 말할 수도 있겠습니다. 잘 보니까 <code>MII</code>(RSA키임을 알 수 있는 base64 DER 단서)로 시작하지는 않네요. 네 AES입니다! 참 좋아요, 그쵸? 심지어, 표면상으로는 랜덤으로 생성된 IV<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>를 CBC<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>에 쓰고 있어요. MAC은 없지만, oracle padding같은 것들은 없으니까 괜찮겠죠?</p>
<p>이 DEK-Info가 의미하는 것이 무엇인지 찾아내는 것은 복잡합니다. openssh-portable 레포에서 이 DEK-Info 문자열을 찾아보면 샘플 키만 나옵니다. 핵심은 이 AES키는 그저 MD5(비밀번호 || IV[:8])이라는 점입니다. 정말 좋지 않아보이죠: 가장 좋은 비밀번호 저장소의 사례는 낮은 엔트로피로 비밀번호를 보관하고, 암호화에 필요한 자료로 변형하기 위해 Argon2와 같은 암호화하는데 매우 큰 비용이 드는 기능을 쓰는 것입니다. MD5는 암호화 비용이 매우 적습니다. 이 디자인이 갖는 유일한 것은 salt가 암호 뒤에 붙는 것 뿐입니다, 따라서 MD5(IV[:8])의 중간 상태를 연산하고 해당 값으로부터 비밀번호를 알아낼 수는 없습니다. 특별히 초당 수십억번의 MD5 연산을 할 수 있는 머신을 빌릴 수 있는 요즘 세상에선 칭찬받을 부분은 아닙니다. 뭐 그리 많은 비밀번호가 나오지는 않지만요.</p>
<p>아마도 OpenSSH가 어떻게 이따위로 구성되게 되었는지 물어볼 수 있겠습니다. 슬픈 답은 OpenSSL 커맨드 라인 툴이 이를 기본값으로 지정하고 있고, 이 때문에 이렇게 되어있는 것이죠.</p>
<p>표준화된 비밀번호 암호화 키가 평문보다 나을 수 없다고 하는 것에는 논쟁의 여지가 있습니다: 이 암호화는 아무 효과가 없습니다. 하지만 저는 강한 성명을 내놓겠습니다: 이건 평문보다 훨씬 나쁩니다. 이 주장은 간단합니다: SSH키 비밀번호가 비밀번호 관리자로 다뤄지는 경우는 거의 적습니다: 대신 그저 당신이 기억하는 것일 뿐이죠. 이걸 당신이 기억하고 있다면, 이걸 다른 어딘가에 또 사용하고 있을 수도 있겠죠. 당신이 쓰고 있는 기기의 비밀번호일 수도 있고요. 이 유출된 키는 오라클(oracle)을 제공합니다: 만약 제가 비밀번호를 정확하게 추측한다면(KDF<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>가 나쁘기 때문에 가능한 일입니다) 저는 제가 정확하게 추측했음을 알 수 있습니다, 왜냐하면 당신의 공개 키를 이용해 이를 검사할 수 있기 때문이죠.</p>
<p>RSA키 쌍 자체에는 아무런 문제가 없습니다: 이건 그저 개인키(private key)의 대칭 암호화일 뿐입니다. 공개키만 있다고 해서 공격에 사용할 순 없습니다.</p>
<p>어떻게 고칠 수 있을까요? OpenSSH에서 꼭 사용해야 할 &quot;새(New)&quot; 키 포맷이 있습니다. &quot;New&quot;가 의미하는 것은 2013입니다. 이 포맷은 bcrypt_pbkdf를 사용하는데 이는 고정적인 난이도(difficulty)를 가진 bcrypt에, pbkdf2 구성으로 작동하는 포맷입니다. 다행히도, 만약 당신이 Ed25519 키를 생성하려 할 때 이 new 포맷이 생성됩니다, 왜냐하면 오래된 SSH 키 포맷은 더이상 새로운 키 타입을 지원하지 않기 때문입니다. 이건 좀 이상한 주장입니다: Ed25519 그 자체가 이미 어떻게 직렬화(serialization)를 할지 정의하고 있기 때문에 어떻게 Ed25519 직렬화가 작동할지에 대해 정의하는데 키 포맷이 필요하지 않습니다. 하지만 만약 이게 어떻게 우리가 좋은 KDFs를 얻을 수 있는지라면, 이건 제가 다루고자 했던 부분이 아닙니다. 따라서 한가지 답은 <code>ssh-keygen -t ed25519</code>입니다. 만약 호환성의 이유로 RSA를 꼭 사용해야만 한다면 <code>ssh-keygen -o</code>를 사용할 수 있습니다. 이미 존재하는 키는 <code>ssh-keygen -p -o -f PRIVATEKEY</code>로 업그레이드할 수 있습니다. 만약 키가 Yubikey나 스마트 카드에 존재한다면 이 문제가 발생하지 않습니다.</p>
<p>저는 이에 대해 더 나은 답을 드리고 싶습니다. 다른 한편으로 aws-vault는 인증 정보를 디스크에서 키체인으로 옮기는 방법을 보여줬습니다. 다른 병렬 접근으로는 개발 환경을 분할된 환경으로 옮기는 것입니다. 마지막으로, 대부분의 스타트업은 오랫동안 SSH 키를 보유하지 않는 것이 좋습니다. 대신 SSH CA에 의해 발행된 임시 인증 정보를 사용하여 SSO를 이상적으로 두는 것이 좋습니다. 불행하게도 이는 GitHub에서 작동하지 않습니다.</p>
<p>PS: 신뢰할 수 있는 소스는 찾는 것은 어렵지만, 제 기억에 따르면: PEM과 같은 OpenSSH 개인키에 포함된 버전 파라미터는 암호화 방식에만 영향을 끼칩니다. 애초에 망가진 KDF기 때문에 이는 그리 중요하지 않습니다. 이는 프로토콜의 단편적인 협상 부분에 대한 논쟁이라고 저는 확신합니다, 이에 대해서는 나중에 블로그에 올려보겠습니다. 어느날 john the ripper를 돌려보고 싶다면, 이 키를 써보세요: <a href="https://gist.github.com/lvh/c532c8fd46115d2857f40a433a2416fd">https://gist.github.com/lvh/c532c8fd46115d2857f40a433a2416fd</a></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Initial Vector; 초기 block의 유추를 어렵게 key와 함께 포함 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Cipher Block Chaining; 이전 block을 다음 block의 입력으로 사용하여 안정성 향상 <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Key Derivation Function; 키 생성 방식 <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[[번역] 잘가요 마이크로서비스: 100개의 문제점 투성이를 1개의 슈퍼스타로]]></title><description><![CDATA[<blockquote>
<p>이 글은 Segment의 Alexandra Noonan이 작성한 <a href="https://segment.com/blog/goodbye-microservices/">Goodbye Microservices: From 100s of problem children to 1 superstar</a>의 번역입니다.</p>
</blockquote>
<p>음지에 숨어서 살아오지 않았더라면, 당신은 아마도 마이크로서비스가 오늘날의 아키텍처란 것을 알고 있을겁니다. 트렌드와 함께 성장하며 Segment는 <a href="https://segment.com/blog/why-microservices/">마이크로서비스를 최고의 선택으로</a> 초기에 빠르게 채택하였습니다. 그 결과 몇몇 케이스에서는 마이크로서비스가 성공적일 수 있었고, 곧 알게</p>]]></description><link>https://tech.ssut.me/goodbye-microservice/</link><guid isPermaLink="false">5b5ef407e2a96873632ed4b6</guid><category><![CDATA[microservices]]></category><category><![CDATA[complexity]]></category><category><![CDATA[simplicity]]></category><category><![CDATA[programming]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Mon, 30 Jul 2018 16:16:56 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/07/monolithic_vs_microservices.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
<img src="https://tech.ssut.me/content/images/2018/07/monolithic_vs_microservices.jpg" alt="[번역] 잘가요 마이크로서비스: 100개의 문제점 투성이를 1개의 슈퍼스타로"><p>이 글은 Segment의 Alexandra Noonan이 작성한 <a href="https://segment.com/blog/goodbye-microservices/">Goodbye Microservices: From 100s of problem children to 1 superstar</a>의 번역입니다.</p>
</blockquote>
<p>음지에 숨어서 살아오지 않았더라면, 당신은 아마도 마이크로서비스가 오늘날의 아키텍처란 것을 알고 있을겁니다. 트렌드와 함께 성장하며 Segment는 <a href="https://segment.com/blog/why-microservices/">마이크로서비스를 최고의 선택으로</a> 초기에 빠르게 채택하였습니다. 그 결과 몇몇 케이스에서는 마이크로서비스가 성공적일 수 있었고, 곧 알게 되겠지만 그 외의 경우에서는 그렇지 않다는 걸 알았죠.</p>
<p>잠시 설명하자면, 마이크로서비스는 서비스 중심의 소프트웨어 아키텍처입니다. 서버사이드 애플리케이션이 단일 목적의, 그리고 낮은 네트워크 오버헤드를 갖는 여러 서비스로 구성되어 있죠. 가장 큰 장점은 향상된 모듈화, 테스트 부담을 덜 수 있다는 점, 더 나은 함수 구성, 환경 격리, 개발 팀 자율성입니다. 이 반대는 모놀리식 아키텍쳐입니다. 많은 기능이 하나의 서비스에 뭉쳐 돌아가는데 테스트, 배포, 스케일링 또한 하나의 유닛을 기반으로 하죠.</p>
<p>2017년 초 우리는 <a href="https://segment.com/product">Segment의 제품</a>의 코어 파트들과 함께 정점에 도달했습니다. 그것은 마치 만약 우리가 마이크로서비스 트리를 떨어지게 한 것 같았고, 모든 가지가 시들기 시작했습니다. 우리가 빠르게 움직일 수 있게 해주는 대신, 작은 팀들은 그들이 엄청난 복잡도의 수렁에 빠지게 된 것을 알게 되었습니다. 이 아키텍처의 본질적인 이득은 짐이 되었습니다. 우리의 개발 속도가 곤두박질침에 따라, 결함율은 폭발하고 말았습니다.</p>
<p>결국, 팀은 그들이 더이상 진행할 수 없다는 것을 알게 되었습니다. 3명의 풀타임 엔지니어가 대부분 그들의 시간을 그저 시스템이 살아있게 하면서 말이죠. 무언가가 변화했습니다. 이 포스트는 우리가 어떻게 돌아갔는지와 우리 제품 요구와 팀의 요구에 맞게 변화하는 목표를 실행했는지에 관한 내용입니다.</p>
<h2 id="whymicroservicesworkworked">Why Microservices <s>work</s> worked</h2>
<p>Segment의 고객 데이터 인프라는 초당 수많은 이벤트를 처리하고 이 이벤트들을 파트너 API로 재전송합니다. 우리가 서버사이드 목적지라고 부르는 곳들이죠. 저들 중에는 Google Analytics, Optimizely, 또는 커스텀 웹훅같은 것들로 이루어진 수백가지 이상의 목적지가 존재합니다.</p>
<p>우리가 프로젝트를 처음 런칭했던 몇년 전으로 돌아가보면 그당시 아키텍처는 심플했습니다. 이벤트를 받는 API가 있었고 그 이벤트를 메시지 큐로 재전송해주기만 했죠. 이 경우 웹 또는 모바일 앱에서 사용자나 사용자의 액션이 포함된 정보가 JSON 형태로 넘어옵니다. 간단히 페이로드를 살펴보면 이렇게 생겼습니다:</p>
<pre><code class="language-json">{
  &quot;type&quot;: &quot;identify&quot;,
  &quot;traits&quot;: {
    &quot;name&quot;: &quot;Alex Noonan&quot;,
    &quot;email&quot;: &quot;anoonan@segment.com&quot;,
    &quot;company&quot;: &quot;Segment&quot;,
    &quot;title&quot;: &quot;Software Engineer&quot;
  },
  &quot;userId&quot;: &quot;97980cfea0067&quot;
}
</code></pre>
<p>이벤트가 큐에서부터 consume될 때, consumer에 의해 관리되는 설정은 어느 목적지에서 이벤트를 받을지를 결정하기 위해 사용되었습니다, 잇따라 이는 굉장히 유용했는데 왜냐하면 개발자들은 수많은 integration을 직접 구성하는 대신, 오직 이 이벤트를 하나의 엔드포인트로 전송하면 됐기 때문입니다. 그게 바로 Segment의 API죠.</p>
<p>만약 목적지로 보내는 요청 중 하나가 실패한다면, 가끔씩 우리는 나중에 해당 이벤트를 다시 전송하기도 했습니다. 몇몇 실패 이벤트는 재시도해도 괜찮았던 반면 일부는 그렇지 않았죠. 재시도 가능한 에러는 아무런 변동 없이 목적지에서 잘 받아줄 수 있었습니다. 예를 들어 HTTP 500 에러, 요청 수 제한, 그리고 타임아웃 같은 것들이 이에 해당됩니다. 재시도 불가능한 에러는 목적지에서 절대로 받아주지 않는단 것을 확신할 수 있는 요청들이었습니다. 예를 들어, 잘못된 인증 정보를 갖고 있거나 빠진 필드가 있는 요청이 이에 해당됩니다.</p>
<p><img src="https://assets.contents.io/asset_QhV9GV3m.png" alt="[번역] 잘가요 마이크로서비스: 100개의 문제점 투성이를 1개의 슈퍼스타로"></p>
<p>이때, 재시도 이벤트와 새 이벤트 모두를 담고 있던 단일 큐는 몇번의 재시도를 시행했고 그 결과 <a href="https://en.wikipedia.org/wiki/Head-of-line_blocking">head-of-line blocking</a> 현상이 나타났습니다. 이런 특별한 상황에서 이게 의미하는 것은, 만약 한 목적지가 느려지거나 죽게 되면 재시도는 큐를 가득 채울 것이고, 모든 목적지에 대해 딜레이가 발생하게 됩니다.</p>
<p>목적지 X가 임시적 이슈가 발생했고 모든 요청이 타임아웃과 함께 에러가 발생한다고 생각해봅시다. 이제, 이 요청은 목적지 X에 도달하지 못한 엄청난 수의 요청을 만들어낼 뿐 아니라 모든 실패한 요청이 다시 큐에 재시도로 쌓이게 됩니다. 우리의 시스템이 로드에 따라 자동으로 스케일되었기 때문에 이렇게 급작스럽게 증가하는 큐는 서비스 스택을 스케일업하는 우리의 능력을 뛰어 넘게 되어버렸고 결과 새 이벤트에 대해 딜레이가 발생하였습니다. 목적지 X가 잠시 죽었기 때문에 모든 목적지에 전달하는 시간이 증가하는 겁니다. 고객들은 제때제때 이 전달이 보장되는 것에 의존한 서비스를 만들었고 따라서 우리는 계속해서 이 파이프라인에서 어느 곳이든 어디론가 향하는 요청의 대기 시간을 늘릴 수는 없었습니다.</p>
<p><img src="https://assets.contents.io/asset_joMYeSNt.png" alt="[번역] 잘가요 마이크로서비스: 100개의 문제점 투성이를 1개의 슈퍼스타로"></p>
<p>head-of-line blocking 문제를 해결하기 위해, 팀은 분리된 서비스를 만들어 각 목적지에 따라 큐를 했습니다. 이 새 아키텍처는 이벤트를 받아 처리하고 복사한 이벤트를 각 선택된 목적지로 분배하여 배포하는 추가적인 라우터 프로세스로 구성되어 있었습니다. 이제 만약 한 목적지에 문제가 생기면 해당 큐만 문제를 겪고 나머지 목적지에는 영향을 끼치지 않았습니다. 마이크로서비스 스타일 아키텍처는 목적지 각각을 격리시켰고, 이는 한 목적지가 자주 그렇듯 문제를 발생시킬 때 매우 유용했었습니다.</p>
<p><img src="https://assets.contents.io/asset_ElKl4CZv.png" alt="[번역] 잘가요 마이크로서비스: 100개의 문제점 투성이를 1개의 슈퍼스타로"></p>
<h2 id="">레포 분리 케이스</h2>
<p>각 목적지의 API는 서로 다른 요청 포맷을 사용합니다. 이벤트를 특정 포맷에 매치시키기 위해 변환하는 커스텀 코드를 필요로 했죠. 간단한 한 예는 우리의 API는 생일을 <code>traits.birthday</code>로 전송하는데 반해 목적지 X가 페이로드에 생일을 <code>traits.dob</code>로 전송해야 했던 예입니다. 목적지 X의 이 변환 코드는 대충 이렇게 생겼습니다:</p>
<pre><code class="language-javascript">const traits = {}
traits.dob = segmentEvent.birthday
</code></pre>
<p>최근 많은 목적지 엔드포인트들은 이런 변환을 상대적으로 쉽게 하는 Segment의 요청 포맷을 채택했습니다. 하지만 이런 변환은 목적지의 API 구조에 의존하여 매우 복잡해질 수도 있습니다. 예로, 몇몇 오래되고 제멋대로 생긴 목적지가 있는데 이때 우리는 이런 값들을 손수 직접 만든 XML 페이로드에 담아넣기도 했습니다.</p>
<p>초기 목적지가 여러 서비스로 나뉘어졌을 때 모든 코드는 한 레포안에 있었습니다. 좌절감을 주었던 가장 큰 부분은 바로 한 개의 깨진 테스트가 모든 목적지 테스트를 실패로 이끌었다는 점이었습니다. 우리가 어떤 변화를 배포하길 원했을 때, 우리는 해당 테스트가 초기의 변경과 아무런 상관이 없었음에도 깨진 테스트를 고치는데 시간을 써야만 헀습니다. 이 문제에 대응하기 위해 각각의 목적지로 향하는 코드는 각각의 레포를 갖게끔 되었습니다. 모든 목적지의 코드는 이미 각각의 서비스로 나뉘어져 있었고 따라서 과정은 순조로웠습니다.</p>
<p>이렇게 레포를 분리하여 코드를 나눈 결과 우리는 손쉽게 테스트를 격리할 수 있게 되었습니다. 이 격리조치는 개발팀이 목적지에 대한 코드를 건드려야 할 때 빠르게 움직일 수 있게끔 해주었습니다.</p>
<h2 id="">마이크로서비스의 스케일링과 레포</h2>
<p>시간이 지남에 따라 우리는 50개가 넘는 목적지를 추가했고 이가 의미하는 바는 50개의 레포가 생겼다는 것입니다. 이러한 코드베이스를 손쉽게 관리하고 개발함에 있어 짐을 덜기 위해 우리는 일반적인 코드와 기능을 공유 라이브러리로 나누어 관리했습니다, 각각의 목적지로 손쉽게 가게끔 HTTP 요청 핸들링 코드 같은 것들이 이에 포함됩니다.</p>
<p>예로 만약에 우리가 한 이벤트로부터 사용자의 이름을 가져오길 원한다면 <code>event.name()</code>을 실행하여 어드 목적지의 코드에서든 이를 가져올 수 있습니다. 이 공유 라이브러리는 이벤트에 <code>name</code>이나 <code>Name</code>이란 키의 속성이 있는지 확인합니다. 만약 이것들이 존재하지 않다면 <code>firstName</code>, <code>first_name</code>, <code>FirstName</code>같은 것들을 찾아보았습니다. last name에도 동일하게 말이죠, 이렇게 찾아서 두 이름을 하나의 풀네임으로 바꾸어줬습니다.</p>
<pre><code class="language-javascript">Identify.prototype.name = function() {
  var name = this.proxy('traits.name');
  if (typeof name === 'string') {
    return trim(name)
  }
  
  var firstName = this.firstName();
  var lastName = this.lastName();
  if (firstName &amp;&amp; lastName) {
    return trim(firstName + ' ' + lastName)
  }
}
</code></pre>
<p>이 공유 라이브러리는 새 목적지 코드를 손쉽게 만들 수 있게 해줬습니다. 친숙함은 머리를 싸매지 않고도 손쉽게 공유된 기능을 사용할 수 있게끔 해주었습니다.</p>
<p>하지만 새로운 문제가 드러나기 시작했습니다. 이 공유 라이브러리에 변화를 주게 되면 모든 목적지에 영향을 끼친다는 것이었습니다. 이로 인해 이 라이브러리를 유지보수할 때 꽤 많은 시간과 신경을 써주어야 했습니다. 수많은 서비스를 테스트하고 배포해야한다는 것을 알고있는 동안 우리가 만든 이 라이브러리에 변화를 주는 것은 매우 위험한 일이었습니다. 시간이 지나, 엔지니어들은 업데이트된 버전의 라이브러리를 하나의 목적지 코드베이스에만 적용하게 되었습니다.</p>
<p>결국 시간이 지난 후, 각기 다른 목적지 코드베이스별로 라이브러리 버전이 갈라지기 시작했습니다. 우리가 각각의 목적지 코드베이스 사이에 커스텀을 줄이는 이점이 뒤집히기 시작했습니다. 결국 모든 목적지 코드는 이 공유 라이브러리의 서로 다른 버전을 사용하고 있었습니다. 우리는 변경사항을 바로바로 적용할 수 있는 자동화 도구를 만들 수 있었지만, 이 시점에서 개발자의 생산성 저하를 겪었을 뿐 아니라 우리는 마이크로서비스 아키텍처로 인한 다른 이슈를 겪기 시작했습니다.</p>
<p>추가적인 다른 문제는 각각의 서비스는 고유의 로드 패턴을 갖고 있었다는 점입니다. 일부 서비스는 하루에 수많은 이벤트를 처리하는 반면 일부 서비스는 초당 수천개의 이벤트를 처리합니다. 적은 수의 이벤트를 처리하는 목적지의 경우, 예상치 못하게 로드가 갑자기 튈 때마다 오퍼레이터가 수동으로 요구하는 로드에 맞게끔 서비스를 스케일업 해줘야 합니다.</p>
<p>우리가 오토 스케일링 구현을 갖고 있었지만, 각각의 서비스는 요구하는 CPU와 메모리 리소스에 맞게끔 구성되어 있었습니다. 그리고 이는 오토 스케일링 구현을 예쁘게만 만들어줬습니다. (more art than science)</p>
<p>목적지의 수가 매우 빠르게 증가하며 팀은 평균적으로 한달에 3개의 목적지를 추가했으며, 이것은 더 많은 레포, 더 많은 큐, 더 많은 서비스를 의미했습니다. 우리의 마이크로 서비스 아키텍쳐와 함께 우리의 운영 오버헤드는 각 목적지 추가에 따라 선형적으로 증가했습니다. 따라서, 우리는 다시 원래대로 돌아가기로 결정했고 모든 파이프라인을 재검토했습니다.</p>
<h2 id="">마이크로서비스와 큐 버리기</h2>
<p>가장 먼저 해야했던 일은 140개가 넘는 마이크로서비스를 1개의 마이크로서비스로 통합하는 일이었습니다. 이 모든 마이크로서비스를 관리하는 오버헤드는 우리 팀에겐 너무나도 컸습니다. 우리는 로드가 튀는 것을 지켜보며 항시 대기해야 하는게 당연했기 때문에 말 그대로 잠도 제대로 자지 못했습니다.</p>
<p>하지만, 그 당시에 아키텍처를 하나의 서비스로 옮기는 작업은 도전에 가까웠습니다. 목적지마자 분리된 큐를 사용함에 따라 각각의 워커는 모든 큐에 작업이 들어왔는지 체크해야 했고 결국 복잡하지 않은 목적지 서비스에도 복잡성을 추가했습니다. 이는 <a href="https://segment.com/blog/introducing-centrifuge/">Centrifuge</a>의 가장 큰 영감이 되었습니다. Centrifuge는 모든 각각의 큐를 교체하고 이벤트를 하나의 모놀리식 서비스에 전송하는 것을 담당합니다.</p>
<p><img src="https://assets.contents.io/asset_a0ViVzT6.png" alt="[번역] 잘가요 마이크로서비스: 100개의 문제점 투성이를 1개의 슈퍼스타로"></p>
<h2 id="">하나의 모노레포로 옮기기</h2>
<p>오직 하나의 서비스가 있다는 것을 감안하면, 목적지 서비스 코드를 하나의 레포로 옮기는 것은 합리적인 일입니다, 이것은 모든 다른 의존성과 테스트를 하나의 레포로 묶는 것을 의미하기도 하죠. 우리는 이게 엄청나게 지저분하게 진행될 것이란 것을 알고 있었습니다.</p>
<p>120개의 각기 다른 의존성에 대해 우리는 모든 목적지에서 오직 하나의 버전을 사용하기로 결정했습니다. 우리가 목적지를 옮김에 따라 우리는 해당 목적지가 사용하고 있었던 의존성의 버전을 체크하고 최신 버전으로 올려야 했습니다. 우리는 새로운 버전의 의존성에서 깨지는 모든 문제를 해결했습니다.</p>
<p>이러한 변화와 함께 우리는 더이상 의존성 버전간의 차이를 추적할 필요가 없어졌습니다. 모든 우리의 목적지 코드는 같은 버전을 사용했고, 이는 코드베이스간의 복잡도를 엄청나게 줄여주었습니다. 목적지 코드를 관리하는 일은 이제 더 적은 시간을 소모했고 더 적은 리스크를 요구했습니다.</p>
<p>우리는 또한 모든 목적지에 대한 테스트가 빠르고 쉽게 돌아가게끔 되기를 원했습니다. 모든 테스트를 돌리는 일은 앞서 설명한 내용대로 공유 라이브러리를 배포할 때 가장 걸리적거리는 일중 하나였습니다.</p>
<p>운좋게도, 모든 목적지 테스트 코드는 비슷한 구조를 가졌습니다. 그것들은 커스텀 변환 로직이 맞는지, 또한 HTTP 요청을 파트너의 엔드포인트로 보내 이벤트가 목적지에 예상했던 대로 나타나는지 검증하기 위한 간단한 유닛 테스트를 가졌습니다.</p>
<p>회상해보면 각각의 목적지 코드베이스를 각각의 레포로 나누는 원래의 목적은 테스트 실패 케이스를 나누는 것이었습니다. 하지만 이는 잘못된 점인 것으로 밝혀졌습니다. HTTP 요청을 만드는 테스트는 여전히 적은 빈도로 실패합니다. 각각의 레포로 나뉜 목적지 코드로 인해 실패하는 테스트를 고쳐야겠다는 동기는 매우 적었습니다. 이러한 열약한 환경은 기술 채무를 끊임없이 만들어냈습니다. 종종 1~2시간밖에 걸리지 않는 작은 변화가 모습을 드러내려면 며칠에서 일주일 정도까지 소요될 겁니다.</p>
<h2 id="">회복력있는 테스트 만들기</h2>
<p>테스트 도중 목적지 엔드포인트로 나가는 HTTP 요청은 테스트 실패의 주 원인이었습니다. 만료된 인증 토큰 등의 관련없는 이슈는 테스트를 실패하게 만들어선 안됩니다. 우리는 또한 몇몇의 목적지 엔드포인트는 다른 것들에 비해 훨씬 느리다는 것을 경험으로부터 알게 되었습니다. 몇몇 목적지는 테스트를 돌리기 위해 5분 넘게 소요되기도 했습니다. 140개가 넘는 목적지로 인해 테스트는 한시간까지 소요되기도 했습니다.</p>
<p>이런 문제를 모두 해결하기 위해 우리는 Traffic Recorder를 만들었습니다. Traffic Recorder는  <a href="https://github.com/flickr/yakbak">yakbak</a>을 이용해서 만들었고 목적지 테스트의 트래픽을 기록하고 저장하는 일을 했습니다. 테스트가 처음 실행될 때마다 모든 요청과 그에 맞는 응답은 한 파일에 기록됐습니다. 이어서 테스트가 실행될 때 요청을 목적지 엔드포인트에 다시 전송하는 대신 파일에 있는 요청과 응답이 다시 실행됐습니다. 이 파일들은 레포에 포함되어 테스트가 지속적으로 모든 변경에 일관적으로 유지되게 했습니다. 이제 테스트가 인터넷을 통한 HTTP 요청에 더이상 의존하지 않게됨에 따라 테스트가 훨씬 더 탄력적으로 실행될 수 있게 되었고, 싱글 레포로 옮길 수 있게끔 되었습니다.</p>
<p>저는 처음으로 모든 목적지에 대해 테스트를 돌릴 때와 Traffic Recorder를 도입한 후를 기억하고 있습니다. Traffic Recorder의 도입으로 140개가 넘는 모든 목적지에 대해 테스트를 돌리는데 겨우 몇 밀리초밖에 소요되지 않습니다. 과거에는 오직 하나의 목적지가 몇 분 이상 잡아먹기도 했습니다. 이건 마치 마법처럼 느껴졌습니다.</p>
<h2 id="whyamonolithworks">Why a Monolith works</h2>
<p>모든 목적지 코드베이스가 한 레포에 들어가게 됨에 따라 이들 모두는 한 서비스로 합쳐졌습니다. 모든 목적지가 한 서비스에 모이면서 개발자들의 생산성은 상당히 많이 향상됐습니다. 우리는 더이상 공유 라이브러리 중 하나를 바꿈에 따라 140개 이상의 서비스를 재배포할 필요가 없어졌습니다. 한 엔지니어가 서비스를 몇 분 안에 배포할 수 있었습니다.</p>
<p>증거는 향상된 속도였습니다. 2016년, 우리가 구성한 마이크로서비 아키텍쳐가 여전히 있었을 당시 우리는 공유 라이브러리에 32회의 개선작업을 진행했습니다. 올해 우리는 46개의 개선작업을 진행했습니다. 우리는 지난 6개월간 2016년 전체에 한 것보다 더 많은 개선을 진행했습니다.</p>
<p>이러한 변화는 우리의 운영 스토리에도 이득이 되었습니다. 모든 목적지가 한 서비스에서 돌아가기 때문에 우리는 CPU와 메모리를 많이 요구하는 목적지를 잘 조합하여 서비스 요구사항에 맞게 엄청나게 쉽게 서비스를 스케일링 할 수 있었습니다. 매우 큰 워커 풀은 로드가 엄청나게 튈 때 대응할 수 있게 되어 우리는 더이상 작은 양의 로드를 처리하는 목적지에 대해 모니터링할 필요가 없어졌습니다.</p>
<h2 id="">트레이드오프</h2>
<p>우리의 마이크로서비스 아키텍처를 하나의 모놀리식으로 옮겨가는 모든 전반적인 일은 매우 큰 향상입니다, 하지만 몇몇 트레이트오프가 존재합니다:</p>
<ol>
<li><strong>장애 격리는 어렵습니다.</strong> 모든 것이 모놀리식으로 돌아가기 때문에 만약에 한 목적지에서 어떤 버그가 발생했을 때 전체 서비스를 죽게끔 합니다. 우리는 포괄적인 자동화된 테스트를 진행하고 있지만 테스트는 그 당시까지만 커버합니다. 우리는 현재 모놀리식을 유지하며 한 목적지가 전체 서비스를 죽이지 않도록 하는 훨씬 더 강력한 방법을 연구하고 있습니다.</li>
<li><strong>In-memory 캐싱은 덜 효율적입니다.</strong> 이전에는 한 서비스가 하나의 목적지를 담당했기 때문에, 적은 트래픽의 목적지는 적은 프로세스를 요구했습니다. 이는 제어부의 메모리 캐시가 Hot 상태를 유지한다는 것을 의미했습니다. 이제 캐시가 3000개가 넘는 프로세스에 걸쳐 분산됨에 따라 이 캐시를 히트할 확률이 더 낮아졌습니다. 우리는 이 문제를 해결하기 위해 Redis같은 것들을 사용할 수 있겠지만 그렇게 하면 우리가 고려해야 하는 또다른 스케일링 지점이 생기게 됩니다. 결국 우리는 운영상의 이점을 고려하여 캐싱 효율성 저하를 받아들였습니다.</li>
<li><strong>의존성 버전의 업데이트는 여러 목적지를 작동 불능하게 만들 수 있습니다.</strong> 모든것을 한 레포로 옮겨 이전에 있던 의존성 문제를 해결한 반면, 이것이 의미하는 또다른 바는 만약 우리가 라이브러리의 새로운 버전을 사용하길 원한다면 우리는 이 새 버전을 모든 목적지 코드에 적용해야 할 필요가 생깁니다. 하지만 우리 생각으로 이정도는 충분히 트레이드오프할만한 부분이라고 봅니다. 또한 포괄적인 자동화된 테스트로 인해 새로운 의존성으로 인해 어떤 것들이 깨지는지 빠르게 확인할 수 있습니다.</li>
</ol>
<h2 id="">결론</h2>
<p>우리의 초기 마이크로서비스 아키텍쳐는 잠깐동안 제대로 동작했습니다, 그 당시 있었던 성능 이슈를 각각의 목적지를 나눔으로 해결했죠. 하지만 우리는 스케일을 고려하지 못했습니다. 우리는 벌크 업데이트가 필요할 때 테스팅과 배포를 어떻게 해야할지 적절한 방식을 찾지 못했습니다. 그 결과 개발자의 생산성이 매우 빠르게 저하됐습니다.</p>
<p>모놀리식으로 옮기는 작업은 우리에게 운영상의 문제를 파이프라인에서 제거하고 개발자 생산성을 매우 크게 높일 수 있게 해줬습니다. 우리는 이 작업을 가볍게 하지는 않았지만 이게 제대로 될것인지 고려해봐야 한다는 것을 알고 있었습니다.</p>
<ol>
<li>우리는 모든 것을 하나의 레포에 넣기 위해 견고한 테스트가 필요했습니다. 이게 없다면 우리는 처음에 우리가 이 서비스를 나누려 했을 떄와 같은 상황에 닥쳤을 겁니다. 끊임없이 실패하는 테스트는 과거의 생산성을 저하시켰고, 우리는 이런 일이 다시는 발생하지 않기를 원합니다.</li>
<li>우리는 모놀리식 아키텍쳐로 전환했을 때 생길 트레이드오프를 받아들였고 각각에 대해 좋은 스토리를 가지는 것을 확실하게 했습니다. 우리는 이 변화와 함께 온 여러 희생에 대해 받아들여야만 했습니다.</li>
</ol>
<p>마이크로서비스와 모놀리식 사이에서 결정을 할 때 고려해야 할 부분이 여럿 있습니다. 우리의 인프라 중 일부 부분에서 마이크로서비스는 잘 작동하지만 우리의 서버 사이드 목적지에서는 어떻게 이런 인기있는 트렌드가 생산성과 성능을 저하시킬 수 있는지를 보여주는 좋은 예였습니다. 알고 보니, 우리를 위한 솔루션은 모놀리식이었습니다.</p>
<hr>
<p>The transition to a monolith was made possible by Stephen Mathieson, Rick Branson, Achille Roussel, Tom Holmes, and many more.</p>
<p>Special thanks to Rick Branson for helping review and edit this post at every stage.</p>
<hr>
<h2 id="">번역 후 정리 및 개인 견해</h2>
<p>이 글에서 저자는</p>
<ul>
<li>Segment가 제공하는 서비스는 어떤 서비스인지</li>
<li>어떤 이슈가 있었는지</li>
<li>왜 마이크로서비스를 도입하게 됐는지</li>
<li>도입 과정은 어떻게 됐는지</li>
<li>도입 후에 어떤 문제가 생겼는지</li>
<li>문제를 어떻게 해결하려 시도했는지</li>
<li>결국 왜 모놀리식을 선택했는지, 모놀리식으로 돌아온 이후 포기해야 했던 것은 무엇이었는지</li>
</ul>
<p>를 설명하고 있습니다. 요약 겸 차근차근 짚어보죠.</p>
<h3 id="segment">Segment가 제공하는 서비스</h3>
<p>글로 이해하기로는 Segment는 개발자가 원하는 이벤트(데이터)를 Segment 서비스로 모아 특정 웹훅(글에서 설명하는 '목적지')으로 보내주는 역할을 합니다.</p>
<h3 id="">어떤 이슈가 있었나</h3>
<p>Segment에서 만든 이 서비스는 각 이벤트가 들어올 때마다 큐로 보내고(publish) 다른 쪽에서는 큐에서 이벤트를 꺼내(consume) 특정 엔드포인트('목적지')로 재전송하는 구조로 되어 있었습니다. 이때 문제가 발생했던 부분은</p>
<ul>
<li>목적지로 가는 요청은 항상 성공하지 않는다</li>
<li>실패한 요청을 재시도하는 루틴이 있는데, 계속해서 실패하는 요청의 경우 다시 시도해도 또 실패할 확률이 매우 높기 때문에 이런 경우 head-of-line blocking 문제가 생겨 큐가 잠겨버리고 다른 요청을 처리할 수 없다</li>
<li>모든 목적지에 대한 테스트를 만들었는데 한 목적지 테스트가 실패하는 경우 전체 테스트 실패로 이어졌다</li>
</ul>
<p>는 것이었습니다.</p>
<h3 id="">왜 마이크로서비스를 도입하게 되었는지</h3>
<p>글 초반부에는 '시대를 따라가며' 마이크로서비스를 도입했다고 간략하게 이야기하지만 실제 마이크로서비스 도입 계기에는 다음과 같은 것들이 있었습니다.</p>
<ul>
<li>테스트 실패의 분리</li>
<li>실패하는 요청으로 인해 발생하는 head-of-line blocking 문제 격리</li>
</ul>
<p>사실 글에서 마이크로서비스를 이래서 도입했다 할만한 마땅한 해답이 있지는 않습니다. 정말로 초반에 말했던대로 트렌드를 쫒다가 마이크로서비스를 도입하게 된 것으로 보입니다.</p>
<h3 id="">도입 과정은 어떻게 됐는지</h3>
<p>일단은 실패하는 테스트를 분리하여 격리시키기 위해 레포를 분리했습니다. 이후에 분리된 레포에서 공용으로 사용되는 코드를 다루기 위해 공유 라이브러리를 여럿 만들었고 이를 각각의 서비스 레포에 외부 패키지 형태로 포함시켰습니다.</p>
<p>마이크로서비스 도입이 그렇게 꼭 필요한 상황은 아니었다고 보기 때문에 도입 과정 또한 간단하게만 느껴집니다.</p>
<h3 id="">도입 후에 어떤 문제가 생겼는지</h3>
<ul>
<li>각각의 서비스별로 요구하는 스펙이 달랐기 때문에 100개가 넘어가는 각각의 서비스를 스케일링하는 것은 매우 어려웠다고 이야기합니다.</li>
<li>공유 라이브러리의 관리가 어려워졌습니다. 모든 곳에서 필요한 하나의 공유 라이브러리를 업데이트하면 100개가 넘는 모든 서비스의 코드를 업데이트해야만 했습니다.</li>
<li>목적지가 추가됨에 따라 서비스, 레포, 큐가 하나씩 하나씩 선형적으로 늘어나게 되었습니다. 이는 운영 비용이 늘어나는 결과를 초래했고 결국 운영 팀은 이렇게 된 상황을 관리하는데 모든 시간을 써야만 했다고 이야기합니다.</li>
</ul>
<h3 id="">문제를 어떻게 해결하려 시도했는지</h3>
<p>너무나도 간단하게 '그냥 마이크로서비스를 걷어버리자'는 의견으로 모였습니다. 복잡한 큐를 없애고, 분리되어 있었던 모든 코드를 한 레포로 다시 합쳤습니다.</p>
<h3 id="">결국 왜 모놀리식을 선택했는지, 포기해야 했던 것은 무엇이었는지</h3>
<p>트레이드오프, 결론 파트를 확인해보시면 됩니다.</p>
<hr>
<p>글에 몇가지를 짚고 싶습니다.</p>
<ul>
<li>일단 기존 구조적인 문제입니다. 초기 구조상으로도 충분히 blocking되지 않도록 서비스를 만들 수 있었다고 보는 입장에서, 하나의 테스트가 기능이 전체 서비스에 영향을 끼치는 구조를 놓고 이러한 문제를 해결하기 위해 마이크로서비스를 도입했다는 것은 이해가 되지 않는 부분입니다.</li>
<li>많은 회사들이 마이크로서비스를 하고 있습니다. 마이크로서비스를 도입할 때 공유 라이브러리를 어떻게 관리해야 하는지, 마이크로서비스의 스케일링 및 orchestration은 어떻게 해야하는지에 대한 best practice는 이미 많이 나와 있습니다. 많은 사람들이 사용하는 netflix 또한 마이크로서비스를 주도하는 선두주자이고 관련된 많은 정보를 공유하고 있습니다. 이렇게 폭넓고 깊은 사례, 사용 예가 나와있는 아키텍쳐를 이 글에서 설명하는 만큼까지만 활용할 수 없었나 하는 생각이 듭니다.</li>
</ul>
<p>(작성중)</p>
]]></content:encoded></item><item><title><![CDATA[Node.js v10.5.0 Worker PR FAQ]]></title><description><![CDATA[<blockquote>
<p>이 글은 nodejs 10.5.0에 실험적 기능(experimental feature)으로 추가된 worker의 PR FAQ를 정리한 글입니다. 원문은 <a href="https://gist.github.com/benjamingr/3d5e86e2fb8ae4abe2ab98ffe4758665">https://gist.github.com/benjamingr/3d5e86e2fb8ae4abe2ab98ffe4758665</a> 에서 확인하실 수 있습니다.</p>
</blockquote>
<h1 id="workerprfaq">Worker PR FAQ</h1>
<p>The content here is mostly copied material written by Anna (@addaleax) and people on the PR and copied here</p>]]></description><link>https://tech.ssut.me/node-js-v10-5-0-worker-pr-faq/</link><guid isPermaLink="false">5b321b62e2a96873632ed494</guid><category><![CDATA[nodejs]]></category><category><![CDATA[v8]]></category><category><![CDATA[performance]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Tue, 26 Jun 2018 13:24:59 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/06/1200px-Node.js_logo.svg.png" medium="image"/><content:encoded><![CDATA[<blockquote>
<img src="https://tech.ssut.me/content/images/2018/06/1200px-Node.js_logo.svg.png" alt="Node.js v10.5.0 Worker PR FAQ"><p>이 글은 nodejs 10.5.0에 실험적 기능(experimental feature)으로 추가된 worker의 PR FAQ를 정리한 글입니다. 원문은 <a href="https://gist.github.com/benjamingr/3d5e86e2fb8ae4abe2ab98ffe4758665">https://gist.github.com/benjamingr/3d5e86e2fb8ae4abe2ab98ffe4758665</a> 에서 확인하실 수 있습니다.</p>
</blockquote>
<h1 id="workerprfaq">Worker PR FAQ</h1>
<p>The content here is mostly copied material written by Anna (@addaleax) and people on the PR and copied here by myself (@benjamingr). Big Props to @AyushG3112 for a lot of these questions.</p>
<p>Corrections, improvements and suggestions from anyone are welcome.</p>
<h2 id="qpr">Q: 이 PR에는 무엇이 포함되어 있나요?</h2>
<p><strong>A:</strong> 이 PR은 Node.js에 스레딩 지원을 추가합니다. 여기에는 표준화된 공유 메모리와 락(mutex)이 포함됩니다.</p>
<h2 id="q">Q: 워커? 스레드?</h2>
<p><strong>A:</strong> 지금은 하나의 워커가 하나의 스레드를 사용하고 각 스레드가 하나의 워커입니다. 나중에는 <code>1:1</code> 스레딩 모델 대신에 <code>n:m</code> 모듈을 도입할 수도 있습니다. 원본 PR에는 유저랜드 라이브러리가 이것을 가능케 하는 것을 포함하고 있습니다.</p>
<h2 id="qprcpu">Q: 왜 이렇게 하나요? 왜 이 PR은 CPU 중점 작업에만 추천하고 있나요?</h2>
<p><strong>A:</strong> 가장 큰 목적은 일반적인 Node.js 작업을 하는데 쓰는게 아니고, CPU를 많이 사용하는 작업을 다른 스레드로 분리하도록 하는 것입니다. 일반적으로 모든 libuv 기반 API 사용이 가능하지만, 우리가 가장 피해야 하는 것은 사용자가 많은 스레드를 생성해서 각각의 스레드가 동기 작업을 하는 것을 돕는겁니다 – 제 생각으로 이 방식은 Node.js의 목적과는 어긋나다고 생각합니다. ?</p>
<p>단언컨대 Node.js는 &quot;코드가 블락되지 않고 돌아가기 때문에&quot; 인기를 끌 수 있게 되었다고 봅니다, I/O 작업을 위해 스레드를 사용할 필요가 전혀 없죠 (스레드의 직접 사용은 어렵습니다), 하지만 대신 콜백을 지원하는 비동기 API를 갖고 있습니다.</p>
<h2 id="qv8isolatev8libuv111">Q: 워커의 플랫폼, V8 Isolate, V8 환경, libuv와 메인 스레드는 어떤 관계인가요? 1:1 인가요 아니면 1:다 인가요?</h2>
<p><strong>A:</strong> 이 PR에서:</p>
<ul>
<li>플랫폼은 프로세스 스코프를 갖습니다.</li>
<li>각 스레드는 1:1로 Isolate, IsolateData, Environment, uv_loop_t의 튜플과 대응합니다.</li>
</ul>
<p>일반적으로 각각의 Isolate가 여러개의 환경을 갖거나, 각각의 uv_loop_t가 여러 환경을 갖는 것은 생각해볼 수 있는 부분입니다, 하지만 우리는 여기서 이 부분을 구현하진 않았습니다.</p>
<h2 id="qenviron">Q: 실행 환경: 부모의 환경(environ **)이 워커에 복제되거나 사용될 수 있나요?</h2>
<p><strong>A:</strong> env 값에 대해 이야기한다고 가정해보자면: env는 (적어도 유닉스 시스템에선)프로세스별로 할당됩니다, 따라서 워커는 메인 스레드가 env를 사용할 때 방해하지 않도록 읽기 전용의 복사본을 갖게 됩니다.</p>
<h2 id="qsyncapiracecondition">Q: 스레딩이 *Sync API와의 race condition을 만들진 않나요?</h2>
<p><strong>A:</strong> 스레드와 *Sync API 사이에 race condition이 생길 수 있는 점은 충분히 가능한 부분입니다, 하지만 그것들을 비동기 API로부터 만드는 것과 아무 차이가 없습니다.</p>
<h2 id="q">Q: 어떤 객체가 스레드끼리 공유될 수 있고, 어떻게 하나요?</h2>
<p><strong>A:</strong> 스레드간 통신은 대체로 <a href="https://developer.mozilla.org/en-US/docs/Web/API/MessageChannel">MessageChannel</a> Web API로 만듭니다. <code>ArrayBuffer</code>를 전송하는 것과 <code>SharedArrayBuffer</code>를 통해 메모리를 공유하는 것이 지원됩니다.</p>
<h2 id="qmutexsemaphore">Q: mutex/semaphore와 같은 것들을 사용할 수 있나요?</h2>
<p><strong>A:</strong> 멀티 프로세스를 쓸 때와 차이가 <em>아예</em> 없습니다! :) 이 PR은 <code>SharedArrayBuffer</code>를 지원하기 때문에 우리는 <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Atomics/wait"><code>Atomics.wait()</code>와 <code>Atomics.wake()</code></a>를 이용해 pure JS에서 실제 mutex나 다른 동기화를 위한 요소를 구현할 수 있습니다.</p>
<h2 id="qpr">Q: 이 PR로 동기 블록이 가능한가요? &quot;진짜&quot; 뮤텍스를 제공하나요?</h2>
<p><strong>A:</strong> 네, 그게 실제로 <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Atomics/wait">Atomics.wait</a>이 하는 일입니다. 모든 사람이 이걸 좋아하진 않지만 적어도 이런 문제를 해결할 수는 있을거고 이 구현과 함께 사용될 수 있습니다. :)</p>
<h2 id="qglobals">Q: globals는 스레드끼리 공유되나요?</h2>
<p><strong>A:</strong> globals는 워커별로 지정됩니다. 이것들은 JS 객체이기 때문에 각각의 독립적인 heap 공간에 상주합니다, 이것이 의미하는 것은 globals는 현재 V8 디자인으로서는 공유될 수 없고, 하나의 global 객체를 수정해도 다른 곳에 영향을 끼치지 않는다는 점입니다.</p>
<h2 id="qcluster">Q: cluster처럼 스레드간 서버 소켓 공유가 가능한가요?</h2>
<p><strong>A:</strong> 네, (우리가 자식 프로세스에서 하는 것과 비슷하게 IPC 메커니즘을 다루는 것과는 반대로) libuv가 이벤트 루프간 핸들을 전송하는 것을 지원하는 것이 더 좋겠지만, 이렇게 하는게 어려운 일은 아닙니다, 하지만 초기 PR의 스코프 밖입니다.</p>
<h2 id="qlisten">Q: 두개의 스레드가 동시에 같은 포트로 listen하려고 하면 어떤 일이 일어나나요?</h2>
<p><strong>A:</strong> 두 개의 프로세스가 같은 포트로 listen하려고 할 때와 동일한 일이 발생합니다. – 예외가 발생하죠.</p>
<h2 id="q">Q: 모듈은 어떻게 불리나요?</h2>
<p><strong>A:</strong> 제안을 환영합니다, 현재(작업중)는 <code>worker</code>로 부르고 있습니다만 모두가 이 이름을 좋아하지 않고 저 패키지의 소유자가 Node.js에서 같이 작업하는 것에 관심이 있지도 않고 이름을 주지도 않기 때문입니다. 이 부분은 아직 논의중인 사항입니다.</p>
<h2 id="q">Q: 코드와 논의중인 사항은 어디에 있나요?</h2>
<p><strong>A:</strong> <a href="https://github.com/nodejs/node/pull/20876#issuecomment-391734861">여기에 코드와 changes가 있습니다</a></p>
<h2 id="q">Q: 스레드가 메인 스레드인지 확인할 수 있나요?</h2>
<p><strong>A:</strong> <code>require('worker').isMainThread</code>로 확인할 수 있습니다.</p>
<h2 id="q">Q: 왜 상대경로나 함수를 워커에 넘길 수 없나요?</h2>
<p><strong>A:</strong> 현재 워커는 절대경로를 필요로 합니다. <code>__filename</code>과 <code>__dirname</code>을 사용해 상대경로를 만들 수 있습니다. 이 부분은 강한 제한은 아니지만 이 PR에서 스코프를 제한하는 것은 선택사항입니다.</p>
<p>다른 옵션은 나중에 고려될 예정입니다.</p>
<h2 id="quncaughtexception">Q: 워커에서 uncaught exception이 발생하면 어떻게 작동하나요?</h2>
<p><strong>A:</strong> 현재는 해당 상황에서 워커 스레드만 멈추기 되고 메인 스레드에 있는 워커 객체에 <code>error</code> 이벤트가 발생하게 됩니다. 예외가 unhandled라 하더라도 메인 스레드를 멈추게 하지 않습니다.</p>
<p>(<code>test/parallel/test-worker-uncaught-exception.js</code> 코드에서 이 동작을 테스트해볼 수 있습니다 – 만약 워커에서 발생한 예외가 부모까지 멈추게 한다면 이벤트 리스너라 하더라도 버그이니 저에게 바로 알려주세요!) :)</p>
<h2 id="qchild_process">Q: 네이티브 애드온이 작동하나요? 클러스터? <code>child_process</code>?</h2>
<p><strong>A:</strong> 아직은 안됩니다, 하지만 향후에 가능하게끔 계획되어 있습니다. 워커에서 자식 프로세스와 클러스터를 띄우는 것은 가능합니다.</p>
<h2 id="qp2p">Q: 메시징: 워커와 메인간 통신인가요? 또는 워커끼리도 되나요? 브로드캐스트 채널인가요 아니면 P2P 채널인가요?</h2>
<p><code>MessageChannel</code> API의 구현부는 1:1 모델을 따릅니다, 따라서 브로드캐스팅은 가능하지 않습니다. 채널은 부모와 자식 스레드 사이에 있지만, 새로운 채널을 만들고 그것들을 존재하는 채널을 통해 전송할 수 있기 때문에 워커간 메시지를 원한다면 할 수 있습니다.</p>
<h2 id="qsignal">Q: 워커가 signal을 받을 수 있나요?</h2>
<p><strong>A:</strong> 아니요, 우연하게도 불가능합니다, signal은 프로세스 단위 정보이기 때문에 비활성화 되어있습니다. 만약 이게 좋은 아이디어라고 생각된다면 우리가 이걸 수정할 순 있겠죠.</p>
<h2 id="qnodeinspector">Q: node inspector가 돌아가나요? 크롬 개발자도구로 워커를 디버깅할 수 있나요?</h2>
<p><strong>A:</strong> 처음엔 안됐습니다, 하지만 높은 우선순위로 개발 예정입니다. 도움이 필요합니다.</p>
<h2 id="q">Q: 리소스: 지금 시점에서 직접 다룰 수 있는 워커 스레드의 어떤 네이티브 속성이 사용 가능한가요?</h2>
<p><strong>A:</strong> 현재: 아무것도 안됩니다. :) <a href="https://github.com/addaleax/node/commit/9a725557d922">addaleax/node@9a72555</a>에서 힙 사이즈를 제한하는 몇몇 아이디어가 나왔지만, V8의 API가 이 시점에서 좋은 에러 핸들링을 허용할 거라고 보지 않습니다. 따라서, 저는 이걸 PR에 포함시키지 않았습니다. 이건 결국 제가 원하던 바 였음에도 불구하고요.</p>
<p>사용 가능한 스택의 크기는 현재 V8에서 프로세스 스코프입니다, 따라서 저는 이 시점에서 제한되는 어떤 점이 있다고 생각하지 않습니다.</p>
<h2 id="qasync_hook">Q: async_hook과는 어떻게 동작하나요?</h2>
<p><strong>A:</strong> async hooks의 동작은 원래대로 동작합니다, 차이점이 있다면 추가적인 빌트인 객체로 인해 몇몇 async ID가 달라지게 됩니다 (예시. 메인 스크립트의 execution ID). 만약 사용자가 완전히 같은 값에 의존하지 않게 허용한다면 문제되지 않습니다.</p>
<p>PR의 마지막 커밋에서 테스트를 확인해보세요. – 슬프게도 많은 async_hook 테스트가 세부적인 부분에 과하게 의존하고 있는 탓에 지금은 Worker에서 스킵되고 있습니다, 하지만 일반적으로 async_hooks 테스트는 다른 것들과 비슷하게 돌아갑니다.</p>
<h2 id="qprocessmemoryusage">Q: process.memoryUsage() 리포트는 어떻게 나오나요? 워커 단위로 나올 것 같은데 힙 합산으로 나오나요?</h2>
<p><strong>A:</strong> rss는 프로세스 단위입니다. 나머지 값은 Isolate/워커 단위로 나옵니다. 이에 관해 문서에 기록해뒀습니다. :)</p>
<h2 id="qasynchooksasyncresource">Q: AsyncHooks이 각 워커마다 독립되어 있는데, 그럼 워커마다 커뮤니케이션을 맞출 수 있는 방법이 있나요? 아니면 메타데이터를 워커간에 전송하고 AsyncResource를 만들어서 필요에 따라 사용하는 것이 사용자에게 달려있나요?</h2>
<p><strong>A:</strong> 네, 각 워커는 독립적인 AsyncHooks의 셋을 가집니다. 이 시점에서, 정보와 같은 것들을 수동으로 커뮤니케이션 해야 합니다. (예로 다른 MessageChannel을 통해)</p>
<p>우리는 이걸 위한 내장 유틸리티를 구현할 수 있었습니다, 하지만 해봐야 확실하게 강력하진 않을겁니다, 왜냐하면 이 기능은 완전히 비동기로 작동해야 하고 따라서 이것은 워커쪽으로부터 간단한 정보만 읽을 수 있습니다. 객체를 inspect할 수 있다던가 하는 대신에 말이죠.</p>
<h2 id="qhttp">Q: 예로 누군가 http 서버를 여러 워커에서 만든다던가 하는 것을 금지할 계획이 있나요?</h2>
<p><strong>A:</strong> 아니요, 그리고 저 개인적으로 기술적으로 제한을 할 이유가 없는데도 사용자가 무엇을 하는 것을 인위적으로 제한하는 것은 좋지 않은 것이라 생각합니다.</p>
<p>하지만, 우리는 사용자에게 동기 I/O 코드를 만들고 워커에게 떠넘기는 방식 등에 대해 경고할 필요가 있습니다 – 이것은 아마 그 누구도 돕진 않을겁니다.</p>
<h2 id="qchild_processcluster">Q: 이 구현과 child_process/cluster는 무엇이 다른가요?</h2>
<p><strong>A:</strong></p>
<p>워커는 개념상으로는 <code>child_process</code>나 <code>cluster</code>와 매우 유사합니다. 몇몇 눈에 띌만한 차이점은 다음과 같습니다:</p>
<ul>
<li>워커간 커뮤니케이션은 어렵습니다: <code>child_process</code> IPC와는 다르게 JSON을 사용하지 않습니다, 하지만 브라우저에서 <code>postMessage()</code>하는 것과 동일한 로직을 사용합니다.
<ul>
<li>이걸 더 최적화 할 수 있고 더 빨라야 하더라도, 이것은 어쩔 수 없이 빠르지 없습니다. (JSON이 얼마나 이곳저곳에서 사용되어 왔는지와 이걸 더 빠르게 하기 위해서 얼마나 노력을 해왔는지를 명심해주시기 바랍니다.)</li>
<li>직렬화된 데이터는 실제로 프로세스를 떠날 필요가 전혀 없습니다, 따라서 결론적으로 커뮤니케이션간에 더 적은 오버헤드가 따릅니다.</li>
<li>typed arrays 형태의 메모리는 전송되거나, 워커/메인스레드 사이에 공유될 수 있습니다, 이는 몇몇 케이스에서 정말로 빠른 통신을 가능하게 해줍니다.</li>
<li>네트워크 소켓과 같은 핸들은 (아직) 전송되거나 공유될 <em>수 없습니다</em>.</li>
</ul>
</li>
<li>worker의 일부 기능(예: <code>process.chdir()</code>)이 프로세스 상태, 네이티브 애드온 로딩 등에 영향을 끼치기 때문에 worker 내부에서 사용할 수 있는 API에는 <em>약간의</em> 제한이 있습니다.</li>
<li>각각의 워커는 고유의 이벤트 루프를 갖습니다, 하지만 리소스중 일부는 워커들 사이에 공유됩니다. (예: 파일시스템 동작을 위한 libuv 스레드 풀)</li>
</ul>
]]></content:encoded></item><item><title><![CDATA[Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다]]></title><description><![CDATA[<blockquote>
<p>이 글은 V8 Javascript Engine 공식 블로그에 올라온 아래 글의 번역글입니다.<br>
<strong>Concurrent marking in V8</strong><br>
<a href="https://v8project.blogspot.com/2018/06/concurrent-marking.html">https://v8project.blogspot.com/2018/06/concurrent-marking.html</a><br>
원글 자체가 쉬우면서 이따금씩 어려운 설명이 나오기 때문에 오역이 있을 수 있습니다. 댓글 또는 이메일로 수정 피드백 남겨주시면 감사하겠습니다. :)</p>
</blockquote>
<h1 id="v8">V8의 지속적인 마킹</h1>
<p>이 글에서는 지속적인 마킹(concurrent</p>]]></description><link>https://tech.ssut.me/concurrent-marking-in-v8/</link><guid isPermaLink="false">5b23cfa6e2a96873632ed47f</guid><category><![CDATA[nodejs]]></category><category><![CDATA[v8]]></category><category><![CDATA[GC]]></category><category><![CDATA[performance]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Fri, 15 Jun 2018 17:19:35 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/06/concurrent-marking-12.png" medium="image"/><content:encoded><![CDATA[<blockquote>
<img src="https://tech.ssut.me/content/images/2018/06/concurrent-marking-12.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><p>이 글은 V8 Javascript Engine 공식 블로그에 올라온 아래 글의 번역글입니다.<br>
<strong>Concurrent marking in V8</strong><br>
<a href="https://v8project.blogspot.com/2018/06/concurrent-marking.html">https://v8project.blogspot.com/2018/06/concurrent-marking.html</a><br>
원글 자체가 쉬우면서 이따금씩 어려운 설명이 나오기 때문에 오역이 있을 수 있습니다. 댓글 또는 이메일로 수정 피드백 남겨주시면 감사하겠습니다. :)</p>
</blockquote>
<h1 id="v8">V8의 지속적인 마킹</h1>
<p>이 글에서는 지속적인 마킹(concurrent marking)이라 불리는 가비지 컬렉션(GC) 기술에 대해 설명해보겠습니다. 이 최적화는 GC가 live object를 입에서 찾고 마킹하는 동안 자바스크립트 앱이 멈추지 않고 계속해서 작동할 수 있도록 하는 최적화입니다. 우리가 직접 해본 벤치마크에서 지속적인 마킹은 메인 스레드에서 마킹하는 시간의 60-70% 정도를 줄여주는 결과로 나타났습니다. 지속적인 마킹은 <a href="https://v8project.blogspot.com/2016/04/jank-busters-part-two-orinoco.html">Orinoco project</a>의 마지막 퍼즐조각입니다 — 이 프로젝트는 오래된 가비지 컬렉터를 거의 동시, 그리고 병렬로 작동하는 새로운 가비지 컬렉터로 서서히 교체하는 프로젝트입니다. 지속적인 마킹은 Chrome 64 및 Node.js v10에서 기본적으로 활성화되어 있습니다.</p>
<h2 id="">배경</h2>
<p>마킹(marking; 식별, 표기)은 V8 <a href="https://en.wikipedia.org/wiki/Tracing_garbage_collection">Mark-Compact</a> GC의 한 부분(단계)입니다. 이 단계에서 컬렉터는 살아있는 모든 객체를 탐색하고 마킹합니다. 마킹은 글로벌 객체나 현재 활성화되어 있는 함수 등의 알려진 살아있는 객체의 셋으로부터 시작됩니다 — roots라고 불리죠. 컬렉터는 roots를 live 상태로 마킹하고 더 많은 살아있는 객체를 찾아내기 위해 해당 객체들의 포인터를 쫒습니다. 이렇게 컬렉터는 계속해서 새롭게 발견된 객체를 마킹하고 더 이상 마킹할 객체가 없을 때까지 포인터를 쫒아갑니다. 마킹이 끝나면, heap에서 마킹되지 않은 모든 객체는 애플리케이션에서 더이상 접근할 수 없고 안전하게 메모리로 반환될 수 있습니다.</p>
<p>마킹을 <a href="https://en.wikipedia.org/wiki/Graph_traversal">graph traversal</a> 과정이라 생각해봅시다. heap에 있는 객체는 모두 그래프의 노드입니다. 다른 객체를 가리키는 한 객체의 포인터는 그래프의 가장자리 부분입니다. 그래프에 노드가 주어지면 우리는 객체의 숨겨진 클래스를 이용하는 해당 노드의 다른 가장자리 부분을 찾을 수 있습니다.</p>
<p><img src="https://4.bp.blogspot.com/-f-oTLDn3bCo/Wx6DKD_jz8I/AAAAAAAACa8/7NpvPKn60bsn_8GI9KBxd-2ov1hZlqZ2gCLcBGAs/s640/concurrent-marking-0.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
Figure 1. 객체 그래프</p>
<p>V8은 마킹을 두개의 마킹 비트(mark-bits)를 한 객체에 두고 marking worklist를 사용하는 방식으로 구현했습니다. 두개의 마킹 비트(mark-bits)는 다음 세가지 컬러로 나뉩니다: 화이트(00), 그레이(10), 블랙(11). 기본적으로 모든 객체는 화이트(00)입니다, 컬렉터가 아직 발견하지 못했다는 것을 의미하죠. 화이트 객체는 컬렉터가 해당 객체를 발견하고 marking worklist에 넣었을 때 그레이(10) 객체가 됩니다. 그레이 객체는 컬렉터가 마킹 리스트에서 해당 객체를 꺼내고(pop) 해당 객체의 모든 필드를 방문(visits)했을 때 블랙(11) 객체가 됩니다. 이런 스키마를 tri-color 마킹이라 부릅니다. 더이상 그레이 객체가 없다면 마킹은 끝나게 됩니다. 남은 모든 화이트 객체는 접근할 수 없고 안전하게 메모리로 반환될 수 있습니다.</p>
<p><img src="https://2.bp.blogspot.com/-2VyhTfwEFKQ/Wx6DKT4S3EI/AAAAAAAACa4/TdPiHggm-usVLycsAZmAwVB6wCkZPdfnACLcBGAs/s400/concurrent-marking-1.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
Figure 2. roots에서부터 시작되는 마킹.</p>
<p><img src="https://2.bp.blogspot.com/-rhwLWlDz2Xc/Wx6DLQouLVI/AAAAAAAACbI/uYlNcv8BN4MoJ26yANrx6-Asb0CjP8y7ACLcBGAs/s400/concurrent-marking-2.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
Figure 3. 컬렉터가 그레이 객체의 포인터를 프로세싱하여 블랙 객체로 전환하는 모습.</p>
<p><img src="https://2.bp.blogspot.com/-RxvMwYmjG5U/Wx6DMG9YqiI/AAAAAAAACbM/xJ-UfoQF6kMcytERRMIvzEURf05oWJ1NwCLcBGAs/s400/concurrent-marking-3.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
Figure 4. 마킹이 끝난 이후 최종 상태.</p>
<p>한가지 주목해야 할 사실은 위에서 소개한 마킹 알고리즘은 마킹이 작동하는 동안 애플리케이션이 일시중지(pause)된 상태에서만 작동한다는 것입니다. 마킹 도중 애플리케이션이 작동하게 된다면 애플리케이션이 그래프를 바꿀 수 있을 것이고 이는 결국 컬렉터가 모든 살아있는 객체를 소멸시킬 수 있는 위험에 처할 수 있게 됩니다.</p>
<h2 id="pause">마킹에 따른 일시중단(pause) 줄이기</h2>
<p>큰 힙에서 한번에 모든 객체를 마킹하게 된다면 몇백 밀리초가 소요됩니다.<br>
<img src="https://2.bp.blogspot.com/-A9uCDbIZJP4/Wx6DMXUB0GI/AAAAAAAACbQ/yNKBvC76jcwCOITRFCQa2RTvUryGKqmUgCLcBGAs/s640/concurrent-marking-4.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
이렇게 긴 일시중단은 애플리케이션이 응답하지 않거나 사용자 경험을 나쁘게 만들 수 있습니다. 2011년에 V8은 stop-the-world 마킹(작동중 전체중단)을 incremental 마킹(점진적으로 쌓아뒀다가 한번에 처리)으로 전환했습니다. incremental 마킹동안 가비지 컬렉터는 마킹하는 작업을 세분화하여 여러 조각으로 나눈 후 해당 조각들 사이에서(틈에서) 앱이 돌아갈 수 있도록 했습니다.<br>
<img src="https://2.bp.blogspot.com/-NhR_qfGAvAE/Wx6DMzspN2I/AAAAAAAACbU/Lqa_fdiN4_cV3TpEc1xBbMNdhDqaMDUIwCLcBGAs/s640/concurrent-marking-5.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
가비지 컬렉터는 애플리케이션에서 할당되는 비율에 맞게끔 각 조각에서 incremental 마킹이 얼마나 작동할지를 결정합니다. 일반적인 상황에서 이런 방식은 애플리케이션의 응답성을 엄청 크게 향상시켜 줍니다. 하지만 메모리 부하가 큰 곳에 올라가있는 큰 힙에선 컬렉터가 alloc을 유지하려 함에 따라 여전히 긴 시간의 중단이 발생할 수 있습니다.</p>
<p>incremental 마킹은 공짜가 아닙니다. 애플리케이션은 객체 그래프를 바꿀 수 있는 모든 행동에 대해 가비지 컬렉터에게 알려야만 합니다. V8은 이 알리는 과정을 다익스트라 스타일의 write-barrier를 이용해 구현했습니다. <code>object.field = value</code>와 같은 자바스크립트 코드가 실행되어 쓰기 작업이 끝난 이후 V8은 다음과 같은 write-barrier 코드를 삽입합니다:</p>
<pre><code class="language-c">// `object.field = value` 이후에 호출됩니다.
write_barrier(object, field_offset, value) {
  if (color(object) == black &amp;&amp; color(value) == white) {
    set_color(value, grey);
    marking_worklist.push(value);
  }
}
</code></pre>
<p>이런 write-barrier는 블랙 객체가 흰색 객체를 가리키지 않도록 강제(보장)합니다. 이런 방식은 강한 불변(invariant) tri-color라 부르기도 하며, 애플리케이션이 가지비 컬렉터로부터 살아있는 객체를 숨기지 못하도록 보장하여 마킹이 끝났을 때 모든 화이트 객체가 확실하게(truly) 애플리케이션에서 접근될 수 없고 안전하게 메모리로 반환될 수 있게끔 합니다.</p>
<p>incremental 마킹은 <a href="https://v8project.blogspot.com/2015/08/getting-garbage-collection-for-free.html">앞서 작성한 블로그 포스트</a>에서 설명했던 유휴 시간 가비지 컬렉터(idle time garbage collection)와 함께 유연하게 작동합니다. Chrome의 Blink 작업 스케쥴러(task scheduler)는 아무런 문제 없이 메인 스레드에서 유휴 시간동안 작은 incremental 마킹 작업을 스케쥴 할 수 있습니다. 이 최적화는 유휴 시간(idle time)만 있다면 매우 잘 작동합니다.</p>
<p>write-barrier 비용(cost)으로 인해 incremental 마킹은 애플리케이션의 스루풋(throughput)을 감소시킬 수도 있습니다. 하지만 워커 스레드를 더 두어 스루풋과 중단 시간 모두 개선할 수 있기도 합니다. 워커 스레드에서 마킹을 하는 방법에는 다음 두가지 방법이 있습니다: 병렬 마킹(parallel marking), 동시 마킹(concurrent marking).</p>
<p><strong>병렬</strong> 마킹은 메인 스레드와 워커 스레드에서 발생합니다. 병렬 마킹동안 애플리케이션은 잠시 중단됩니다. 멀티 스레드 버전의 stop-the-world 마킹이라고 볼 수 있죠.<br>
<img src="https://2.bp.blogspot.com/-OkLv_w_7EpQ/Wx6DNTMe8hI/AAAAAAAACbY/xEHeglE_O4gh9J3QleBgTt71NfP-_SYqwCLcBGAs/s640/concurrent-marking-6.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"></p>
<p><strong>동시</strong> 마킹은 대부분 워커 스레드에서 발생합니다. 동시 마킹이 작동중일 때 애플리케이션은 계속해서 작동할 수 있습니다.<br>
<img src="https://3.bp.blogspot.com/-d7EzTYssdH0/Wx6DN9jdJXI/AAAAAAAACbc/eeO7asRATQ4KydCGNOCeZAeD_RnS_ebtQCLcBGAs/s640/concurrent-marking-7.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"></p>
<p>이제 나오는 다음 두 섹션에서 어떻게 우리가 병렬 및 동시 마킹을 V8에 구현했는지에 대해 설명합니다.</p>
<h2 id="parallelmarking">병렬 마킹 (parallel marking)</h2>
<p>병렬 마킹이 작동하는 동안 우리는 애플리케이션이 같은 시간동안 작동하지 않을 거라고 가정할 수 있습니다. 이런 가정은 (마킹동안)객체 그래프가 변하지 않을 것을 내포하고 있기 때문에 구현을 상당히 쉽게 합니다. 병렬로 객체 그래프를 마킹하기 위해 우리는 우선 가비지 컬렉터 자료구조를 thread-safe하게 만들고 각 스레드간 효율적으로 마킹을 공유할 수 있는 방법을 찾아야 했습니다. 다음 다이어그램은 병렬 마킹과 관련된 자료구조입니다. 화살표는 데이터의 방향을 나타냅니다. 간단하게 설명하기 위해 다이어그램에서 힙 조각모음(heap defragmentation)에 필요한 자료구조는 생략하였습니다.</p>
<p><img src="https://2.bp.blogspot.com/-43b8L8vIa5c/Wx6DOUPWdFI/AAAAAAAACbg/0Lu-Tf_rkGIJFFH09i_PFsP1QlUtqRHugCLcBGAs/s640/concurrent-marking-8.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
Figure 5. 병렬 마킹을 위한 자료구조</p>
<p>여기서 알아두어야 할 사실은 각 스레드는 객체 그래프로부터만 읽기만 하고 객체 그래프를 변경하지 않습니다. 객체의 mark-bits와 marking worklist는 읽기 및 쓰기 권한 모두 있어야 합니다.</p>
<h2 id="worklistworkstealing">worklist 마킹하기 및 work 훔치기(stealing)</h2>
<p>marking worklist의 구현은 서로 다른 스레드의 작업량을 잘 조율하여 성능 및 밸런스를 조절하는데 매우 중요합니다.</p>
<p>이 교환 공간에서 고려해야 하는 가장 큰 부분은 (a) 모든 객체가 잠재적으로 공유될 수 있다는 부분에 따라 최고의 공유(best sharing)를 위해 완전한 동시성 자료구조 사용 및 (b) 스레드-로컬 스루풋의 최적화를 위해 어떤 객체도 공유할 수 없는 완전히 분리된 스레드-로컬 자료구조 사용입니다. Figure 8은 어떻게 V8이 스레드-로컬 삽입 및 제거를 위한<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> 세그먼트에 기반한 marking worklist를 사용해 이러한 요구에 따른 밸런스를 조절하는지를 보여줍니다. 만약 한 세그먼트가 가득차게 되면, 다른 세그먼트가 가져가서 작업을 처리할 수 있도록 공유된 글로벌 풀에 마킹 작업이 이동(publish and steal)하게 됩니다. 이리하여 V8은, 다른 스레드가 해당 스레드가 가진 로컬 세그먼트를 모두 처리하여 비게되는 동안 한 싱글 스레드가 객체의 새로운 서브 그래프(sub-graph of objects)에 도달한 케이스를 여전히 처리할 수 있다면 마킹 스레드가 어떠한 동기화 작업도 없이 로컬에서 처리할 수 있도록 합니다.<br>
<img src="https://2.bp.blogspot.com/-7htYmmE6xdU/Wx6DO6Vpq0I/AAAAAAAACbk/BnsxhgD3E9U77EO3gV6PNVZHzC0grF3ywCLcBGAs/s640/concurrent-marking-9.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
Figure 6. Marking worklist</p>
<h2 id="">동시 마킹</h2>
<p>동시 마킹은 워커 스레드가 힙에서 객체를 방문(visits; 찾아다니는 여행)하는동안 메인 스레드에서 자바스크립트 코드가 작동할 수 있게끔 합니다. 이런 동작은 데이터 레이스(race condition)가 발생할 수 있는 가능성을 남겨둡니다. 예로, 자바스크립트는 워커 스레드 풀에서 어떤 필드를 읽고있는 도중에 해당 객체 필드에 쓰기 작업을 할 수도 있습니다. 이런 데이터 레이스는 가비지 컬렉터가 살아있는 객체를 반환하거나, 포인터와 원시값(primitive values)을 혼합하는 등, 가비지 컬렉터를 혼란스럽게 할 수 있습니다.</p>
<p>객체 그래프를 변경할 수 있는 메인 스레드에서의 각각의 동작은 데이터 레이스의 가능성을 제공합니다. V8은 많은 객체 레이아웃 최적화가 포함된 고성능 엔진이기 때문에, 데이터 레이스가 발생할 수 있는 근원지의 목록은 더 길 수도 있습니다. 다음은 높은 레벨(high-level)에서의 항목입니다:</p>
<ul>
<li>객체 (메모리)할당 (object allocation)</li>
<li>객체 필드 쓰기작업 (write to an object field)</li>
<li>객체 레이아웃 변경 (object layout changes)</li>
<li>스냅샷으로부터 역직렬화 (deserialization from the snapshot)</li>
<li>최적화되지 않은 함수의 처리? (materialization during deoptimization of a function)</li>
<li>낮은 세대 GC<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>의 수행 (evacuation during young generation garbage collection)</li>
<li>코드 패칭 (code patching)</li>
</ul>
<p>메인 스레드는 이런 동작에 대해 워커 스레드와 동기화할 필요가 있습니다. 동기화를 하는데 드는 비용과 복잡도는 각 오퍼레이션에 따라 다릅니다. 대부분의 작업은 원자적 메모리 접근(atomic memory accesses)와 함께 가벼운 동기화를 합니다만 몇몇 작업은 객체에 대한 독립적인(exclusive) 접근을 요구합니다. 다음 서브섹션에서 우리는 몇가지 재밌는 케이스를 모아봤습니다.</p>
<h3 id="writerbarrier">Writer barrier</h3>
<p>객체 필드에 쓰는 작업으로 인해 발생하는 데이터 레이스는 쓰기 작업을 <a href="https://en.cppreference.com/w/cpp/atomic/memory_order#Relaxed_ordering">relaxed atomic write</a><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>으로 전환하고 write barrier를 트윅하는 방식으로 해결될 수 있습니다:</p>
<pre><code class="language-c">// atomic_relaxed_write(&amp;object.field, value); 이후 실행
write_barrier(object, field_offset, value) {
  if (color(value) == white &amp;&amp; atomic_color_transition(value, white, grey)) {
    marking_worklist.push(value);
  }
}
</code></pre>
<p>이전에 쓰였던 write barrier 코드와 한번 비교해보세요:</p>
<pre><code class="language-c">// `object.field = value` 이후 실행
write_barrier(object, field_offset, value) {
  if (color(object) == black &amp;&amp; color(value) == white) {
    set_color(value, grey);
    marking_worklist.push(value);
  }
}
</code></pre>
<p>두가지 변화가 있습니다:</p>
<ol>
<li>소스 객체의 컬러 체크 코드가 사라졌습니다. (<code>color(object) == black</code>)</li>
<li><code>value</code>의 컬러를 화이트에서 그레이로 전환하는 코드가 원자적(atomically)으로 작동합니다.</li>
</ol>
<p>소스 객체 컬러 체크 부분을 제외한다면 write barrier는 더 보수적으로(conservative) 작동하게 되었습니다. 예로 위 코드는 실제로 객체가 정말로 도달할 수 없더라 하더라도 살아있는 것으로 마킹할 겁니다. 우리는 쓰기 작업과 write barrier 사이에 많은 비용을 요구로 하는 메모리 fence를 방지하기 위해 체크하는 부분을 제거했습니다:</p>
<pre><code class="language-c">atomic_relaxed_write(&amp;object.field, value);
memory_fence();
write_barrier(object, field_offset, value);
</code></pre>
<p>메모리 fence가 없다면 객체의 컬러를 가져오는 작업은 쓰기작업 전으로 재배치될 수 있습니다. 만약 우리가 재배치를 막지 않는다면 write barrier는 해당 객체를 그레이로 보고 끄집어 낼겁니다(bail out), 워커 스레드가 새로운 값을 확인하지 못하고 객체를 마킹하는동안 말이죠. 다익스트라 등이 제안한 원래 write barrier 또한 객체 색상을 체크하지 않습니다. 간단하게 하기 위해서요, 하지만 우리는 정확하게 작동하기 위해 그것을 할 필요가 있습니다.</p>
<h3 id="bailoutworklist">Bailout worklist</h3>
<p>코드 패칭과 같은 몇몇 작업은 객체에 대한 단독 액세스(exclusive access)를 요구로 합니다. 초기에 우리는 객체별로 락을 거는 것을 피했었습니다, 왜냐하면 우선순위 역전 현상(priority inversion problem)<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>이 생길 수도 있기 때문입니다. 메인 스레드는 객체 락을 쥐고 있는 워커 스레드가 디스케쥴 될때까지 기다려야 하는거죠. 객체에 락을 거는 대신, 우리는 워커 스레드가 직접 객체를 방문하지 않고 처리할 수 있도록 하였습니다. 워커 스레드는 메인 스레드에서만 처리되는 bailout worklist에 객체를 넣음으로서 이를 처리합니다:</p>
<p><img src="https://2.bp.blogspot.com/-7R63Ca45I_A/Wx6DKHRkNtI/AAAAAAAACa0/ihdiEyY7ckcXR1bmKAB1ijq-Byhd5zdMwCLcBGAs/s640/concurrent-marking-10.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"><br>
Figure 7. The bailout worklist</p>
<p>워커 스레드는 최적화된 코드 객체, 숨겨진 클래스, 그리고 약한 컬렉션(weak collections)을 bailout하도록 합니다, 왜냐하면 이걸 직접 방문(visiting)하게 되면 락을 걸 필요가 생기거나, 복잡한 동기화 프로토콜이 필요해지기 때문입니다.</p>
<p>뒤돌아서 다시보면 bailout worklist는 점진적 개발에 아주 좋은 것으로 판별되었습니다. 우리는 워커 스레드가 모든 객체 타입을 bailout하게끔 구현하기 시작했고 동시성을 차근차근 늘렸습니다.</p>
<h3 id="">객체 레이아웃 변동</h3>
<p>어떤 객체의 한 필드에는 다음 세가지 값이 들어갈 수 있습니다: 태그된 포인터(a tagged pointer), 태그된 작은 정수형(a tagged small integer, Smi로도 알려짐), 태그되지 않은 값(an untagged value like an unboxed floating-point number). <a href="https://en.wikipedia.org/wiki/Tagged_pointer">포인터 태깅</a><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>은 unboxed integers의 효율적인 표현을 가능하게 해주는 잘 알려진 기술입니다. V8에서 태그된 값의 가장 작은 의미있는 비트(the least significant bit of a tagged value)는 그것이 포인터인지 아니면 integer인지를 나타냅니다. 이는 포인터가 word-aligned라는 사실에 기반합니다. 필드가 tagged인지 아니면 untagged인지에 대한 정보는 객체의 숨겨진 클래스에 저장됩니다.</p>
<p>V8의 몇몇 작업은 객체를 다른 숨겨진 클래스로 변환하는 과정을 통해 객체의 필드를 tagged에서 untagged(또는 그 외)로 변경합니다. 이런 객체 레이아웃 변경은 동시 마킹에 매우 위험합니다. 만약 워커 스레드가 오래된 숨겨진 클래스를 이용해 객체를 동시에 방문하고 있는 도중에 변화가 발생한다면, 두가지 버그가 발생할 수 있습니다. 첫번째로, 워커가 해당 객체가 untagged value인 것으로 생각하고 포인터를 놓칠 수 있습니다. writer barrier는 이러한 버그로부터 보호합니다. 두번째로, 워커가 untagged value를 pointer로 착각하고 레퍼런스를 해제(dereference)할 수도 있습니다, 이렇게 되면 잘못된 메모리 접근으로 보통 프로그램에 크래시가 뒤따르게 됩니다. 이러한 문제를 다루기 위해 우리는 객체의 mark-bit를 동기화하는 스냅샷 프로토콜을 사용합니다. 이 프로토콜은 두가지 파트를 포함합니다: 객체 필드를 tagged에서 untagged로 바꾸는 메인 스레드, 객체를 방문하는 워커 스레드. 필드를 바꾸기 전에, 메인 스레드는 객체가 블랙으로 마크되었음을 보장한 후 나중에 방문할 bailout worklist에 넣습니다:</p>
<pre><code class="language-c">atomic_color_transition(object, white, grey);
if (atomic_color_transition(object, grey, black)) {
  // 이 객체는 bailout worklist가 메인 스레드에서 소모될 때 (처리할 때)
  // 다시 방문하게 됩니다.
  bailout_worklist.push(object);
}
unsafe_object_layout_change(object);
</code></pre>
<p>아래 코드에서 볼 수 있듯, 워커 스레드는 우선 객체의 숨겨진 클래스를 불러와 <a href="https://en.cppreference.com/w/cpp/atomic/memory_order#Relaxed_ordering">atomic relaxed load operations</a>을 이용해 숨겨진 클래스에 정의된 객체의 필드에 있는 모든 포인터를 스냅샷합니다. 이후 원자적 비교와 스왑 작업을 통해 객체를 블랙으로 마킹하도록 시도합니다. 만약 마킹에 성공한다면 이것이 의미하는 것은 스냅샷이 반드시 숨겨진 클래스와 일관성을 유지해야 한다는 것입니다, 왜냐하면 메인 스레드가 레이아웃을 바꾸기 전에 객체를 블랙으로 마킹하게 될테니까요.</p>
<pre><code class="language-c">snapshot = [];
hidden_class = atomic_relaxed_load(&amp;object.hidden_class);
for (field_offset in pointer_field_offsets(hidden_class)) {
  pointer = atomic_relaxed_load(object + field_offset);
  snapshot.add(field_offset, pointer);
}
if (atomic_color_transition(object, grey, black)) {
  visit_pointers(snapshot);
}
</code></pre>
<p>여기서 알아야 할 점은 안전하지 않은 레이아웃 변화에 영향을 받은 화이트 객체는 메인 스레드에서 마킹되어야 한다는 것입니다. 안전하지 않은 레이아웃 변화는 매우 드문 현상입니다, 따라서 실제 상황에서 성능에 그리 큰 영향을 끼치지는 않습니다.</p>
<h2 id="">종합</h2>
<p>우리는 동시 마킹을 이미 존재하는 점진적 마킹 구조에 포함시켰습니다. 메인 스레드는 roots를 싹 훑어보고 marking worklist를 채움으로서 마킹을 준비하게 됩니다. 이후 메인 스레드는 동시 마킹 작업을 워커 스레드에 맡깁니다. 워커 스레드는 메인 스레드를 도와 서로 협력하여 marking worklist를 빠르게 처리하게끔 합니다. 가끔 메인 스레드는 bailout worklist와 marking worklist를 처리하여 마킹에 참여하게 됩니다. marking worklist가 비게 되는 때에 메인 스레드는 가비지 컬렉션을 끝내게 됩니다. 완료 시점에 메인 스레드는 roots를 다시 탐색하여 더 많은 화이트 객체를 발견하게 될 수도 있습니다. 이 객체들은 다른 워커 스레드의 도움과 함께 병렬로 마크됩니다.</p>
<p><img src="https://2.bp.blogspot.com/-EYo8-h3FNgI/Wx6DLBh8qhI/AAAAAAAACbA/H96MmoKQ-zIo8_JAeTOaT7u2-xRySc5nACLcBGAs/s640/concurrent-marking-11.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"></p>
<h2 id="">결과</h2>
<p>우리의 <a href="https://v8project.blogspot.com/2016/12/how-v8-measures-real-world-performance.html">실제 환경 벤치마크 프레임워크</a>로 테스트해본 결과, 모바일 및 데스크탑 환경 모두에서 한 가비지 컬렉션 사이클마다 메인 스레드 마킹 시간이 65%에서 70% 줄어든 것을 확인할 수 있었습니다.</p>
<p><img src="https://2.bp.blogspot.com/-yk4xrtTzCqg/Wx6DLfHy_bI/AAAAAAAACbE/1lCRuixgGSMqvOdPVl6RnwL-gghxUIr9ACLcBGAs/s640/concurrent-marking-12.png" alt="Chrome 64, Node.js v10에서는 GC가 mark하는 동안 앱이 멈추지 않습니다"></p>
<p>동시 마킹 또한 Node.js에서의 가비지 컬렉션으로 인한 영향을 줄였습니다. 이게 Node.js에서는 특별이 중요한 부분인데 왜냐하면 Node.js는 유휴 시간 가비지 컬렉션 스케쥴링(idle time garbage collection scheduling)이 구현되어있지 않으며 따라서 마킹 시간을 줄일 틈이 없었기 때문입니다. 동시 마킹은 Node.js v10에서 제공됩니다.</p>
<p><em>Posted by Ulan Degenbaev, Michael Lippautz, and Hannes Payer — main thread liberators</em></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>다른 스레드로부터 간섭받지 않는 독립된 공간이란 의미를 뜻합니다. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>generational gc에서 생긴지 얼마 안된 객체가 이곳에 할당됩니다. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>실제 메모리 액세스 코드가 성능을 위해 CPU내에서 뒤섞일 수 있기 때문에 메모리 접근을 순차적으로 하게끔 합니다. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>각 작업별로 우선순위가 정해져 있는데 특정 리소스를 사용하는데 하위 작업이 리소스를 점유함에 따라 우선순위가 밀려 발생하는 현상입니다. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>쉽게 생각해서 &lt;정보가 담긴 포인터&gt; <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
]]></content:encoded></item><item><title><![CDATA[소프트웨어가 얼마나 복잡해질 수 있는지에 대한 예제]]></title><description><![CDATA[모두가 코드를 깔끔하게 간단하게 짜라고 말합니다. 그리고 우리 모두는 이 말에 동의합니다. 만약 우리 모두가 이 목표를 알고 있다면, 어째서 프로젝트를 하는 시간이 지나면 지날수록 일이 복잡하게 엉켜가는 걸까요? 여기서 우리는 아마 단순한 문제를 해결하기 위해 무엇을 생각해봐야 할지에 대해 몇가지 다뤄봐야 할 것 같습니다.]]></description><link>https://tech.ssut.me/an-example-of-how-software-becomes-complicated/</link><guid isPermaLink="false">5b12a164e2a96873632ed471</guid><category><![CDATA[programming]]></category><category><![CDATA[complexity]]></category><category><![CDATA[simplicity]]></category><category><![CDATA[javascript]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Sat, 02 Jun 2018 15:18:51 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/06/code.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
<img src="https://tech.ssut.me/content/images/2018/06/code.jpg" alt="소프트웨어가 얼마나 복잡해질 수 있는지에 대한 예제"><p>이 글은 <a href="https://jorin.me/">Jorin</a>님이 작성하신 <a href="https://jorin.me/an-example-of-how-software-becomes-complicated/">An Example of How Software Becoms Complicated</a>의 번역글입니다.</p>
</blockquote>
<p><em>자바스크립트에서 캐시 코드를 작성하고 코드를 간결하게 유지하는 것이 무엇을 의미하는지에 대해 확인해봅시다.</em></p>
<p>우리는 소프트웨어 개발자들이 자주 <em>코드는 간결해야 하고 복잡성을 컨트롤 할 수 있어야 해</em>라고 말하는 것을 듣습니다. 그러기 위해 코드를 재사용 가능하고 공유 기능한 형태로, 또한 확장하기 쉬운 형태로 개발하려 하죠.</p>
<p>소프트웨어 코드를 작성할 때 <a href="https://www.youtube.com/watch?v=ubaX1Smg6pY">코드를 복잡하게 만드는 것보다 훨씬 복잡하게 만드는 것이</a>이 훨씬 쉽습니다.</p>
<blockquote>
<p>원문: When writing software it is very easy to end up with code more complicated than complex, that tries to do too many things and is hard to work with.<br>
참고: <a href="https://english.stackexchange.com/questions/10459/what-is-the-difference-between-complicated-and-complex">What is the difference between “complicated” and “complex”?</a></p>
</blockquote>
<p>모두가 코드를 깔끔하게 간단하게 짜라고 말합니다. 그리고 우리 모두는 이 말에 동의합니다. 만약 우리 모두가 이 목표를 알고 있다면, 어째서 프로젝트를 하는 시간이 지나면 지날수록 일이 복잡하게 엉켜가는 걸까요? 여기서 우리는 아마 단순한 문제를 해결하기 위해 무엇을 생각해봐야 할지에 대해 몇가지 다뤄봐야 할 것 같습니다.</p>
<hr>
<p>간단한 캐시를 만들어봅시다.</p>
<p>이 캐시는 key-value 페어로 즉시 값을 설정하고 받을 수 있습니다. 간단하게 구현한다면 다음과 같은 코드가 나올겁니다:</p>
<pre><code class="language-javascript">const cache = () =&gt; {
  const store = {}

  const set = (key, value) =&gt; {
    store[key] = value
  }

  const remove = key =&gt; {
    const value = store[key]
    delete store[key]
    return value
  }

  return { set, remove }
}

// Let's use the cache

const simpleCache = cache()

simpleCache.set('a', 1)
simpleCache.set('b', 2)
simpleCache.set('b', 3)

console.log(simpleCache.remove('a')) // 1
console.log(simpleCache.remove('b')) // 3
console.log(simpleCache.remove('b')) // undefined
</code></pre>
<p>자 그런데 프로젝트가 진행됨에 따라 이 캐시에 만료(expires)기능을 넣을 필요가 생겼습니다. TTL(time to live)을 받아서 setTimeout으로 일정 시간이 지나면 만료되게 해봅시다. 이렇게 바뀔거에요:</p>
<pre><code class="language-javascript">const cache = (ttl, expirationHandler) =&gt; {
  const store = {}

  const set = (key, value) =&gt; {
    // Clear existing timer
    const record = store[key]
    if (record) {
      clearTimeout(record.timer)
    }
    // Set expiration timer
    const timer = setTimeout(() =&gt; {
      expirationHandler(key, store[key].value)
      delete store[key]
    }, ttl)
    // Store timer and value
    store[key] = { timer, value }
  }

  const remove = key =&gt; {
    // Find record
    const record = store[key]
    if (!record) {
      return undefined
    }
    delete store[key]
    const { timer, value } = record
    // Clear timer and store
    clearTimeout(timer)
    return value
  }

  return { set, remove }
}


const expirationHandler = (key, value) =&gt; {
  console.log(`expired ${key}: ${value}`) // expired b: 2
}
const expiringCache = cache(1000, expirationHandler)

expiringCache.set('a', 1)
expiringCache.set('b', 2)

console.log(expiringCache.remove('a')) // 1
console.log(expiringCache.remove('a')) // undefined
setTimeout(() =&gt; {
  console.log(expiringCache.remove('b')) // undefined
}, 1100)
</code></pre>
<p>모든게 잘 돌아갑니다. 네 좋아요. 그런데 같이 코드를 짜던 동료가 코드리뷰를 해주는데 이 캐시가 만료될 필요가 전혀 없는 다른 곳에서도 쓰일 수 있다고 합니다. 뭐 간단히는 예전 캐시와 지금 캐시 코드 모두를 두는 방법도 있겠죠, 그렇지만 <a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a> 원칙을 지키고 싶습니다.</p>
<p>그래서 대신 새로 짠 캐시 코드가 두 케이스를 모두 대응할 수 있도록 바꿉니다:</p>
<pre><code class="language-javascript">const cache = (ttl, expirationHandler) =&gt; {
  const store = {}

  const set = (key, value) =&gt; {
    // If no TTL is specified, behave as before and return early
    if (!ttl) {
      store[key] = value
      return
    }
    // Clear existing timer
    const record = store[key]
    if (record) {
      clearTimeout(record.timer)
    }
    // Set expiration timer
    const timer = setTimeout(() =&gt; {
      expirationHandler(key, store[key].value)
      delete store[key]
    }, ttl)
    // Store timer and value
    store[key] = { timer, value }
  }

  const remove = key =&gt; {
    // Find record
    const record = store[key]
    if (!record) {
      return undefined
    }
    delete store[key]
    // If no TTL is specified, behave as before and return early
    if (!ttl) {
      return record
    }
    const { timer, value } = record
    // Clear timer and store
    clearTimeout(timer)
    return value
  }

  return { set, remove }
}

// Let's use the simple cache

const simpleCache = cache()

simpleCache.set('a', 1)
simpleCache.set('b', 2)
simpleCache.set('b', 3)

console.log(simpleCache.remove('a')) // 1
console.log(simpleCache.remove('b')) // 3
console.log(simpleCache.remove('b')) // undefined

// Let's use the expiring cache

const expirationHandler = (key, value) =&gt; {
  console.log(`expired ${key}: ${value}`) // expired b: 2
}
const expiringCache = cache(1000, expirationHandler)

expiringCache.set('a', 1)
expiringCache.set('b', 2)

console.log(expiringCache.remove('a')) // 1
console.log(expiringCache.remove('a')) // undefined
setTimeout(() =&gt; {
  console.log(expiringCache.remove('b')) // undefined
}, 1100)
</code></pre>
<p>엄청 빨리 해결됐네요. 이 작업을 하는데 필요했던 부분은 그저 두 개의 <em>IF</em> 구문을 추가하는 것일 뿐이었습니다.</p>
<p>그리고 이게 바로 어떻게 이러한 과정이 복잡해지는지를 설명해줍니다: <em>간단한</em> 캐시는 더이상 간단해지지 않았습니다, 만료 기능을 넣으며 뭔가 얽혀졌죠. 이런 간단한 시나리오가 이해하기 어려워졌고, 느려졌으며 버그를 발생시킬 수 있는 여지도 생겼습니다.</p>
<p><strong>기능을 추가할 때마다 그저 <em>IF</em>문을 하나씩 더 추가하는 식으로 떼우게 된다면 코드 전체가 더 엉망이 되는데 기여하는 것일 뿐입니다.</strong> - <a href="http://www.joeyoder.com/PDFs/mud.pdf">the big ball of mud</a></p>
<p>그러면 어떻게 원래 캐시 코드를 간단하게 유지할 수 있을까요?</p>
<p><em>간단한 것을 복잡하게 만드는 대신 코드를 복제합시다.</em></p>
<p>코드를 복제하면 어느 부분을 공유해서 재사용했는지 알기가 더 쉬워집니다. 단 한가지 일만 하는 코드를 여럿 짜고, 이 모두를 섞어서(엮어서) 또다른 도구를 만들어냅시다. 이 내용은 새로운 것이 아닌 <a href="https://en.wikipedia.org/wiki/Unix_philosophy">이전에도 수많이 언급되었던</a> 내용입니다.</p>
<p>그러면 어떻게 간단한 캐시를 복잡하게 만들지 않고 만료되는 캐시를 구현할 수 있을까요?</p>
<p>예제에서 만료 기능은 초기 코드 위에 손쉽게 얹어질 수 있습니다:</p>
<pre><code class="language-javascript">const cache = () =&gt; {
  const store = {}

  const set = (key, value) =&gt; {
    store[key] = value
  }

  const remove = key =&gt; {
    const value = store[key]
    delete store[key]
    return value
  }

  return { set, remove }
}

const expire = (cache, ttl, expirationHandler) =&gt; {
  const timers = {}

  const set = (key, value) =&gt; {
    // Store value
    cache.set(key, value)
    // Clear existing timer
    clearTimeout(timers[key])
    // Set expiration timer
    timers[key] = setTimeout(() =&gt; {
      const value = cache.remove(key)
      delete timers[key]
      expirationHandler(key, value)
    }, ttl)
  }

  const remove = key =&gt; {
    clearTimeout(timers[key])
    delete timers[key]
    return cache.remove(key)
  }

  return { set, remove }
}

// Let's use the simple cache

const simpleCache = cache()

simpleCache.set('a', 1)
simpleCache.set('b', 2)
simpleCache.set('b', 3)

console.log(simpleCache.remove('a')) // 1
console.log(simpleCache.remove('b')) // 3
console.log(simpleCache.remove('b')) // undefined

// Let's use the expiring cache

const expirationHandler = (key, value) =&gt; {
  console.log(`expired ${key}: ${value}`)
}
const expiringCache = expire(cache(), 1000, expirationHandler)

expiringCache.set('a', 1)
expiringCache.set('b', 2)

console.log(expiringCache.remove('a')) // 1
console.log(expiringCache.remove('a')) // undefined
setTimeout(() =&gt; {
  console.log(expiringCache.remove('b')) // undefined
}, 1100)
</code></pre>
<p>이 예처럼 몇몇의 경우에는 쉽게 코드가 구성될 수 있습니다. 다른 경우라면 부분부분을 재사용해야겠죠. 로직의 부분부분을 작은 함수단위로 나누고 그걸 공유해서 사용하면 그걸 또다른 도구로 사용할 수 있게 됩니다.</p>
<p>기존 코드에서 새로운 부분(조건)을 추가할 때마다 주의를 해야할 필요가 있습니다. 코드에서 어느 부분이 분리될 수 있고 재사용할 수 있을지에 대해 생각해보세요. 코드를 복사하여 재사용하는 것에 대해 두려워하지 마세요.</p>
]]></content:encoded></item><item><title><![CDATA[당신이 와일드카드 인증서를 사용하면 안될 수도 있는 이유]]></title><description><![CDATA[최근 Let's Encrypted는 무료 와일드카드 인증서를 제공하기 시작했습니다. 이는 그동안 값비쌌던 상용 인증서를 사용해야 했던 이유 중 하나를 없애줄 좋은 소식이지만, 저는 많은 사람들이 와일드카드 인증서에 대해 위험할 정도로 잘못 이해하고 있다는 것을 확인했습니다. 이에 따라, 이 글에서는 왜 당신이 보안상 위험에 빠질 수도 있는 와일드카드 인증서를 왜 사용하면 안되는지에 대해 설명해보려 합니다....]]></description><link>https://tech.ssut.me/why-you-probably-should-not-use-a-wildcard-certificate/</link><guid isPermaLink="false">5b127195e2a96873632ed46b</guid><category><![CDATA[security]]></category><category><![CDATA[TLS]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Sat, 02 Jun 2018 13:12:54 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2018/06/Lets-Encrypt_1.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
<img src="https://tech.ssut.me/content/images/2018/06/Lets-Encrypt_1.jpg" alt="당신이 와일드카드 인증서를 사용하면 안될 수도 있는 이유"><p>이 글은 <a href="https://gist.github.com/joepie91/7e5cad8c0726fd6a5e90360a754fc568">Why you probably should not use a wildcard certificate</a>의 번역글입니다.</p>
</blockquote>
<p>최근 <a href="https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579">Let's Encrypted는 무료 와일드카드 인증서</a>를 제공하기 시작했습니다. 이는 그동안 값비쌌던 상용 인증서를 사용해야 했던 이유 중 하나를 없애줄 좋은 소식이지만, 저는 많은 사람들이 와일드카드 인증서에 대해 위험할 정도로 잘못 이해하고 있다는 것을 확인했습니다.</p>
<p>이에 따라, 이 글에서는 왜 <strong>당신이 보안상 위험에 빠질 수도 있는 와일드카드 인증서를 왜 사용하면 안되는지에 대해</strong> 설명해보려 합니다.</p>
<h2 id="">개요</h2>
<p>일반적으로 TLS(&quot;SSL&quot;)가 어떻게 동작하는지에 대해서는 정말 잘 이해하고 있는 경우는 거의 없습니다. 그 그러니까 먼저 중요한 부분을 간략하게 짚고 넘어갑시다.</p>
<p>일반적인(간단하게 표현한) TLS 동작은 다음과 같습니다:</p>
<ol>
<li>암호 키 쌍을 만듭니다. (비밀키 + 공개키)</li>
<li>키 쌍으로 '인증서'를 만듭니다. (공개키와 hostname, 인증서 만료일 등이 들어간 메타데이터를 포함함)</li>
<li>인증서를 인증기관(Let's Encrypt와 같은)으로 보냅니다. 인증기관에서는 인증서를 받아 메타데이터를 검증합니다. -- 이 부분이 바로 당신이 인증서에 포함된 호스트네임을 소유하고 있다는 것을 증명합니다.</li>
<li><em>서명된</em> 인증서를 받습니다. -- 원본 인증서와 CA(인증기관)가 이 인증서를 검증했다는 서명(사인)이 포함됩니다.</li>
<li>이 서명된 인증서를 클라이언트에게 제공(serve)합니다.</li>
</ol>
<p>이제 클라이언트는 다음과 같이 동작하게 됩니다:</p>
<ol>
<li>클라이언트가 신뢰하고 있는 인증기관으로부터 인증서가 서명되었는지를 검증합니다. (이는 보통 '신뢰할 수 있는 루트 인증기관(Trusted root certificate authority)'으로 시스템에 이미 포함되어 있습니다.)</li>
<li>신뢰할 수 있는 인증서라면, 인증서가 포함된 공개키를 서버의 공개키로 처리합니다. 그리고 이 키를 이용해서 서버와 암호화된 통신을 합니다.</li>
</ol>
<p>이 설명은 정말 간략합니다만 여기서 <em>왜</em> 이 방식이 많은 공격으로부터 안전한지에 대해 자세하게 다루지는 않겠습니다. 하지만 일반적인 개념으로: 그 누구도 트래픽을 스누핑하거나 서버를 속일 수 없습니다, 적어도 (1) CA 키가 털리거나 (2) 당신의 키 쌍 + 서명된 인증서가 유출되지 않는 한은요.</p>
<h2 id="">그럼 와일드카드 인증서는 정확히 뭘까요?</h2>
<p>보통 TLS 인증서 메타데이터에는 명확한 호스트네임이 들어가 있습니다. 예로 G메일의 경우에는 <code>mail.google.com</code>이 들어가 있겠죠. 이때 이 인증서는 <code>https://mail.google.com/</code>에 접속할 때에만 유효합니다. <code>https://images.google.com/</code> 또는 <code>https://my.mail.google.com/</code>에서는 유효하지 않다는 거죠. 다른 말로하면, 호스트네임은 완전히 일치하여야 합니다. 만약 <code>https://www.my.mail.google.com/</code>에 <code>mail.google.com</code> 호스트네임을 가진 인증서를 사용하면 브라우저가 오류를 낼겁니다.</p>
<p>와일드카드 인증서는 좀 다릅니다. 이름이 의미하듯, 와일드카드 인증서는 완전히 일치할 필요 없이 와일드카드 부분만 일치하면 됩니다. 만약 <code>*.google.com</code>에 대한 와일드카드 인증서를 사용한다면 <code>https://mail.google.com/</code> 및 <code>https://images.google.com/</code> 모두 유효합니다. 하지만 여전히 <code>https://google.com/</code> 및 <code>https://my.mail.google.com</code>에서는 유효하지 않습니다. 다른 말로, 별(*) 표시가 있는 곳에서만 유효한거죠.</p>
<p>이 방식은 여러 상황에서 굉장히 유용합니다. 예로 제가 어떤 단일 서버를 만드는데 이를 사용하는 모든 사용자가 그들만을 위한 서브도메인을 할당받는다고 가정해봅시다. 더 구체적으로 제 사이트는 <code>https://joepie91.somesitebuilder.com/</code>이고 당신의 사이트는 <code>https://catdogcat.somesitebuilder.com/</code>일 수도 있습니다.</p>
<p>이 상황에서 사용자가 늘어날 때마다 새로운 인증서를 발급받는 것은 매우 비효율적입니다. 더 쉽게 그저 <code>*.somesitebuilder.com</code>에 대해서만 인증서를 발급받으면 되고, 이 인증서 하나면 모든 사용자에게 유효하게 작용하는 거죠.</p>
<p>지금까진 좋습니다.</p>
<h2 id="">그럼 이걸 왜 모든 서브도메인에 사용하면 안되나요?</h2>
<p>이제 문제가 발생하게 됩니다. 위 예에서 모든 사이트는 <em>한 서버</em>에서 호스팅된다고 설명했습니다. 만약 당신이 많은 서브도메인을 갖는 엄청 큰 웹 사이트나 조직을 운영한다면 - 구글을 예로 들면 <code>images.google.com</code>, <code>mail.google.com</code> - 이 서브도메인 모두는 아마 여러 서버에서 호스팅되겠죠.</p>
<p>자 이 부분이 와일드카드 인증서에서 보안 문제가 생길 수도 있는 부분입니다.</p>
<p>TLS 보안을 위한 요구사항 중 하나가 &quot;키 쌍 + 서명된 인증서가 유출되지 않음&quot;라고 이야기했습니다. 그런데 인증서는 가끔 <em>유출되기도</em> 합니다. - 예로 서버는 간혹 해킹당하기도 하죠.</p>
<p>만약 진짜로 이러한 일이 일어난다면 당신은 이러한 상황에서 최대한 피해를 줄이고 싶어할 겁니다. - 이상적으로, 인증서를 매우 빠르게 만료시킬 거고, 해킹당한 서버를 제외하고는 아무 영향을 끼치지 않겠죠. 이슈를 해결하고 나서는 유출당한 인증서를 더 이상 사용할 수 없게(revoke) 할거고, 안전한 새 인증서를 발급받고, 다른 서버는 영향을 받지 않을겁니다.</p>
<p>하지만, &quot;여러대의 서버를 운영&quot;하는 경우는 반드시 고려해야 합니다. 만약 <code>images.google.com</code> 서버만 해킹당한다면 <code>mail.google.com</code>은 영향을 받지 않을겁니다. 하지만, <code>images.google.com</code> 서버의 인증서가 <code>*.google.com</code>을 위한 와일드카드 인증서였다면 <code>mail.google.com</code>까지 털어서 이메일 트래픽을 훔쳐본다던가 할 수 있게 되겠죠. <code>mail.google.com</code> 서버는 해킹당하지 않았는데도 말이죠!</p>
<p>오직 한 서버만 해킹당했음에도 이 상황이 닥친다면 우리는 피해를 줄이기가 어렵습니다. 이메일 서버까지 리스크를 감당해야 하는거죠. 만약 우리가 <code>mail.google.com</code>, <code>images.google.com</code>으로 각각 분리된 인증서를 사용했다면 이런 상황은 닥치지 않을겁니다.</p>
<h2 id="">이야기에 대한 교훈</h2>
<p>각각의 인증서는 한 서버만을 위해 사용되어야만 합니다, 또는 같은 역할을 하는 서버의 클러스터에만요. 서로 다른 서버에 서로 다른 서비스가 올라간다면 이러한 상황에서는 와일드카드가 아닌 각각의 인증서를 사용해야 합니다.</p>
<p>만약 당신이 같은 서버에서 같은 서비스를 가리키는 여러 호스트네임을 갖고 있다면 와일드카드 인증서를 써도 좋습니다. 이 와일드카드 인증서가 다른 서버를 가리키는 호스트네임까지 커버하지 않는다면 말이죠. 그렇지 않다면, 각각의 서비스는 반드시 각각의 서비스를 위한 인증서를 가져야 합니다.</p>
<p>만약 당신이 단일 서버면서 단일 서비스를 가리키는 호스트네임을 갖고 있다면 - 예로 <code>login.mysite.com</code> 및 사용자에게 생성된 사이트 - 사용자에게 생성된 사이트의 경우는 그 서비스를 위한 접두사(prefix)를 두어야 합니다. 예로, 인증서 하나는 <code>login.mysite.com</code>, 나머지(와일드카드)는 <code>*.users.mysite.com</code> 처럼요.</p>
<p>뭐 실제로는, 당신은 아마 절대로 와일드카드 인증서가 필요하지 않습니다. 와일드카드 인증서라는 선택지가 있는 것은 굉장히 좋지만, 위 예처럼 사용자마다 인증서를 자동으로 생성해야 하는 경우를 제외하고는 와일드카드 인증서는 아마 완전히 불필요하고 안전하지 않은 선택지입니다.</p>
<p><strong>HIGHLIGHT:</strong></p>
<p><strong>(명확하게: 이 내용은 Let's Encrypt에만 해당하는 것이 아니라 <em>일반적인</em> 모든 와일드카드 인증서에 해당합니다. 하지만 Let's Encrypt 덕분에 와일드카드 인증서가 더이상 값비싸지지 않은 지금, 이 문제는 조금 더 주의를 기울일 필요가 있다고 봅니다.)</strong></p>
<p>&lt; 후에 댓글까지 번역해서 올려보겠습니다. &gt;</p>
]]></content:encoded></item><item><title><![CDATA[WSL(Windows Subsystem for Linux) 사용기 및 ArchLinux로의 전환]]></title><description><![CDATA[<p><img src="https://tech.ssut.me/content/images/2017/10/pacman.png" alt="archlinux pacman"></p>
<p>Windows 10에 <a href="https://msdn.microsoft.com/en-us/commandline/wsl/about">WSL(Windows Subsystem for Linux)</a> 기능이 추가되기 이전에는 SSH를 통해 미리 셋팅해둔 우분투 서버에 원격으로 접속하여 개발을 진행했었습니다. 하지만 원격으로 접속해야 한다는 단점을 비롯해 윈도우에서 사용할 수 있는 터미널 앱들은 24비트 트루컬러를 제대로 지원하지 않고 속도가 느린 문제점 때문에 주로 <a href="https://github.com/paradoxxxzero/butterfly">Butterfly Shell</a>을 이용했었습니다.</p>
<p><img src="https://tech.ssut.me/content/images/2017/10/pacman-2.png" alt="butterfly shell"></p>
<p>여기까지의 이야기는 WSL을 접하기</p>]]></description><link>https://tech.ssut.me/install-archlinux-on-windows-subsystem-for-linux/</link><guid isPermaLink="false">59e8bc1056511e91539fc7d0</guid><category><![CDATA[WSL]]></category><category><![CDATA[Linux]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Fri, 20 Oct 2017 03:45:00 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2017/10/wsl-dev-1.PNG" medium="image"/><content:encoded><![CDATA[<img src="https://tech.ssut.me/content/images/2017/10/wsl-dev-1.PNG" alt="WSL(Windows Subsystem for Linux) 사용기 및 ArchLinux로의 전환"><p><img src="https://tech.ssut.me/content/images/2017/10/pacman.png" alt="WSL(Windows Subsystem for Linux) 사용기 및 ArchLinux로의 전환"></p>
<p>Windows 10에 <a href="https://msdn.microsoft.com/en-us/commandline/wsl/about">WSL(Windows Subsystem for Linux)</a> 기능이 추가되기 이전에는 SSH를 통해 미리 셋팅해둔 우분투 서버에 원격으로 접속하여 개발을 진행했었습니다. 하지만 원격으로 접속해야 한다는 단점을 비롯해 윈도우에서 사용할 수 있는 터미널 앱들은 24비트 트루컬러를 제대로 지원하지 않고 속도가 느린 문제점 때문에 주로 <a href="https://github.com/paradoxxxzero/butterfly">Butterfly Shell</a>을 이용했었습니다.</p>
<p><img src="https://tech.ssut.me/content/images/2017/10/pacman-2.png" alt="WSL(Windows Subsystem for Linux) 사용기 및 ArchLinux로의 전환"></p>
<p>여기까지의 이야기는 WSL을 접하기 전까지의 이야기입니다. WSL이 베타로 공개된 직후 이는 Bash on windows 로 불렸었는데, 베타 딱지가 붙어있는 동안은 실제로 사용하기에는 문제점이 많았습니다. 대표적으로 I/O 성능이 떨어지는 문제, drivefs(기존 windows 드라이브를 마운트 가능하게 하는 역할) 성능 문제, <a href="https://github.com/Microsoft/BashOnWindows/issues/18">Ping</a>이 되지 않는 문제 등이 있었습니다. Microsoft에서는 이러한 문제점들을 조속히 해결해 나가고자 <a href="https://github.com/Microsoft/BashOnWindows/issues">WSL용 이슈 트래커</a>를 운영하기 시작했고 많은 유저의 이슈 제보로 지금은 베타 딱지를 떼고(Build 1709) 정식으로 공개됐죠.</p>
<p>WSL의 공개는 매우 성공적이게 되었습니다. 더이상 개발을 위해 맥을 구입할 필요가 없게 되었고 Windows와의 자연스러운 연동으로 WSL에서는 런타임을, Windows 에서는 IDE를 사용하는 식의 개발도 가능하게 되었습니다 (참고: drivefs로 마운트한 경로에서만 Windows/WSL 동시작업이 가능합니다). 심지어 2016년 10월에는 <a href="https://blogs.msdn.microsoft.com/commandline/2016/10/07/wsl-adds-inotify-filesystem-change-notification-support/">INOTIFY 지원이 추가</a>되어 Windows(drivefs)에서 변경한 파일의 변동/변경 이벤트까지 WSL에서 받을 수 있게 되었습니다. 덕분에 개발이 엄청 편해졌죠. :)</p>
<p><img src="https://tech.ssut.me/content/images/2017/10/wsl-dev.PNG" alt="WSL(Windows Subsystem for Linux) 사용기 및 ArchLinux로의 전환"></p>
<p>IDE는 Gogland EAP(맥에서는 vscode를 Go 개발에 사용했으나 gocode의 느린 성능때문에 Gogland로 갈아타게 되었습니다.), 터미널은 mintty(cmd보다 더 나은 color scheme 지원, 폰트 렌더링) + wslbridge(트루컬러 지원)를 사용했습니다. 실제 소스코드가 위치한 폴더는 <code>/mnt/b</code>인데 이는 실제 Windows에서의 <code>B:</code>에 대응합니다. WSL의 rootfs에서 직접 작업하면 되지 않겠냐고 생각하겠지만 Windows에서 WSL의 rootfs로 직접 접근하여 파일을 수정하는 행위는 rootfs 파일시스템이 망가질 수 있는 원인을 제공한다고 매뉴얼에 나와있기 때문에 drivefs로 마운트된 파일시스템에서 작업하는 것이 안전합니다. (성능은 조금 떨어지더라도 말이죠.)</p>
<h2 id="archlinux">ArchLinux</h2>
<p>WSL을 사용하는 모든 면은 마음에 들었지만 하나 마음에 들지 않는 부분이 있었다면 바로 Ubuntu입니다. Ubuntu 레포 미러를 떠본 분들은 아시겠지만 Ubuntu의 경우 각 OS의 버전마다 패키지 버전이 정해져있고 따라서 Ubuntu 버전을 올리지 않는한 해당 Ubuntu 버전에서 제공되는 패키지보다 더 높은 버전의 패키지를 사용하기가 어렵습니다 (<a href="https://help.ubuntu.com/community/PinningHowto">Pinning</a>을 이용하면 특정 패키지만 상위 우분투 버전의 패키지로 사용하는 것이 가능해지긴 합니다). 심지어 실제 배포 환경(production)이 아닌 개발 환경에서도 LTS 버전의 Ubuntu를 사용하는 경우가 매우 많아 이 문제점은 더욱 돋보이게 됩니다.</p>
<p><img src="https://tech.ssut.me/content/images/2017/10/rolling-release.PNG" alt="WSL(Windows Subsystem for Linux) 사용기 및 ArchLinux로의 전환"></p>
<p>보통 맥에서 개발을 할 때에는 homebrew를 이용해 가장 최신 버전의 패키지를 사용해 개발을 진행하기 때문에 이와 같은 문제점을 느끼기가 어렵습니다. 물론 <a href="http://linuxbrew.sh/">linuxbrew</a>를 이용하는 방법도 있지만 근본적인 원인은 해결되지 않기 때문에 결국에는 rolling release를 지원하는 리눅스 배포판을 이용하는 방법밖에 없습니다. 보통 글을 쓰는 본인은 개발 환경으로 앞서 말한 맥(+brew), Alpine(on docker, edge repo), ArchLinux를 사용하고 있습니다. 모두 rolling release로 패키지 매니저가 구성되어 있는 장점이 있죠.</p>
<p>계속해서 Ubuntu에서 귀찮게 개발하던 도중 Windows 10 Fall Creators (1709) 업데이트가 등장하게 되었고 <a href="http://www.zdnet.com/article/windows-subsystem-for-linux-graduates-in-windows-10-fall-creators-update/">Beta 딱지가 떼어진 기념</a>으로 ArchLinux를 설치해보기로 했습니다. 다행히 <a href="https://wiki.archlinux.org/index.php/Install_on_WSL">ArchLinux 위키에 설치 방법</a>이 자세히 나와있어 과정이 어렵지는 않았습니다.</p>
<p>과정은 다음과 같습니다:<br>
(참고: Windows 10 1709에서 제공하는 Store 앱 대신 legacy WSL 환경으로 설치하는 것을 추천드립니다. 폴더 찾기가 너무 귀찮습니다..)</p>
<ol>
<li>이미 WSL이 설치되어 있다면 <code>lxrun /uninstall /full /y</code>로 환경을 완전히 제거해줍니다.</li>
<li><code>lxrun /install /y</code>로 WSL 환경을 셋업해줍니다.</li>
<li>WSL 셸에 root 계정으로 들어가 <code>wget http://ftp.kaist.ac.kr/ArchLinux/iso/2017.10.01/archlinux-bootstrap-2017.10.01-x86_64.tar.gz</code> 명령어로 archlinux bootstrapping 파일을 다운받습니다.</li>
<li><code>~/root.x86_64/etc/pacman.d/mirrorlist</code> 파일을 에디터로 열어 서버 하나를 주석 해제합니다.</li>
<li>WSL이 자동으로 <code>/etc/resolv.conf</code>(DNS) 파일을 생성하도록 다음 명령어를 입력합니다: <code>echo &quot;# This file was automatically generated by WSL. To stop automatic generation of this file, remove this line.&quot; &gt; ~/root.x86_64/etc/resolv.conf</code></li>
<li>열려있는 모든 Bash 셸을 닫습니다.</li>
<li><code>%localappdata%\lxss\rootfs</code> 경로로 가서 다음 폴더를 완전 삭제합니다: <code>bin</code>, <code>etc</code>, <code>lib</code>, <code>lib64</code>, <code>sbin</code>, <code>usr</code>, <code>var</code>.</li>
<li>이제 <code>%localappdata%\lxss\rootfs\root\root.x86_64</code>에 가서 위에서 삭제한 폴더를 그대로 이동(주의: 절대로 복사하면 안됩니다.)합니다. 폴더가 아닌 파일로 나올 수도 있으나 이는 WSL 파일 시스템(rootfs)과 Windows의 파일 시스템(NTFS, ReFS)이 완전히 호환되지 않기 때문입니다.</li>
<li>다른 리눅스 환경에서 <a href="https://aur.archlinux.org/packages/fakeroot-tcp/">fakeroot-tcp</a> 패키지와 <a href="https://aur.archlinux.org/packages/glibc-wsl/">glibc-wsl</a> 패키지를 빌드합니다. 2017년 10월 19일 기준으로 미리 빌드해둔 파일은 <a href="https://ssut.me/glibc-wsl.pkg.tar.xz">glibc-wsl.pkg.tar.xz</a>, <a href="https://ssut.me/fakeroot-tcp.pkg.tar.xz">fakeroot-tcp.pkg.tar.xz</a> 링크를 클릭해 받으시면 됩니다.</li>
<li>WSL 셸에서 다음 명령어를 입력합니다:<pre><code># pacman-key --init
# pacman-key --populate archlinux
</code></pre>
</li>
<li>위에서 빌드한 패키지를 <code>pacman --force -U glibc-wsl.pkg.tar.xz</code>, <code>pacman --force -U fakeroot-tcp.pkg.tar.xz</code> 명령어로 설치해줍니다.</li>
<li><code>pacman -Syyu base base-devel</code> 명령어로 셋업해줍니다. (주의: 이 과정에서 glibc, fakeroot-tcp 패키지가 덮어씌워질 수 있습니다, 만약 덮어씌워진 경우 11번을 반복하시면 됩니다.)</li>
<li>사용자를 등록하고 암호를 설정해줍니다:<pre><code># useradd -m -G wheel -s /bin/bash username
# passwd root
# passwd username
</code></pre>
</li>
<li>WSL 셸에서 빠져나와 다음 명령어로 기본 사용자를 지정해줍니다: <code>lxrun /setdefaultuser username</code></li>
</ol>
<p>설치 끝! 이제 ArchLinux를 열심히 즐기면 됩니다. :)</p>
<hr>
<p><img src="https://tech.ssut.me/content/images/2017/10/wsl-dev.png" alt="WSL(Windows Subsystem for Linux) 사용기 및 ArchLinux로의 전환"></p>
<p>WSL에서의 개발 환경은 실제 리눅스에서 개발하는 것과 완전히 동일하기 때문에 Windows에서도 이제 상당히 편하게 리눅스 환경을 사용할 수 있게 되었습니다. 이렇게 하나하나 맥만의 장점이 사라지는 것이 아쉽기도 하지만 더욱 좋고 다양한 환경에서 개발자가 개발을 할 수 있게된 부분은 굉장히 높게 칠만한 부분이라고 생각합니다. :)</p>
]]></content:encoded></item><item><title><![CDATA[Peephole: CPython은 어떻게 코드를 최적화하는가]]></title><description><![CDATA[<p>많은 스크립트 언어는 &quot;실행 성능(속도)&quot;이 좋지 않다는 큰 단점을 지니고 있습니다. 이를 해결하기 위해 JIT 런타임을 붙이거나 개발자 스스로 코드를 최적화하기도 하지만 개발자 스스로 코드를 최적화한다고 하여(좋은 코드를 작성했다고 했을 때) 투자하는 시간에 비해 큰 성능향상을 얻기는 어렵습니다. Python은 이러한 문제를 조금이나마 해소시키고자 Peephole이라는 Python</p>]]></description><link>https://tech.ssut.me/peephole-how-python-optimizes-bytecode/</link><guid isPermaLink="false">599edca22555824a21f79114</guid><category><![CDATA[python]]></category><category><![CDATA[python 3]]></category><category><![CDATA[optimization]]></category><category><![CDATA[cpython]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Sun, 27 Aug 2017 10:29:27 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2017/08/2.7-vs-3.6.png" medium="image"/><content:encoded><![CDATA[<img src="https://tech.ssut.me/content/images/2017/08/2.7-vs-3.6.png" alt="Peephole: CPython은 어떻게 코드를 최적화하는가"><p>많은 스크립트 언어는 &quot;실행 성능(속도)&quot;이 좋지 않다는 큰 단점을 지니고 있습니다. 이를 해결하기 위해 JIT 런타임을 붙이거나 개발자 스스로 코드를 최적화하기도 하지만 개발자 스스로 코드를 최적화한다고 하여(좋은 코드를 작성했다고 했을 때) 투자하는 시간에 비해 큰 성능향상을 얻기는 어렵습니다. Python은 이러한 문제를 조금이나마 해소시키고자 Peephole이라는 Python 바이트코드 최적화를 위한 구현을 두고 있으며 이는 Python 내부적으로 효율적인 작동을 보장해주고 있습니다.</p>
<h2 id="dispython">dis: Python 바이트코드</h2>
<ul>
<li>Python에는 CPython 내부 머신코드를 disassemble하여 바이트코드로 보여주는 <a href="https://docs.python.org/3/library/dis.html">dis</a> 모듈이 존재합니다. 이 모듈을 통해 Python이 내부적으로 어떻게 작동하는지를 이해할 수 있으며(<a href="https://github.com/python/cpython/blob/master/Include/opcode.h"><code>Include/opcode.h</code></a>) 더 나아가 복잡하고 어려운 코드를 최적화하는데도 도움이 될 수 있습니다. 또한 이를 이용해 멋진 <a href="https://github.com/ponyorm/pony">ORM</a>을 만들어낼 수도 있습니다.</li>
</ul>
<p>간단하게 시작해봅시다. 다음 코드는 내부적으로 어떻게 돌아갈까요?</p>
<pre><code class="language-python">def exists(filename: str) -&gt; bool:
     return os.path.exists(filename)
</code></pre>
<p>위 코드의 바이트코드를 확인해볼까요? <code>func.__code__.co_code</code>를 통해 실제 바이트코드를 확인할 수 있습니다.</p>
<pre><code class="language-python">In [19]: exists.__code__
Out[19]: &lt;code object exists at 0x7f5644025f60, file &quot;&lt;ipython-input-5-0dc3deb969f7&gt;&quot;, line 1&gt;

In [20]: exists.__code__.co_code
Out[20]: b't\x00\x00j\x01\x00j\x02\x00|\x00\x00\x83\x01\x00S'

In [21]: [x for x in exists.__code__.co_code]
Out[21]: [116, 0, 0, 106, 1, 0, 106, 2, 0, 124, 0, 0, 131, 1, 0, 83]
</code></pre>
<p>자 바이트코드를 확인했습니다. 어라 그런데 저걸 어떻게 읽을까요? 위에서 결과로 나온 <code>[116, 0, 0, 106, 1, 0, 106, 2, 0, 124, 0, 0, 131, 1, 0, 83]</code> 리스트는 Python이 내부적으로 읽을 수 있는 형태의 바이트코드입니다. 위에 링크로 걸어둔 <code>Include/opcode.h</code> 파일에 있는대로 추적해서 <code>116</code> -&gt; <code>LOAD_GLOBAL</code>, <code>106</code> -&gt; <code>LOAD_ATTR</code>.. 이라는 것을 알아낼 수는 있지만 사람이 직접 알아내기는 여전히 어렵습니다.</p>
<p>자 그러면 다른 방법을 이용해봅시다. 위 코드에 대해 <code>dis.dis(exists)</code>를 실행해봅시다.</p>
<pre><code>  2           0 LOAD_GLOBAL              0 (os)
              3 LOAD_ATTR                1 (path)
              6 LOAD_ATTR                2 (exists)
              9 LOAD_FAST                0 (filename)
             12 CALL_FUNCTION            1 (1 positional, 0 keyword pair)
             15 RETURN_VALUE
</code></pre>
<p>훨씬 보기 쉬운 결과가 나왔네요! 얼추 읽기 쉬워보입니다.</p>
<p>제일 왼쪽에 위에 보이는 <code>2</code>는 실제 코드가 시작하는 라인의 번호입니다. 다음 2번째 컬럼으로 있는 숫자는 바이트코드의 offset이며 그 다음은 OPCODE, 그 다음은 arg를 나타냅니다. 위 코드를 예로 볼 때 위 코드에서는 실행되는 과정에서 가장 먼저 GLOBAL(전역)로부터 <code>os</code> 변수가 있는지 찾습니다(<code>LOAD_GLOBAL</code>). 다음 <code>os</code>변수 내에 <code>path</code> 속성이 있는지 확인(<code>LOAD_ATTR</code>) -&gt; <code>os.path</code> 내에 <code>exists</code> 속성이 있는지 확인(<code>LOAD_ATTR</code>)합니다. 마지막으로 우리가 함수의 인자로 넘겨받은 <code>filename</code>은 <code>LOAD_FAST</code> OP를 통해 불러와서 <code>os.path.exists</code> 함수를 호출하고 가져온 값을 반환하게 됩니다.</p>
<p>Python의 <code>dis</code> 모듈은 어떻게 바이트코드를 해독해내는 걸까요? 이는 CPython <code>Include/opcode.h</code>에 나와있는 바이트코드 맵대로 <a href="https://github.com/python/cpython/blob/master/Lib/opcode.py"><code>Lib/opcode.py</code></a>에 다음과 같이 바이트코드 사전을 만들어두었기 때문입니다.</p>
<pre><code class="language-python">In [23]: import opcode
In [24]: opcode
Out[24]: &lt;module 'opcode' from '/usr/lib/python3.5/opcode.py'&gt;
In [25]: opcode.opmap
Out[25]:
{'BEFORE_ASYNC_WITH': 52,
 'BINARY_ADD': 23,
 'BINARY_AND': 64,
 'BINARY_FLOOR_DIVIDE': 26,
 'BINARY_LSHIFT': 62,
 'BINARY_MATRIX_MULTIPLY': 16,
 'BINARY_MODULO': 22,
 'BINARY_MULTIPLY': 20,
 'BINARY_OR': 66,
 'BINARY_POWER': 19,
 'BINARY_RSHIFT': 63,
 ...
 }
</code></pre>
<h2 id="peepholeoptimizerpython">Peephole optimizer: Python 바이트코드 최적화 구현</h2>
<p>Python 바이트코드가 Peephole에 의해 어떻게 바뀌는지, 왜 그렇게 바뀌는지에 대해 자세히 알아보고 싶었지만 현재 Python 버전에서 공식적으로 Peephole optimizations을 완전히 비활성화 하는 방법이 없기 때문에(관련 논의는 <a href="https://bugs.python.org/issue2506">여기</a>) 간단하게 알려진 내용에 대해서만 작성해보겠습니다.</p>
<h3 id="constantfolding">Constant folding</h3>
<p>Constant folding은 peephole이 가장 많이 최적화하는 부분입니다. 런타임에서 비효율적인 계산 과정을 생략할 수 있도록 유도하고 있으며 이를 통해 줄여지는 바이트코드의 양은 상당한 편입니다.</p>
<h4 id="unaryoperatorsbinaryoperations">unary operators, binary operations</h4>
<p>런타임에서 <code>+a</code>, <code>-a</code>, <code>~a</code>와 같은 코드를 최적화하여 constant로 구성합니다.</p>
<p>즉 아래와 같은 코드는</p>
<pre><code class="language-python">def func():
    return ~1
</code></pre>
<p>최적화되지 않은 상태에는 다음과 같은 바이트코드가 나오지만:</p>
<pre><code>  1           0 LOAD_CONST               1 (1)
              2 UNARY_INVERT
              4 RETURN_VALUE
</code></pre>
<p>최적화된 상태에서는 다음과 같은 바이트코드가 나오게 됩니다:</p>
<pre><code>  1           0 LOAD_CONST               2 (-2)
              3 RETURN_VALUE
</code></pre>
<p>binary operations 최적화의 경우 다른 두 상수간의 <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>//</code>, <code>%</code>, <code>**</code> 연산을 동일한 방식으로 최적화합니다.</p>
<h4 id="build_tupletuple">BUILD_TUPLE: <code>tuple</code></h4>
<p>Python에서 튜플은 크기가 정해져있고 변하지 않기 때문에 튜플 또한 Peephole의 최적화 대상입니다. <code>a = (1, 2, 3, 4, )</code>와 같은 튜플이 있다면 이를 내부적으로 변수(variable) -&gt; 상수(constant)로 변환하는 과정을 거치게 됩니다. 즉 최적화되지 않은 상태의 바이트코드는 다음과 같이 나오지만:</p>
<pre><code>  2           0 LOAD_CONST               1 (1)
              2 LOAD_CONST               2 (2)
              4 LOAD_CONST               3 (3)
              6 LOAD_CONST               4 (4)
              8 BUILD_TUPLE              4
             10 STORE_FAST               0 (a)
             12 LOAD_CONST               0 (None)
             14 RETURN_VALUE
</code></pre>
<p>최적화 후에는 다음과 같이 바뀌게 됩니다:</p>
<pre><code>  2           0 LOAD_CONST               5 ((1, 2, 3, 4))
              3 STORE_FAST               0 (a)
              6 LOAD_CONST               0 (None)
              9 RETURN_VALUE
</code></pre>
<p>이 내용은 <code>tuple</code>에만 한정되는 것이 아닙니다. Peephole은 &quot;변경되지 않을 <code>list</code>&quot;를  찾아 내부적으로 하나의 튜플로 전환하기도 합니다. 예로 <code>if number in [1, 2]: pass</code>라는 코드가 있을 때 <code>[1, 2]</code>는 <code>if</code>문에 직접적으로 쓰여졌기 때문에 런타임에서 변경될 수 없는 부분입니다. 이 코드의 최적화되지 않은 바이트코드는 다음과 같습니다:</p>
<pre><code>  1           0 LOAD_NAME                0 (number)
              2 LOAD_CONST               0 (1)
              4 LOAD_CONST               1 (2)
              6 BUILD_LIST               2
              8 COMPARE_OP               6 (in)
             10 POP_JUMP_IF_FALSE       12
        &gt;&gt;   12 LOAD_CONST               2 (None)
             14 RETURN_VALUE
</code></pre>
<p>이 바이트코드는 Peephole을 거치면 다음과 같이 바뀌게 됩니다:</p>
<pre><code>  1           0 LOAD_NAME                0 (number)
              3 LOAD_CONST               3 ((1, 2))
              6 COMPARE_OP               6 (in)
              9 POP_JUMP_IF_FALSE       12
        &gt;&gt;   12 LOAD_CONST               2 (None)
             15 RETURN_VALUE
</code></pre>
<p>즉, 이미 성능을 염두해두고 고정적으로 쓰이는 배열을 <code>tuple</code>로 사용했던 분들이 많이 계셨겠지만 <code>list</code>로 써도 Python이 내부적으로 바이트코드 최적화를 통해 <code>tuple</code>로 바꿔주는 역할을 해주고 있었던 것입니다.</p>
<h4 id="setfrozenset">Set -&gt; Frozenset</h4>
<p><code>set</code>에 대해서도 동일한 작업을 진행합니다. 변경될 수 없는 곳에 존재하는 <code>set</code>(mutable)을 <code>frozenset</code>(immutable)로 바꾸는 역할을 하죠. 위에서 썼던 코드를 <code>if number in {1, 2}: pass</code>로 바꿔서 다시 확인봅시다. 우선 최적화되지 않은 바이트코드입니다.</p>
<pre><code>  1           0 LOAD_NAME                0 (number)
              2 LOAD_CONST               0 (1)
              4 LOAD_CONST               1 (2)
              6 BUILD_SET                2
              8 COMPARE_OP               6 (in)
             10 POP_JUMP_IF_FALSE       12
        &gt;&gt;   12 LOAD_CONST               2 (None)
             14 RETURN_VALUE
</code></pre>
<p><code>BUILD_LIST</code>만 <code>BUILD_SET</code>으로 바뀌었을 뿐 동일한 바이트코드가 나왔습니다. 이제 최적화된 바이트코드입니다.</p>
<pre><code>  1           0 LOAD_NAME                0 (number)
              3 LOAD_CONST               3 (frozenset({1, 2}))
              6 COMPARE_OP               6 (in)
              9 POP_JUMP_IF_FALSE       12
        &gt;&gt;   12 LOAD_CONST               2 (None)
             15 RETURN_VALUE
</code></pre>
<p>최적화된 바이트코드에서 우리는 자료형이 <code>set</code>에서 <code>fronzenset</code>으로 바뀐 것을 확인할 수 있습니다.</p>
<p>이 구현에서 중요한 부분이 있습니다. 이 최적화 구현은 <a href="https://docs.python.org/3/whatsnew/3.2.html#optimizations">Python 3.2부터</a> 제공되기 시작했고 Python 2에는 구현되지 않았기 때문에 Python 3.2 이상 버전을 사용해야만 이 최적화 혜택을 누릴 수 있습니다(?). 네, Python 3 씁시다. (뿐만 아니라 계속해서 개선되고있는 Peephole 코드는 Python 3에서만 적용되고 있습니다)</p>
<h3 id="compare_op">COMPARE_OP</h3>
<pre><code class="language-python">In [12]: def compare(a: Any, b: Any) -&gt; bool:
    ...:     return not(a is b)
    ...:

In [13]: dis.dis(compare)
  2           0 LOAD_FAST                0 (a)
              3 LOAD_FAST                1 (b)
              6 COMPARE_OP               9 (is not)
              9 RETURN_VALUE
</code></pre>
<p><code>not(a is b)</code>와 같은 코드는 <code>a is not b</code>와 동일한 바이트코드로, <code>not(a is not b)</code>와 같은 코드는 <code>a is b</code>로, <code>not(a in b)</code>는 <code>a in b</code>, <code>not(a not in b)</code>는 <code>a in b</code>로 최적화합니다. 원래대로라면 아래와 같이 <code>COMPARE_OP</code>를 먼저 실행하고 <code>UNARY_NOT</code>으로 NOT 연산을 실행하는 바이트코드가 나오게 됩니다.</p>
<pre><code>  2           0 LOAD_FAST                0 (a)
              2 LOAD_FAST                1 (b)
              4 COMPARE_OP               8 (is)
              6 UNARY_NOT
              8 RETURN_VALUE
</code></pre>
<p>Python 3.7부터 적용되는 내용(<a href="https://bugs.python.org/issue30501">bpo-30501: Make the compiler producing optimized code for condition expressions</a>)에 따르면 <code>if not a and b: x</code>와 같은 코드는 원래 다음과 같은 바이트코드가 나오는데:</p>
<pre><code>  1           0 LOAD_NAME                0 (a)
              2 UNARY_NOT
              4 POP_JUMP_IF_FALSE       14
              6 LOAD_NAME                1 (b)
              8 POP_JUMP_IF_FALSE       14
             10 LOAD_NAME                2 (x)
             12 POP_TOP
        &gt;&gt;   14 LOAD_CONST               0 (None)
             16 RETURN_VALUE
</code></pre>
<p>위 코드의 경우 <code>and</code> 조건이 붙고 <code>not a &amp;&amp; b</code> 이기 때문에 앞에 있는 조건인 <code>not a</code>가 <code>True</code>일 때에는 실행이 되어선 안됩니다. 따라서 Python 3.6까지는 <code>UNARY_NOT</code>으로 한 번 뒤집어주고 <code>POP_JUMP_IF_FALSE</code>로 if문을 건너뛰도록 설계되어 있었지만 Python 3.7부터는 다음과 같이 약간 더 효율적인 바이트코드가 생성(<code>UNARY_NOT</code> + <code>POP_JUMP_IF_FALSE</code>를 <code>POP_JUMP_IF_TRUE</code>로 변환)됩니다:</p>
<pre><code>  1           0 LOAD_NAME                0 (a)
              2 POP_JUMP_IF_TRUE        12
              4 LOAD_NAME                1 (b)
              6 POP_JUMP_IF_FALSE       12
              8 LOAD_NAME                2 (x)
             10 POP_TOP
        &gt;&gt;   12 LOAD_CONST               0 (None)
             14 RETURN_VALUE
</code></pre>
<h3 id="nopinstructions">NOP instructions 제거</h3>
<p>예로 다음 코드를 봅시다.</p>
<pre><code class="language-python">def test():
    if a:
        return 1
    else:
        return 2
</code></pre>
<p>위 코드의 최적화되지 않은 바이트코드는 다음과 같습니다:</p>
<pre><code>  2           0 LOAD_GLOBAL              0 (a)
              2 POP_JUMP_IF_FALSE       10

  3           4 LOAD_CONST               1 (1)
              6 RETURN_VALUE
              8 JUMP_FORWARD             4 (to 14)

  5     &gt;&gt;   10 LOAD_CONST               2 (2)
             12 RETURN_VALUE
        &gt;&gt;   14 LOAD_CONST               0 (None)
             16 RETURN_VALUE
</code></pre>
<p>눈에 띄는 부분이 있나요? 바로<code>8</code>번 OP인 <code>JUMP_FORWARD</code>입니다. Python 인터프리터가 처음 바이트코드를 생성해낼 때는 <code>if</code>문 안에 있는 코드를 실행하고 <code>if</code>문 바깥으로 나가도록 유도하지만 실제로는 <code>RETURN_VALUE</code>가 실행되어 값이 반환되기 때문에 <code>JUMP_FORWARD</code> OP는 아예 필요가 없습니다. 따라서 Peephole은 다음과 같이 바이트코드를 최적화합니다:</p>
<pre><code>  2           0 LOAD_GLOBAL              0 (a)
              3 POP_JUMP_IF_FALSE       10

  3           6 LOAD_CONST               1 (1)
              9 RETURN_VALUE

  5     &gt;&gt;   10 LOAD_CONST               2 (2)
             13 RETURN_VALUE
             14 LOAD_CONST               0 (None)
             17 RETURN_VALUE
</code></pre>
<p>즉, <code>pass</code>나 불필요하게 생성된 바이트코드나 no-op를 제거하는 역할 또한 Peephole이 맡고있는 셈입니다. 실제 <code>Python/peephole.c</code> 파일에서 이와 관련된 코드를 찾을 수 있었습니다:</p>
<pre><code class="language-c">...
/* Remove unreachable ops after RETURN */
case RETURN_VALUE:
    h = i + 1;
    while (h &lt; codelen &amp;&amp; ISBASICBLOCK(blocks, i, h)) {
        h++;
    }
    if (h &gt; i + 1) {
        fill_nops(codestr, i + 1, h);
        nexti = find_op(codestr, h);
    }
    break;
...
</code></pre>
<hr>
<p>컴파일러가 런타임에 알아서 바이트코드를 최적화해주는 부분은 해당 컴파일러를 사용하는 사용자가 힘을 들여 코드를 최적화하지 않아도 내부적으로 간단한 최적화를 해주기 때문에 굉장히 멋지고 아름다운 부분입니다. 이러한 과정을 통해 Python은 매 릴리즈마다 작지만 충분한 성능 향상을 이루고 있고 이는 Python 3.7에 와서 Python 2.7보다 더이상 성능이 뒤쳐지지 않도록 하는 결과를 만들었습니다.</p>
<p><em>이 글에서 설명하는 모든 내용은 <a href="https://github.com/python/cpython/blob/master/Python/peephole.c"><code>Python/peephole.c</code></a> 파일에서 확인할 수 있으며, 코드가 매우 읽기 쉽게 구성되어 있으므로 한 번 쯤 읽어보는 것을 추천드립니다.</em></p>
<p><em>최적화되지 않은 Python 바이트코드는 <a href="https://github.com/python/cpython.git">https://github.com/python/cpython.git</a> 레포를 클론한 후 다음 패치를 적용하면 확인할 수 있습니다: (HEAD: <a href="https://github.com/python/cpython/tree/7028e5986fceeeb73dffb5d5bf8f03d88f73b63d">7028e5986f</a>)</em></p>
<pre><code class="language-diff">diff --git i/Python/compile.c w/Python/compile.c
index 78e797a2c3..cbaa88d4dd 100644
--- i/Python/compile.c
+++ w/Python/compile.c
@@ -5325,10 +5325,6 @@ makecode(struct compiler *c, struct assembler *a)
     if (flags &lt; 0)
         goto error;

-    bytecode = PyCode_Optimize(a-&gt;a_bytecode, consts, names, a-&gt;a_lnotab);
-    if (!bytecode)
-        goto error;
-
     tmp = PyList_AsTuple(consts); /* PyCode_New requires a tuple */
     if (!tmp)
         goto error;
@@ -5339,7 +5335,7 @@ makecode(struct compiler *c, struct assembler *a)
     kwonlyargcount = Py_SAFE_DOWNCAST(c-&gt;u-&gt;u_kwonlyargcount, Py_ssize_t, int);
     co = PyCode_New(argcount, kwonlyargcount,
                     nlocals_int, stackdepth(c), flags,
-                    bytecode, consts, names, varnames,
+                    a-&gt;a_bytecode, consts, names, varnames,
                     freevars, cellvars,
                     c-&gt;c_filename, c-&gt;u-&gt;u_name,
                     c-&gt;u-&gt;u_firstlineno,
</code></pre>
]]></content:encoded></item><item><title><![CDATA[Goroutines vs Threads]]></title><description><![CDATA[<p>Google이 Go 언어를 만들어낸 이후 많은 시스템 관리용 유틸리티, 서버가 Go로 짜여지기 시작했고 매 업데이트마다 엄청난 성능 향상과 발전으로 이제 어디서든 Go 언어로 짜여진 프로그램을 쉽게 만날 수 있게 되었습니다. 특히 이전에 올렸던 <a href="https://tech.ssut.me/2017/04/17/http-server-benchmark/">HTTP Server Benchmark</a> 글에서 보여준 Go의 성능은 많은 이들에게 감명을 줄 수 있는 수준이라 생각됩니다. 이러한 Go의</p>]]></description><link>https://tech.ssut.me/goroutine-vs-threads/</link><guid isPermaLink="false">5997f027f1e09e3b20b3a07d</guid><category><![CDATA[Go]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Sun, 20 Aug 2017 05:34:05 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2017/08/golang.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://tech.ssut.me/content/images/2017/08/golang.jpg" alt="Goroutines vs Threads"><p>Google이 Go 언어를 만들어낸 이후 많은 시스템 관리용 유틸리티, 서버가 Go로 짜여지기 시작했고 매 업데이트마다 엄청난 성능 향상과 발전으로 이제 어디서든 Go 언어로 짜여진 프로그램을 쉽게 만날 수 있게 되었습니다. 특히 이전에 올렸던 <a href="https://tech.ssut.me/2017/04/17/http-server-benchmark/">HTTP Server Benchmark</a> 글에서 보여준 Go의 성능은 많은 이들에게 감명을 줄 수 있는 수준이라 생각됩니다. 이러한 Go의 놀라운 성능 뒤에는 어떤 비결이 숨겨져 있을까요? 단순 컴파일 언어라서, 최적화가 잘 되어있어서 Go가 빠른 것일까요? Goroutines에 대해 알아봅시다.</p>
<h2 id="">프로세서 성능의 한계</h2>
<p>무어의 법칙은 깨진지 꽤 오래됐습니다. 2004년 3GHz CPU가 세상에 선보인 이후로 2017년 공개되고 있는 CPU의 클럭은 약 4GHz 수준입니다. 아 하이엔드만 그렇고 일반적으로 2~3GHz 클럭을 가진 CPU를 아직도 사용하고 있죠.</p>
<p><img src="https://tech.ssut.me/content/images/2017/08/35-years-of-microprocessor-trend-data.png" alt="Goroutines vs Threads"><br>
<img src="https://tech.ssut.me/content/images/2017/08/transistor-costs.png" alt="Goroutines vs Threads"></p>
<p>위 자료로 볼 수 있듯 클럭 속도는 떨어지고 있습니다. 게다가 가격 대비 트랜지스터 수 또한 떨어지기 시작한 상황입니다.하지만 과거와 비교해볼 때 CPU의 성능은 떨어지지 않았습니다. 무엇이 이를 가능하게 해줬을까요? 여기에는 하이퍼스레딩(hyperthreading), 멀티코어(multicore), 캐시(cache) 기술이 큰 도움이 되었습니다.</p>
<p><img src="https://tech.ssut.me/content/images/2017/08/cache-performance.png" alt="Goroutines vs Threads"></p>
<p>캐시는 물리적인 한계를 갖고 있습니다. 캐시가 크면 클수록 그 속도는 더 느려진다는 부분입니다.  현대의 CPU는 캐시와 코어간의 레이턴시를 낮추기 위해 칩 내부에 L1, L2, 그리고 L3 캐시까지 탑재하고 있습니다. 빛의 속도가 한정되어 있듯 거리는 (<em>nm</em>단위라 하더라도)속도를 의미합니다. 캐시는 RAM보다 훨씬 빠르지만 그 속도에는 결국 제한이 존재합니다.</p>
<p><img src="https://tech.ssut.me/content/images/2017/08/amd_zen_ccx.png" alt="Goroutines vs Threads"></p>
<div style="text-align: center; font-style: italic">(AMD Zen 프로세서의 CCX)</div>
<p>지난 몇 년간 CPU 시장(특히 Workstation/Server 시장)은 더 많은 CPU 코어를 추가하는 방식으로 성장해왔습니다. 2005년, 인텔이 본격적으로 멀티코어를 가진 Core 프로세서를 내놓기 시작하면서 높이는 데 한계가 있었던 클럭 상승폭이 주춤하기 시작했고 싱글코어 성능은 10년 단위로 봐도 큰 향상이 없었지만 멀티코어 성능은 매우 큰 폭으로 가파르게 상승하기 시작했습니다. 하지만 많은 코어 수에도 불구하고 수많은 소프트웨어가 이를 제대로 활용하지 못하는 사태가 벌어지기 시작했고(<em>실제 2017년 Ryzen 7 프로세서 출시 이후 CPU 시장이 크게 뒤바뀌는듯 했으나 Adobe 제품군에서 4C8T인 Intel i7-7700K의 성능이 6C12T의 Ryzen 7 1700X를 가볍게 뛰어넘는 등 이는 현재진행형으로 이어지고 있습니다</em>) 이는 결국 CPU 코어를 활용하기 위해 CPU 코어를 가상화(ESXi 등)해서 사용하는 등 비효율적인 멀티코어 활용 방법을 보편화시켰습니다.</p>
<h3 id="">멀티프로세싱</h3>
<p>싱글코어의 성능을 높이는 데에는 한계가 분명히 존재하기 때문에 결국에는 소프트웨어 개발자가 나서서 멀티코어를 제대로 지원할 수 있는 소프트웨어를 만들어내야 합니다. 많은 프로그래밍 언어들은 90년대에 처음 등장했고 싱글 스레드를 사용하는 것이 당연했습니다. 멀티스레딩 또한 가능하긴 했지만 복잡하고 구현이 어려우며 값비싼 런타임 비용 때문에 쉽게 개발자가 접근할 수 없었습니다. Java에서의 스레드를 예로 들어봅시다. Java 스레드는 시작과 동시에 1MB의 스택 공간을 요구로 하고 스레드와 스레드 사이의 보호 공간(guard page)까지 필요로 합니다. 게다가 더 많은 스레드를 생성해낼 수록 heap 공간이 더 적어지는 문제점이 존재합니다.</p>
<p><img src="https://tech.ssut.me/content/images/2017/08/thread-stacks-and-guard-pages.jpg" alt="Goroutines vs Threads"></p>
<p>이 문제를 풀어서 해석해보면 웹 서버를 만든다고 했을 때 요청 당 1개의 스레드를 만들게 되면 결국 우리는 <code>OutOfMemoryError</code>를 만나게 되거나, 메모리 공간을 넘어 프로그램이 <a href="https://tech.ssut.me/2017/08/04/hack-the-virtual-memory-c-strings-and-proc/">가상 메모리</a>에 페이징되면서 전체적인 성능 저하를 겪게 됩니다. 즉 스레드 자체는 프로그램의 성능을 높이는 데에 분명한 도움을 주기는 하지만 멀티프로세싱의 해답이라 보기는 어렵다는 것입니다.</p>
<p>싱글 스레드를 사용하는 Nodejs, Ruby, Python 등의 잘 알려진 여러 스크립트 언어에는 위와 같은 문제가 분명히 존재하고(위 문제는 Java에만 국한되는 것이 아닌 OS의 스레드를 사용하는 모든 프로그래밍 언어가 포함됩니다) 이를 해결하기 위해 OS 프로세서당 1개의 프로세스를 띄우고 로드밸런싱(LB)을 구성하는 오래됐지만 안정적인 방법을 꽤 오래 전부터 사용해왔습니다.</p>
<h2 id="goroutinesingo">Goroutines in Go</h2>
<p>Go의 동시성 모델인 고루틴(Goroutines)은 기존의 낡은 방식에서 벗어나 새로운 방식의 멀티프로세싱을 제공합니다. 한 프로세스로 모든 코어를 활용하면서 적은 메모리를 사용하는 방식 말이죠. 이에 대한 글(+번역글)은 수도없이 많이 나와있으니 해당 글들을 찾아보시기 바랍니다. (<a href="https://stonzeteam.github.io/How-Goroutines-Work/">고루틴은 어떻게 동작하는가</a>)</p>
<p>고루틴은 M:N 스레드 모델(LWP)을 사용하고 있습니다. 따라서 기존의 스레드/스레드 풀 방식보다 훨씬 가볍고 빠른 특성을 지니고 있습니다.</p>
<h3 id="threads">스레드(Threads)의 종류</h3>
<p>스레드는 다음과 같이 분류할 수 있습니다:</p>
<ul>
<li><code>N:1</code>: 여러 user-level 스레드가 하나의 OS 스레드 위에서 돌아갑니다.
<ul>
<li>컨텍스트 스위치(context switching) 속도가 빠릅니다</li>
<li>멀티코어를 활용할 수 없습니다</li>
</ul>
</li>
<li><code>1:1</code>: 1개의 스레드는 1개의 OS 스레드와 일치합니다.
<ul>
<li>멀티코어를 제대로 활용할 수 있습니다</li>
<li>컨텍스트 스위치 속도가 매우 느립니다</li>
</ul>
</li>
<li><code>M:N</code>: 여러개의 OS 스레드 위에 여러개의 고루틴을 돌립니다.
<ul>
<li>컨텍스트 스위치 속도도 빠르고 멀티코어도 활용할 수 있습니다</li>
<li>구현이 어렵습니다</li>
</ul>
</li>
</ul>
<p>Go의 경우 M:N 모델을 채택함과 동시에 구현이 어렵다는 단점을 언어 차원에서 구현하여 제공함으로 해소시켰습니다.</p>
<h3 id="go">Go 스케쥴러 내부</h3>
<p>Go의 스케쥴러는 G, M, P로 구성되어 돌아가고 있습니다.</p>
<h4 id="g">G</h4>
<pre><code class="language-go">type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
	stack       stack   // offset known to runtime/cgo
	stackguard0 uintptr // offset known to liblink
	stackguard1 uintptr // offset known to liblink

	_panic         *_panic // innermost panic - offset known to liblink
	_defer         *_defer // innermost defer
	m              *m      // current m; offset known to arm liblink
	sched          gobuf
	syscallsp      uintptr        // if status==Gsyscall, syscallsp = sched.sp to use during gc
	syscallpc      uintptr        // if status==Gsyscall, syscallpc = sched.pc to use during gc
	stktopsp       uintptr        // expected sp at top of stack, to check in traceback
	param          unsafe.Pointer // passed parameter on wakeup
	atomicstatus   uint32
	stackLock      uint32 // sigprof/scang lock; TODO: fold in to atomicstatus
	goid           int64
	waitsince      int64  // approx time when the g become blocked
	waitreason     string // if status==Gwaiting
	schedlink      guintptr
	preempt        bool     // preemption signal, duplicates stackguard0 = stackpreempt
	paniconfault   bool     // panic (instead of crash) on unexpected fault address
	preemptscan    bool     // preempted g does scan for gc
	gcscandone     bool     // g has scanned stack; protected by _Gscan bit in status
	gcscanvalid    bool     // false at start of gc cycle, true if G has not run since last scan; TODO: remove?
	throwsplit     bool     // must not split stack
	raceignore     int8     // ignore race detection events
	sysblocktraced bool     // StartTrace has emitted EvGoInSyscall about this goroutine
	sysexitticks   int64    // cputicks when syscall has returned (for tracing)
	traceseq       uint64   // trace event sequencer
	tracelastp     puintptr // last P emitted an event for this goroutine
	lockedm        *m
	sig            uint32
	writebuf       []byte
	sigcode0       uintptr
	sigcode1       uintptr
	sigpc          uintptr
	gopc           uintptr // pc of go statement that created this goroutine
	startpc        uintptr // pc of goroutine function
	racectx        uintptr
	waiting        *sudog         // sudog structures this g is waiting on (that have a valid elem ptr); in lock order
	cgoCtxt        []uintptr      // cgo traceback context
	labels         unsafe.Pointer // profiler labels
	timer          *timer         // cached timer for time.Sleep
	selectDone     uint32         // are we participating in a select and did someone win the race?

	// Per-G GC state

	// gcAssistBytes is this G's GC assist credit in terms of
	// bytes allocated. If this is positive, then the G has credit
	// to allocate gcAssistBytes bytes without assisting. If this
	// is negative, then the G must correct this by performing
	// scan work. We track this in bytes to make it fast to update
	// and check for debt in the malloc hot path. The assist ratio
	// determines how this corresponds to scan work debt.
	gcAssistBytes int64
}
</code></pre>
<p>G는 Goroutines, 고루틴을 의미합니다.</p>
<h4 id="m">M</h4>
<pre><code class="language-go">type m struct {
	g0      *g     // goroutine with scheduling stack
	morebuf gobuf  // gobuf arg to morestack
	divmod  uint32 // div/mod denominator for arm - known to liblink

	// Fields not known to debuggers.
	procid        uint64     // for debuggers, but offset not hard-coded
	gsignal       *g         // signal-handling g
	sigmask       sigset     // storage for saved signal mask
	tls           [6]uintptr // thread-local storage (for x86 extern register)
	mstartfn      func()
	curg          *g       // current running goroutine
	caughtsig     guintptr // goroutine running during fatal signal
	p             puintptr // attached p for executing go code (nil if not executing go code)
	nextp         puintptr
	id            int32
	mallocing     int32
	throwing      int32
	preemptoff    string // if != &quot;&quot;, keep curg running on this m
	locks         int32
	softfloat     int32
	dying         int32
	profilehz     int32
	helpgc        int32
	spinning      bool // m is out of work and is actively looking for work
	blocked       bool // m is blocked on a note
	inwb          bool // m is executing a write barrier
	newSigstack   bool // minit on C thread called sigaltstack
	printlock     int8
	incgo         bool // m is executing a cgo call
	fastrand      uint32
	ncgocall      uint64      // number of cgo calls in total
	ncgo          int32       // number of cgo calls currently in progress
	cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily
	cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call
	park          note
	alllink       *m // on allm
	schedlink     muintptr
	mcache        *mcache
	lockedg       *g
	createstack   [32]uintptr // stack that created this thread.
	freglo        [16]uint32  // d[i] lsb and f[i]
	freghi        [16]uint32  // d[i] msb and f[i+16]
	fflag         uint32      // floating point compare flags
	locked        uint32      // tracking for lockosthread
	nextwaitm     uintptr     // next m waiting for lock
	needextram    bool
	traceback     uint8
	waitunlockf   unsafe.Pointer // todo go func(*g, unsafe.pointer) bool
	waitlock      unsafe.Pointer
	waittraceev   byte
	waittraceskip int
	startingtrace bool
	syscalltick   uint32
	thread        uintptr // thread handle

	// these are here because they are too large to be on the stack
	// of low-level NOSPLIT functions.
	libcall   libcall
	libcallpc uintptr // for cpu profiler
	libcallsp uintptr
	libcallg  guintptr
	syscall   libcall // stores syscall parameters on windows

	mOS
}
</code></pre>
<p>M은 Machine, OS의 스레드를 뜻합니다. 표준 POSIX 스레드를 따릅니다.</p>
<h4 id="p">P</h4>
<pre><code class="language-go">type p struct {
	lock mutex

	id          int32
	status      uint32 // one of pidle/prunning/...
	link        puintptr
	schedtick   uint32     // incremented on every scheduler call
	syscalltick uint32     // incremented on every system call
	sysmontick  sysmontick // last tick observed by sysmon
	m           muintptr   // back-link to associated m (nil if idle)
	mcache      *mcache
	racectx     uintptr

	deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)
	deferpoolbuf [5][32]*_defer

	// Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen.
	goidcache    uint64
	goidcacheend uint64

	// Queue of runnable goroutines. Accessed without lock.
	runqhead uint32
	runqtail uint32
	runq     [256]guintptr
	// runnext, if non-nil, is a runnable G that was ready'd by
	// the current G and should be run next instead of what's in
	// runq if there's time remaining in the running G's time
	// slice. It will inherit the time left in the current time
	// slice. If a set of goroutines is locked in a
	// communicate-and-wait pattern, this schedules that set as a
	// unit and eliminates the (potentially large) scheduling
	// latency that otherwise arises from adding the ready'd
	// goroutines to the end of the run queue.
	runnext guintptr

	// Available G's (status == Gdead)
	gfree    *g
	gfreecnt int32

	sudogcache []*sudog
	sudogbuf   [128]*sudog

	tracebuf traceBufPtr

	// traceSweep indicates the sweep events should be traced.
	// This is used to defer the sweep start event until a span
	// has actually been swept.
	traceSweep bool
	// traceSwept and traceReclaimed track the number of bytes
	// swept and reclaimed by sweeping in the current sweep loop.
	traceSwept, traceReclaimed uintptr

	palloc persistentAlloc // per-P to avoid mutex

	// Per-P GC state
	gcAssistTime     int64 // Nanoseconds in assistAlloc
	gcBgMarkWorker   guintptr
	gcMarkWorkerMode gcMarkWorkerMode

	// gcw is this P's GC work buffer cache. The work buffer is
	// filled by write barriers, drained by mutator assists, and
	// disposed on certain GC state transitions.
	gcw gcWork

	runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point

	pad [sys.CacheLineSize]byte
}
</code></pre>
<p>P는 Processor,  프로세서를 뜻합니다. 정확히 말해서 P는 스케쥴링에 대한 context를 지니고 있습니다.</p>
<h2 id="">스케쥴러의 작동 원리</h2>
<p><img src="https://tech.ssut.me/content/images/2017/08/in-motion.jpg" alt="Goroutines vs Threads"></p>
<p>위에서 말했듯 M은 스레드(machine), P는 프로세서(context), G는 고루틴(goroutine)을 의미합니다. context인 P의 갯수는 <code>GOMAXPROCS</code> 환경 변수로 설정이 되는데 Go 런타임에서 <code>runtime.GOMAXPROCS()</code> 함수로 그 갯수가 조절될 수도 있습니다. 대부분 프로그램을 처음 실행할 때에 이 값을 설정하기 때문에 Go 애플리케이션이 돌아가는 도중에 이 값이 바뀔 일은 거의 없죠.</p>
<p>회색으로 나타나는 부분은 아직 실행중이지는 않지만 스케쥴될 준비가 되어있는 고루틴입니다. 회색으로 나타나있는 부분을 <code>runqueues</code>라고 부르는데 이 큐는 앞으로 실행될 고루틴을 쌓아두는 역할을 하고 P(context)에 종속되어 있습니다. 우리는 한 스레드는 한 개의 작업을 할 수 있다는 사실을 알고 있습니다. 이에 따라 Go의 스케쥴러는 <code>runqueues</code>에 앞으로 실행될 고루틴을 순차적(sequential)으로 쌓아두고 이미 실행중인 고루틴이 작업을 마쳤거나(한 번의 statement 실행도 해당) syscall(후술)을 했을 경우에 큐에서 다음 작업을 꺼내 실행하고 새로 추가되는 작업(이미 돌아가고 있는 작업이거나 새로운 고루틴)을 local runqueue의 가장 끝 부분에 추가하게 됩니다.</p>
<h3 id="syscallblocking">syscall이 발생했을 때 (blocking)</h3>
<p><img src="https://tech.ssut.me/content/images/2017/08/syscall.jpg" alt="Goroutines vs Threads"></p>
<p>Go 코드를 실행하던 도중 syscall이 발생(주로 I/O)하게 되면 blocking이 발생하게 되는데 이 경우 해당 스레드에 영향을 끼치기 때문에 성능 저하를 불러올 수 있게 됩니다. 따라서 Go의 스케쥴러는 계속해서 멈추지 않고 스케쥴링을 할 수 있도록 syscall이 발생한 고루틴을 다른 스레드로 넘겨(hand off) 모든 고루틴이 정상적으로 작동할 수 있도록 보장합니다. 이후에 syscall 처리가 끝난 고루틴은 잠시 넘겼던 P(context)를 다시 찾아와서 붙게 되거나 global runqueues에 들어가게 됩니다.</p>
<p>이에 대한 글이 Go Google Groups에 올라왔었는데 제목은 <a href="https://groups.google.com/forum/#!topic/golang-nuts/2IdA34yR8gQ">왜 고루틴 1000개를 만들었는데 OS 스레드 1000개가 생기나요?</a> 입니다. 앞서 우리는 고루틴은 스레드 위에서 다중화하여 돌아간다고 배웠습니다. 그런데 왜그럴까요? 이 파트에서 설명하는 내용 때문입니다. 위 이미지에서 <code>G0</code>이 돌아가다가 syscall을 만났을 때 이는 다음 고루틴 실행을 지연시킬 수 있으며 스레드 성능에 영향을 끼치게 됩니다. 따라서 Go의 스케쥴러는 syscall이 발생한 고루틴을 독립적으로 처리하도록 하면서 나머지 context는 다른 스레드(위 이미지에서 <code>M1</code>)로 넘기게 됩니다.</p>
<p>이것이 바로 M에 P(context)가 붙는 이유입니다. 실행중인 스레드가 blocking 되었을 때 다른 스레드로 현재 상태(state)를 그대로 넘겨 지속적으로 처리를 할 수 있도록 보장하기 위해 P(context)가 구현되었고 이는 <code>GOMAXPROCS</code>값이 1이더라도 Go가 멀티스레드로 동작하는 이유입니다.</p>
<h3 id="">효율적인 스레드 스케쥴링: 고루틴 훔쳐가기</h3>
<p><img src="https://tech.ssut.me/content/images/2017/08/steal.jpg" alt="Goroutines vs Threads"></p>
<p>위의 예는 <code>GOMAXPROCS</code>가 2로 설정된 경우입니다. 두 개의 M이 존재하고 왼쪽에 있는 M(편의상 <code>M0</code>이라 부르겠습니다)은 1개의 고루틴을 처리하고 있고 3개의 고루틴을 runqueues에 지니고 있는 상태입니다. 즉 unbalanced 상태에 놓여져 있습니다. 이 상황은 Go 런타임에서 생각보다 자주 등장하게 됩니다. <code>M0</code>이 더 무거운 작업을 처리하고 있다면 이러한 상황이 빈번히 발생하게 되겠죠. (과거 Go에는 global runqueues가 있었지만 lock-release 문제로 P마다 own runqueues를 할당하는 방식으로 변경되었습니다)</p>
<p>위 상황에서 Go 스케쥴러는 가장 우선적으로 global runqueues에 쌓여있는 고루틴을 먼저 가져오려 시도하고 그 후에는  <code>M0</code>에 있는 고루틴 4개의 절반인 2개(<code>Gm</code>과 <code>G</code>)를 <code>M1</code>에 재배치하게 되고 이 상황을 steals(훔쳐가기)라고 표현합니다. 그 결과 절반을 가져가서 <code>Gm</code>을 바로 <code>M1</code> 스레드에서 실행할 수 있게 되고 전반적으로 볼 때 Go는 놀지 않고 많은 작업을 더 효율적으로 동시에 처리할 수 있게 됩니다.</p>
<h3 id="cs">고루틴의 CS 시점</h3>
<p>시스템 콜이 발생하면 고루틴의 CS가 이루어진다는 것을 알았습니다. 그러면 정확히 어느 시점에서 고루틴의 CS가 이루어질까요? 고루틴의 CS는 다음 시점에서 이루어집니다:</p>
<ul>
<li>unbuffered 채널에 접근할 때(읽거나 쓸 때)</li>
<li>시스템 I/O가 발생했을 때</li>
<li>메모리가 할당되었을 때</li>
<li><code>time.Sleep()</code> 코드 실행(python asyncio에서 <code>asyncio.sleep()</code>을 이용해 yield하는 것과 유사합니다)</li>
<li><code>runtime.Gosched()</code> 코드 실행</li>
</ul>
<hr>
<p>고루틴과 스레드의 간단한 비교부터 고루틴 내부의 작동 원리(스케쥴링 방식)까지 알아보았습니다. 고루틴은 M:N 스레드 모델로 볼 때 세상에 없던 모델도 아니고 아무도 상상하지 못했던 모델도 아닙니다. 하지만 GIL(Global Interpreter Lock) 또는 여러 이유에서 멀티코어를 제대로 활용하지 못하고 있는 대부분 스크립트 언어와 비교해볼 때 언어 차원에서 다중화(multiplexed)된 스레드 모델을 제공한다는 점은 Go의 가장 큰 장점이면서 오늘날 Go가 이렇게 크게 성장하는데 큰 원동력이 되었습니다.</p>
<p>다음 글에서는 <strong>고루틴의 제대로 된 사용 방법</strong>과 <strong>고루틴에 대한 오해와 진실</strong>을 알아보도록 하겠습니다. :)</p>
]]></content:encoded></item><item><title><![CDATA[Docker(container)의 작동 원리: namespaces and cgroups]]></title><description><![CDATA[<p>리눅스 컨테이너(LXC) 기술이 등장한 이후로 전가상화(full virtualization) 및 반가상화(para virtualization)의 시대가 저물어버렸습니다. Docker는 LXC에서 사용하는 리눅스 커널 컨테이너 기술을 이용해 만든 컨테이터 관리 유틸리티로 <a href="https://en.wikipedia.org/wiki/Microservices">마이크로서비스</a> 전환은 물론 DevOps, 테스팅 등 다양한 분야에서 많은 사랑을 받고 있어 리눅스 컨테이너 구현체의 사실상(de-facto) 업계 표준이 되었습니다.</p>
<p>Docker에 대한</p>]]></description><link>https://tech.ssut.me/what-even-is-a-container/</link><guid isPermaLink="false">5989a358b053c786c613ecbd</guid><category><![CDATA[docker]]></category><category><![CDATA[container]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Tue, 15 Aug 2017 13:05:53 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2017/08/maxresdefault-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://tech.ssut.me/content/images/2017/08/maxresdefault-1.jpg" alt="Docker(container)의 작동 원리: namespaces and cgroups"><p>리눅스 컨테이너(LXC) 기술이 등장한 이후로 전가상화(full virtualization) 및 반가상화(para virtualization)의 시대가 저물어버렸습니다. Docker는 LXC에서 사용하는 리눅스 커널 컨테이너 기술을 이용해 만든 컨테이터 관리 유틸리티로 <a href="https://en.wikipedia.org/wiki/Microservices">마이크로서비스</a> 전환은 물론 DevOps, 테스팅 등 다양한 분야에서 많은 사랑을 받고 있어 리눅스 컨테이너 구현체의 사실상(de-facto) 업계 표준이 되었습니다.</p>
<p>Docker에 대한 사용 방법은 <a href="http://pyrasis.com/docker.html">가장 빨리 만나는 Docker</a>를 읽어보시는 것을 추천드립니다. 이 글은 Docker의 핵심 기술로 쓰이는 리눅스 커널의 cgroups와 namespaces를 알아보는 글입니다.</p>
<h2 id="container">Container는 가상머신이다?</h2>
<p>Container는 hypervisor와 완전히 다릅니다. 궁극적으로는 hypervisor와 유사한 형태의 &quot;가상화&quot;를 목표로 하고 있지만 hypervisor는 OS 및 커널이 통째로 가상화되는 반면에 container는 간단히 보면 filesystem의 가상화만을 이루고 있습니다. container는 호스트 PC의 커널을 공유하고 따라서 init(1) 등의 프로세스가 떠있을 필요가 없으며, 따라서 가상화 프로그램과는 다르게 적은 메모리 사용량, 적은 overhead를 보입니다.</p>
<h3 id="containervsvm">Container vs VM의 성능 차이</h3>
<p><img src="https://tech.ssut.me/content/images/2017/08/maxresdefault.jpg" alt="Docker(container)의 작동 원리: namespaces and cgroups"><br>
실제 하드웨어인 것처럼 에뮬레이션(emulation)을 하는 VM과 달리 container는 호스트 PC의 자원을 격리(isolation)된 상태로 그대로 활용하기 때문에 VM에 비해 성능 저하가 눈에 띄게 적습니다.</p>
<p><img src="https://tech.ssut.me/content/images/2017/08/docker_benchmarks.jpg" alt="Docker(container)의 작동 원리: namespaces and cgroups"><br>
실제 온라인상에 돌아다니는 많은 벤치마크 자료를 통해 이를 입증할 수 있으며 container 사용으로 인한 성능 손실은 native에 비해 1% 수준에 불과합니다. 이는 가상화(virtualization)를 하지 않는 부분에서 오는 container의 가장 큰 장점입니다.</p>
<h2 id="">컨테이너 기술</h2>
<h3 id="namespaces">namespaces</h3>
<p>VM에서는 각 게스트 머신별로 독립적인 공간을 제공하고 서로가 충돌하지 않도록 하는 기능을 갖고 있습니다. 리눅스에서는 이와 동일한 역할을 하는 namespaces 기능을 커널에 내장하고 있습니다. 글을 쓰는 시점을 기준으로 현재 리눅스 커널에서는 다음 6가지 namespace를 지원하고 있습니다:</p>
<ul>
<li>mnt (파일시스템 마운트): 호스트 파일시스템에 구애받지 않고 독립적으로 파일시스템을 마운트하거나 언마운트 가능</li>
<li>pid (프로세스): 독립적인 프로세스 공간을 할당</li>
<li>net (네트워크): namespace간에 network 충돌 방지 (중복 포트 바인딩 등)</li>
<li>ipc (SystemV IPC): 프로세스간의 독립적인 통신통로 할당</li>
<li>uts (hostname): 독립적인 hostname 할당</li>
<li>user (UID): 독립적인 사용자 할당</li>
</ul>
<p>namespaces를 지원하는 리눅스 커널을 사용하고 있다면 다음 명령어를 통해 바로 namespace를 만들어 실행할 수 있습니다, 여기서는 간단하게 PID namespace를 띄워봅시다:</p>
<pre><code class="language-bash">$ sudo unshare --fork --pid --mount-proc bash
</code></pre>
<p>자 이제 독립적인 공간이 할당됐을까요?</p>
<pre><code class="language-bash">root@ssut:~# ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  4.0  0.0  17656  6924 pts/9    S    22:06   0:00 bash
root         2  0.0  0.0  30408  1504 pts/9    R+   22:06   0:00 ps aux
</code></pre>
<p>네, 잘 작동하고 있습니다. PID namespace에 실행한 bash가 PID 1로 할당되어 있고(일반적으로 <code>init(커널)</code>이 PID 1) 바로 다음으로 실행한 &quot;ps aux&quot; 명령어가 PID 2를 배정받았습니다.</p>
<p>이 PID namespace 안에서 실행한 프로세스를 밖에서도 볼 수 있을까요? 한 번 확인해보죠.</p>
<pre><code class="language-bash">root@ssut:~# top
...
</code></pre>
<pre><code class="language-bash">ssut@ssut:~$ ps aux | grep top
root      9710  0.0  0.0  34716  2988 pts/9    S+   22:29   0:00 top
</code></pre>
<p>우리가 가둬서 실행했던 PID namespace 밖의 공간(regular namespace)에서도 프로세스를 확인할 수 있다는 것을 알았습니다. 즉, namespaces 기능은 같은 공간을 공유하되 <em>조금 더 제한된 공간</em>을 할당해주는 것이라 볼 수 있습니다.</p>
<p>namespace를 통해 독립적인 공간을 할당한 후에는 <code>nsenter</code>라는 명령어를 통해 이미 돌아가고 있는 namespace 공간에 접근할 수 있습니다. 이 명령어는 명령어 이름 자체가 암시하고 있듯 <strong>n</strong>ame<strong>s</strong>pace <strong>enter</strong>의 약자입니다. Docker에서는 <code>docker exec</code>가 이와 비슷한 역할을 하고 있습니다. (단 <code>nsenter</code>의 경우 <code>docker exec</code>와는 다르게 cgroups에 들어가지 않기 때문에 리소스 제한의 영향을 받지 않습니다)</p>
<h3 id="cgroupscontrolgroups">cgroups (Control Groups)</h3>
<p>cgroups(Control Groups)는 자원(resources)에 대한 제어를 가능하게 해주는 리눅스 커널의 기능입니다. cgroups는 다음 리소스를 제어할 수 있습니다:</p>
<ul>
<li>메모리</li>
<li>CPU</li>
<li>I/O</li>
<li>네트워크</li>
<li>device 노드(<code>/dev/</code>)</li>
</ul>
<p>실행중인 프로그램의 메모리를 제한해볼까요? 일단 &quot;ssut&quot; 유저가 소유하고 메모리를 제어할 <code>testgrp</code>를 생성해봅시다.</p>
<pre><code class="language-bash">ssut@ssut:~$ sudo cgcreate -a ssut -g memory:testgrp
ssut@ssut:~$ ls -alh /sys/fs/cgroup/memory/testgrp
합계 0
drwxr-xr-x 2 ssut root 0  8월  8 23:19 .
dr-xr-xr-x 8 root root 0  7월  7 15:30 ..
-rw-r--r-- 1 ssut root 0  8월  8 23:19 cgroup.clone_children
--w--w--w- 1 ssut root 0  8월  8 23:19 cgroup.event_control
-rw-r--r-- 1 ssut root 0  8월  8 23:19 cgroup.procs
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.failcnt
--w------- 1 ssut root 0  8월  8 23:19 memory.force_empty
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.failcnt
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.limit_in_bytes
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.max_usage_in_bytes
-r--r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.slabinfo
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.tcp.failcnt
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.tcp.limit_in_bytes
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.tcp.max_usage_in_bytes
-r--r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.tcp.usage_in_bytes
-r--r--r-- 1 ssut root 0  8월  8 23:19 memory.kmem.usage_in_bytes
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.limit_in_bytes
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.max_usage_in_bytes
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.move_charge_at_immigrate
-r--r--r-- 1 ssut root 0  8월  8 23:19 memory.numa_stat
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.oom_control
---------- 1 ssut root 0  8월  8 23:19 memory.pressure_level
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.soft_limit_in_bytes
-r--r--r-- 1 ssut root 0  8월  8 23:19 memory.stat
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.swappiness
-r--r--r-- 1 ssut root 0  8월  8 23:19 memory.usage_in_bytes
-rw-r--r-- 1 ssut root 0  8월  8 23:19 memory.use_hierarchy
-rw-r--r-- 1 ssut root 0  8월  8 23:19 notify_on_release
-rw-r--r-- 1 root root 0  8월  8 23:19 tasks
</code></pre>
<p><code>/sys/fs/cgroup/*/groupname</code> 경로에 있는 파일을 통해 그룹의 여러 옵션을 변경할 수 있습니다. 최대 메모리 사용량을 2MB로 제한해볼까요? <code>memory.kmem.limit_in_bytes</code> 파일을 다음 명령어로 수정해줍시다:</p>
<pre><code class="language-bash">ssut@ssut:~$ echo 2000000 &gt; /sys/fs/cgroup/memory/testgrp/memory.kmem.limit_in_bytes
</code></pre>
<p>이제 우리가 생성한 cgroup에서 bash 셸을 실행시켜 메모리 제한이 잘 먹히나 확인해봅시다:</p>
<pre><code class="language-bash">ssut@ssut:~$ sudo cgexec -g memory:testgrp bash
root@ssut:~# top
top: error while loading shared libraries: libgpg-error.so.0: failed to map segment from shared object
</code></pre>
<p>메모리 제한이 잘 되는 것을 확인할 수 있습니다. 이를 통해 container에서는 VM에서와 동일하게 리소스 할당량을 제한할 수 있게 됩니다.</p>
<h2 id="lxclibcontainerrunc"><code>lxc</code>, <code>libcontainer</code>, <code>runc</code>...</h2>
<p><img src="https://tech.ssut.me/content/images/2017/08/QVNR6.png" alt="Docker(container)의 작동 원리: namespaces and cgroups"></p>
<p>LXC, LibContainer, runC 등은 위에서 설명한 cgroups, namespaces를 표준으로 정의해둔 OCI(Open Container Initative) 스펙을 구현한 컨테이너 기술의 구현체입니다. LXC는 캐노니컬(Canonical)이 지원하고 있는 리눅스 컨테이너 프로젝트로 Docker의 경우 1.8 이전 버전까지 LXC를 이용해 구현해서 사용했었습니다. 이후에 Docker는 libcontainer -&gt; <a href="https://blog.docker.com/2015/06/runc/">runC</a> (libcontainer의 리팩토링 구현체)로 자체 구현체를 갖게 되었습니다.</p>
<h2 id="docker">Docker</h2>
<p><img src="https://tech.ssut.me/content/images/2017/08/docker.png" alt="Docker(container)의 작동 원리: namespaces and cgroups"></p>
<p>docker는 1.11버전부터 실제로 위와 같은 구조로 작동하고 있습니다. containerd는 OCI 구현체(주로 runC)를 이용해 container를 관리해주는 daemon입니다. Docker engine 자체는 이미지, 네트워크, 디스크 등의 관리 역할을 하고 있으며, 여기서 Docker engine과 containerd 각각이 완전히 분리된 덕분에 Docker engine 버전을 올릴 때 Docker engine을 재시작해도 container의 재시작 없이 사용할 수 있게 됩니다.</p>
<p>위와 같이 docker에서 각각의 역할이 분리됨에 따라 docker는 4개의 독립적인 프로세스로 작동하고 있습니다. <code>ps aux | grep docker</code> 명령으로 확인해보면 <code>docker</code>, <code>docker-containerd</code>, <code>docker-containerd-shim</code>, <code>docker-runc</code> 4개의 프로세스가 돌아가고 있는 것을 확인할 수 있습니다.<br>
<img src="https://tech.ssut.me/content/images/2017/08/docker.PNG" alt="Docker(container)의 작동 원리: namespaces and cgroups"></p>
]]></content:encoded></item><item><title><![CDATA[가상 메모리 파헤치기: 2. Python bytes]]></title><description><![CDATA[<p>이 글에서 우리는 앞서 알아봤던 <a href="https://tech.ssut.me/2017/08/04/hack-the-virtual-memory-c-strings-and-proc/">1. C strings &amp; /proc</a>와 비슷한 일을 해볼 겁니다, 하지만 이번에는 실행중인 Python 3 스크립트가 대상입니다. 쉽게 끝나지 않을겁니다. 이를 통해 Python 3 내부가 어떻게 돼있는 지도 확인해봅시다.</p>
<blockquote>
<p>들어가기 전에:<br>
이 글은 <a href="https://blog.holbertonschool.com/">Holberton school</a>에서 연재되고 있는 <a href="https://blog.holbertonschool.com/hack-the-virtual-memory-c-strings-proc/">Hack The Virtual Memory</a>의 번역본입니다.</p>
</blockquote>
<h2 id="">필요한</h2>]]></description><link>https://tech.ssut.me/hack-the-virtual-memory-python-bytes/</link><guid isPermaLink="false">59847cd3b053c786c613ecb5</guid><category><![CDATA[Hack The Virtual Memory]]></category><dc:creator><![CDATA[SuHun Han]]></dc:creator><pubDate>Sat, 05 Aug 2017 01:26:05 GMT</pubDate><media:content url="https://tech.ssut.me/content/images/2017/08/hacke_the_vm_1.png" medium="image"/><content:encoded><![CDATA[<img src="https://tech.ssut.me/content/images/2017/08/hacke_the_vm_1.png" alt="가상 메모리 파헤치기: 2. Python bytes"><p>이 글에서 우리는 앞서 알아봤던 <a href="https://tech.ssut.me/2017/08/04/hack-the-virtual-memory-c-strings-and-proc/">1. C strings &amp; /proc</a>와 비슷한 일을 해볼 겁니다, 하지만 이번에는 실행중인 Python 3 스크립트가 대상입니다. 쉽게 끝나지 않을겁니다. 이를 통해 Python 3 내부가 어떻게 돼있는 지도 확인해봅시다.</p>
<blockquote>
<p>들어가기 전에:<br>
이 글은 <a href="https://blog.holbertonschool.com/">Holberton school</a>에서 연재되고 있는 <a href="https://blog.holbertonschool.com/hack-the-virtual-memory-c-strings-proc/">Hack The Virtual Memory</a>의 번역본입니다.</p>
</blockquote>
<h2 id="">필요한 준비물</h2>
<p>이 글을 완전히 습득하기 위해 이러한 것들을 알고 있어야 합니다:</p>
<ul>
<li>C 프로그래밍 언어 기초</li>
<li>약간의 Python 지식</li>
<li>아주 간략한 리눅스 파일시스템과 셸에 대한 이해</li>
<li><code>/proc</code> 파일시스템에 대한 기초 이해 (<a href="https://tech.ssut.me/2017/08/04/hack-the-virtual-memory-c-strings-and-proc/">1. C strings &amp; /proc</a> 글을 먼저 보시는 것을 추천드립니다)</li>
</ul>
<h2 id="">환경</h2>
<p>모든 스크립트와 프로그램은 다음 환경에서 테스트됐습니다:</p>
<ul>
<li>Ubuntu 14.04 LTS
<ul>
<li>Linux ubuntu 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux</li>
</ul>
</li>
<li>gcc
<ul>
<li>gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4</li>
</ul>
</li>
<li>Python 3
<ul>
<li>Python 3.4.3 (default, Nov 17 2016, 01:08:31)</li>
<li>[GCC 4.8.4] on linux</li>
</ul>
</li>
</ul>
<h2 id="python">Python 스크립트</h2>
<p>먼저 다음 스크립트를 이용해 돌고 있는 프로세스의 가상 메모리에서 &quot;Holberton&quot; 문자열을 찾아 바꿔봅시다:</p>
<pre><code class="language-python">#!/usr/bin/env python3
'''
Prints a b&quot;string&quot; (bytes object), reads a char from stdin
and prints the same (or not :)) string again
'''

import sys

s = b&quot;Holberton&quot;
print(s)
sys.stdin.read(1)
print(s)
</code></pre>
<h2 id="bytes">bytes 객체에 대해</h2>
<h3 id="bytesvsstr">bytes vs str</h3>
<p>우리가 확인할 수 있듯, 우리는 문자열을 담기 위해 bytes 객체를 사용(<code>b</code> 리터럴을 문자열 가장 앞에 붙였습니다)했습니다. bytes 타입은 문자열의 각 문자를 bytes(또는 경우에 따라 multibytes로 -- <code>unicodeobject.h</code> 파일을 통해 Python 3가 어떻게 문자열을 인코드하는지 알 수 있습니다)로 메모리에 기록합니다.<br>
이는 string이 실행중인 스크립트 프로세스의 가상 메모리에서 ASCII 값의 연속일 것이라는 것을 확실하게 해줍니다.</p>
<p><em>기술적으로 <code>s</code>는 Python 문자열이 아닙니다.</em>(하지만 여기서 중요한 부분은 아닙니다):</p>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ python3
Python 3.4.3 (default, Nov 17 2016, 01:08:31) 
[GCC 4.8.4] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; s = &quot;Betty&quot;
&gt;&gt;&gt; type(s)
&lt;class 'str'&gt;
&gt;&gt;&gt; s = b&quot;Betty&quot;
&gt;&gt;&gt; type(s)
&lt;class 'bytes'&gt;
&gt;&gt;&gt; quit()
</code></pre>
<h3 id="object">모든 것은 객체(object)입니다</h3>
<p>Python에서의 모든 것은 객체입니다: 정수(integers), 문자열(strings), 바이트 배열(bytes), 함수(functions)를 포함해서 모든 것이 말이죠. 따라서 <code>s = b&quot;Holberton&quot;</code> 줄에서는 <code>bytes</code> 타입의 객체를 생성한 후 <code>b&quot;Holberton&quot;</code>을 메모리 어딘가에 기록하게 됩니다. 아마 heap 이겠죠. 객체나 바이트 배열이 필요한 공간을 예약해야 하니까요.</p>
<h2 id="pythonread_write_heappy">Python 스크립트에 대해 <code>read_write_heap.py</code> 돌려보기</h2>
<p><em>참고: <code>read_write_heap.py</code> 파일은 <a href="https://tech.ssut.me/2017/08/04/hack-the-virtual-memory-c-strings-and-proc/">1. C strings &amp; /proc</a> 글에서 우리가 작성했던 스크립트입니다.</em></p>
<p>위에서 작성한 코드를 실행해보고 <code>read_write_heap.py</code> 스크립트를 실행해봅시다:</p>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ ./main.py 
b'Holberton'
</code></pre>
<p>이 때 <code>main.py</code>는 사용자가 <code>Enter</code> 키를 누를 때까지 기다리고 있습니다. 스크립트의 가장 첫 줄인 <code>sys.stdin.read(1)</code>에서 기다리고 있네요. 이제 <code>read_write_heap.py</code> 파일을 실행해봅시다:</p>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ ps aux | grep main.py | grep -v grep
julien     3929  0.0  0.7  31412  7848 pts/0    S+   15:10   0:00 python3 ./main.py
julien@holberton:~/holberton/w/hackthevm1$ sudo ./read_write_heap.py 3929 Holberton &quot;~ Betty ~&quot;
[*] maps: /proc/3929/maps
[*] mem: /proc/3929/mem
[*] Found [heap]:
    pathname = [heap]
    addresses = 022dc000-023c6000
    permisions = rw-p
    offset = 00000000
    inode = 0
    Addr start [22dc000] | end [23c6000]
[*] Found 'Holberton' at 8e192
[*] Writing '~ Betty ~' at 236a192
julien@holberton:~/holberton/w/hackthevm1$
</code></pre>
<p>참 쉽죠? 예상했듯 heap에서 해당 문자열을 찾아 바꾸었습니다. 자 이제 <code>Enter</code> 키를 치면 <code>b'~ Betty ~'</code>가 출력되겠죠:</p>
<pre><code class="language-sh">
b'Holberton'
julien@holberton:~/holberton/w/hackthevm1$ 
</code></pre>
<p>잠깐? 이게 왜이럴까요?!</p>
<p><img src="https://tech.ssut.me/content/images/2017/08/giphy-4.gif" alt="가상 메모리 파헤치기: 2. Python bytes"></p>
<p>위에서 우리는 분명히 &quot;Holberton&quot; 문자열을 찾아 바꿨습니다, 그런데 왜 안바뀌어있죠?</p>
<p>함정에 빠지기 전에 한 가지 더 체크해봐야 할 것이 있습니다. 우리가 짠 스크립트는 문자열을 처음 찾았을 때 멈추게 되어있습니다. heap에서 같은 문자열이 몇 번 더 출현할 수도 있으니 더 돌려보자고요.</p>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ ./main.py 
b'Holberton'

</code></pre>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ ps aux | grep main.py | grep -v grep
julien     4051  0.1  0.7  31412  7832 pts/0    S+   15:53   0:00 python3 ./main.py
julien@holberton:~/holberton/w/hackthevm1$ sudo ./read_write_heap.py 4051 Holberton &quot;~ Betty ~&quot;
[*] maps: /proc/4051/maps
[*] mem: /proc/4051/mem
[*] Found [heap]:
    pathname = [heap]
    addresses = 00bf4000-00cde000
    permisions = rw-p
    offset = 00000000
    inode = 0
    Addr start [bf4000] | end [cde000]
[*] Found 'Holberton' at 8e162
[*] Writing '~ Betty ~' at c82162
julien@holberton:~/holberton/w/hackthevm1$ sudo ./read_write_heap.py 4051 Holberton &quot;~ Betty ~&quot;
[*] maps: /proc/4051/maps
[*] mem: /proc/4051/mem
[*] Found [heap]:
    pathname = [heap]
    addresses = 00bf4000-00cde000
    permisions = rw-p
    offset = 00000000
    inode = 0
    Addr start [bf4000] | end [cde000]
Can't find 'Holberton'
julien@holberton:~/holberton/w/hackthevm1$ 
</code></pre>
<p>한 번 밖에 없네요. 그럼 스크립트에 분명히 넣은 &quot;Holberton&quot; 문자열은 어디에 있는 걸까요? Python bytes는 대체 어디에 기록되는 걸까요? stack에 있는 걸까요? &quot;[heap]&quot;을 &quot;stack&quot;으로 바꿔 <code>read_write_stack.py</code> 파일을 만들어 다시 돌려봅시다:</p>
<pre><code class="language-python">#!/usr/bin/env python3
'''
Locates and replaces the first occurrence of a string in the stack
of a process

Usage: ./read_write_stack.py PID search_string replace_by_string
Where:
- PID is the pid of the target process
- search_string is the ASCII string you are looking to overwrite
- replace_by_string is the ASCII string you want to replace
search_string with
'''

import sys

def print_usage_and_exit():
    print('Usage: {} pid search write'.format(sys.argv[0]))
    sys.exit(1)

# check usage
if len(sys.argv) != 4:
    print_usage_and_exit()

# get the pid from args
pid = int(sys.argv[1])
if pid &lt;= 0:
    print_usage_and_exit()
search_string = str(sys.argv[2])
if search_string  == &quot;&quot;:
    print_usage_and_exit()
write_string = str(sys.argv[3])
if search_string  == &quot;&quot;:
    print_usage_and_exit()

# open the maps and mem files of the process
maps_filename = &quot;/proc/{}/maps&quot;.format(pid)
print(&quot;[*] maps: {}&quot;.format(maps_filename))
mem_filename = &quot;/proc/{}/mem&quot;.format(pid)
print(&quot;[*] mem: {}&quot;.format(mem_filename))

# try opening the maps file
try:
    maps_file = open('/proc/{}/maps'.format(pid), 'r')
except IOError as e:
    print(&quot;[ERROR] Can not open file {}:&quot;.format(maps_filename))
    print(&quot;        I/O error({}): {}&quot;.format(e.errno, e.strerror))
    sys.exit(1)

for line in maps_file:
    sline = line.split(' ')
    # check if we found the stack
    if sline[-1][:-1] != &quot;[stack]&quot;:
        continue
    print(&quot;[*] Found [stack]:&quot;)

    # parse line
    addr = sline[0]
    perm = sline[1]
    offset = sline[2]
    device = sline[3]
    inode = sline[4]
    pathname = sline[-1][:-1]
    print(&quot;\tpathname = {}&quot;.format(pathname))
    print(&quot;\taddresses = {}&quot;.format(addr))
    print(&quot;\tpermisions = {}&quot;.format(perm))
    print(&quot;\toffset = {}&quot;.format(offset))
    print(&quot;\tinode = {}&quot;.format(inode))

    # check if there is read and write permission
    if perm[0] != 'r' or perm[1] != 'w':
        print(&quot;[*] {} does not have read/write permission&quot;.format(pathname))
        maps_file.close()
        exit(0)

    # get start and end of the stack in the virtual memory
    addr = addr.split(&quot;-&quot;)
    if len(addr) != 2: # never trust anyone, not even your OS :)
        print(&quot;[*] Wrong addr format&quot;)
        maps_file.close()
        exit(1)
    addr_start = int(addr[0], 16)
    addr_end = int(addr[1], 16)
    print(&quot;\tAddr start [{:x}] | end [{:x}]&quot;.format(addr_start, addr_end))

    # open and read mem
    try:
        mem_file = open(mem_filename, 'rb+')
    except IOError as e:
        print(&quot;[ERROR] Can not open file {}:&quot;.format(mem_filename))
        print(&quot;        I/O error({}): {}&quot;.format(e.errno, e.strerror))
        maps_file.close()
        exit(1)

    # read stack
    mem_file.seek(addr_start)
    stack = mem_file.read(addr_end - addr_start)

    # find string
    try:
        i = stack.index(bytes(search_string, &quot;ASCII&quot;))
    except Exception:
        print(&quot;Can't find '{}'&quot;.format(search_string))
        maps_file.close()
        mem_file.close()
        exit(0)
    print(&quot;[*] Found '{}' at {:x}&quot;.format(search_string, i))

    # write the new stringprint(&quot;[*] Writing '{}' at {:x}&quot;.format(write_string, addr_start + i))
    mem_file.seek(addr_start + i)
    mem_file.write(bytes(write_string, &quot;ASCII&quot;))

    # close filesmaps_file.close()
    mem_file.close()

    # there is only one stack in our example
    break
</code></pre>
<p>위 스크립트(<code>read_write_stack.py</code>)는 전에 짰던 스크립트(<code>read_write_heap.py</code>)와 같은 방식으로 완전히 동일한 로직으로 돌아갑니다. heap 대신에 stack을 한 번 확인해봅시다:</p>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ ./main.py
b'Holberton'

</code></pre>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ ps aux | grep main.py | grep -v grep
julien     4124  0.2  0.7  31412  7848 pts/0    S+   16:10   0:00 python3 ./main.py
julien@holberton:~/holberton/w/hackthevm1$ sudo ./read_write_stack.py 4124 Holberton &quot;~ Betty ~&quot;
[sudo] password for julien: 
[*] maps: /proc/4124/maps
[*] mem: /proc/4124/mem
[*] Found [stack]:
    pathname = [stack]
    addresses = 7fff2997e000-7fff2999f000
    permisions = rw-p
    offset = 00000000
    inode = 0
    Addr start [7fff2997e000] | end [7fff2999f000]
Can't find 'Holberton'
julien@holberton:~/holberton/w/hackthevm1$ 
</code></pre>
<p>아, stack에도 없네요: 그럼 대체 어딨는 걸까요? 이제 Python 3 내부 동작을 확인해볼 차례입니다. 지금부터 재밌어지겠네요.</p>
<h2 id="">가상 메모리에서 문자열 찾기</h2>
<p><em>참고: Python 3의 구현은 굉장히 많다는 것을 알아두시기 바랍니다. 이 글에서 우리는 가장 많이 사용되는 (C로 짜여진)CPython 구현체를 사용합니다. 여기서 이야기하는 모든 내용은 이 구현체에만 유효할 것입니다.</em></p>
<h3 id="id">id</h3>
<p>객체(object)가 가상 메모리의 어디에 있는지 찾는 가장 쉬운 방법이 존재합니다. (주의: <em>object</em>는 <em>string</em>이 아닙니다) CPython은 <a href="https://docs.python.org/3.4/library/functions.html#id">id()</a> 라는 내장 함수를 구현해두고 있습니다: <code>id()</code>는 메모리에서의 객체 주소를 반환합니다.</p>
<p>Python 스크립트에서 id를 출력하는 줄을 추가하면 주소를 가져올 수 있겠죠(<code>main_id.py</code>):</p>
<pre><code class="language-python">#!/usr/bin/env python3
'''
Prints:
- the address of the bytes object
- a b&quot;string&quot; (bytes object)
reads a char from stdin
and prints the same (or not :)) string again
'''

import sys

s = b&quot;Holberton&quot;
print(hex(id(s)))
print(s)
sys.stdin.read(1)
print(s)
</code></pre>
<pre><code class="language-sh">julien@holberton:~/holberton/w/hackthevm1$ ./main_id.py
0x7f343f010210
b'Holberton'

</code></pre>
<p>-&gt; <code>0x7f343f010210</code>. <code>/proc</code>를 살펴 실제 객체가 어디에 있는지 살펴봅시다.</p>
<pre><code class="language-sh">julien@holberton:/usr/include/python3.4$ ps aux | grep main_id.py | grep -v grep
julien     4344  0.0  0.7  31412  7856 pts/0    S+   16:53   0:00 python3 ./main_id.py
julien@holberton:/usr/include/python3.4$ cat /proc/4344/maps
00400000-006fa000 r-xp 00000000 08:01 655561                             /usr/bin/python3.4
008f9000-008fa000 r--p 002f9000 08:01 655561                             /usr/bin/python3.4
008fa000-00986000 rw-p 002fa000 08:01 655561                             /usr/bin/python3.4
00986000-009a2000 rw-p 00000000 00:00 0 
021ba000-022a4000 rw-p 00000000 00:00 0                                  [heap]
7f343d797000-7f343de79000 r--p 00000000 08:01 663747                     /usr/lib/locale/locale-archive
7f343de79000-7f343df7e000 r-xp 00000000 08:01 136303                     /lib/x86_64-linux-gnu/libm-2.19.so
7f343df7e000-7f343e17d000 ---p 00105000 08:01 136303                     /lib/x86_64-linux-gnu/libm-2.19.so
7f343e17d000-7f343e17e000 r--p 00104000 08:01 136303                     /lib/x86_64-linux-gnu/libm-2.19.so
7f343e17e000-7f343e17f000 rw-p 00105000 08:01 136303                     /lib/x86_64-linux-gnu/libm-2.19.so
7f343e17f000-7f343e197000 r-xp 00000000 08:01 136416                     /lib/x86_64-linux-gnu/libz.so.1.2.8
7f343e197000-7f343e396000 ---p 00018000 08:01 136416                     /lib/x86_64-linux-gnu/libz.so.1.2.8
7f343e396000-7f343e397000 r--p 00017000 08:01 136416                     /lib/x86_64-linux-gnu/libz.so.1.2.8
7f343e397000-7f343e398000 rw-p 00018000 08:01 136416                     /lib/x86_64-linux-gnu/libz.so.1.2.8
7f343e398000-7f343e3bf000 r-xp 00000000 08:01 136275                     /lib/x86_64-linux-gnu/libexpat.so.1.6.0
7f343e3bf000-7f343e5bf000 ---p 00027000 08:01 136275                     /lib/x86_64-linux-gnu/libexpat.so.1.6.0
7f343e5bf000-7f343e5c1000 r--p 00027000 08:01 136275                     /lib/x86_64-linux-gnu/libexpat.so.1.6.0
7f343e5c1000-7f343e5c2000 rw-p 00029000 08:01 136275                     /lib/x86_64-linux-gnu/libexpat.so.1.6.0
7f343e5c2000-7f343e5c4000 r-xp 00000000 08:01 136408                     /lib/x86_64-linux-gnu/libutil-2.19.so
7f343e5c4000-7f343e7c3000 ---p 00002000 08:01 136408                     /lib/x86_64-linux-gnu/libutil-2.19.so
7f343e7c3000-7f343e7c4000 r--p 00001000 08:01 136408                     /lib/x86_64-linux-gnu/libutil-2.19.so
7f343e7c4000-7f343e7c5000 rw-p 00002000 08:01 136408                     /lib/x86_64-linux-gnu/libutil-2.19.so
7f343e7c5000-7f343e7c8000 r-xp 00000000 08:01 136270                     /lib/x86_64-linux-gnu/libdl-2.19.so
7f343e7c8000-7f343e9c7000 ---p 00003000 08:01 136270                     /lib/x86_64-linux-gnu/libdl-2.19.so
7f343e9c7000-7f343e9c8000 r--p 00002000 08:01 136270                     /lib/x86_64-linux-gnu/libdl-2.19.so
7f343e9c8000-7f343e9c9000 rw-p 00003000 08:01 136270                     /lib/x86_64-linux-gnu/libdl-2.19.so
7f343e9c9000-7f343eb83000 r-xp 00000000 08:01 136253                     /lib/x86_64-linux-gnu/libc-2.19.so
7f343eb83000-7f343ed83000 ---p 001ba000 08:01 136253                     /lib/x86_64-linux-gnu/libc-2.19.so
7f343ed83000-7f343ed87000 r--p 001ba000 08:01 136253                     /lib/x86_64-linux-gnu/libc-2.19.so
7f343ed87000-7f343ed89000 rw-p 001be000 08:01 136253                     /lib/x86_64-linux-gnu/libc-2.19.so
7f343ed89000-7f343ed8e000 rw-p 00000000 00:00 0 
7f343ed8e000-7f343eda7000 r-xp 00000000 08:01 136373                     /lib/x86_64-linux-gnu/libpthread-2.19.so
7f343eda7000-7f343efa6000 ---p 00019000 08:01 136373                     /lib/x86_64-linux-gnu/libpthread-2.19.so
7f343efa6000-7f343efa7000 r--p 00018000 08:01 136373                     /lib/x86_64-linux-gnu/libpthread-2.19.so
7f343efa7000-7f343efa8000 rw-p 00019000 08:01 136373                     /lib/x86_64-linux-gnu/libpthread-2.19.so
7f343efa8000-7f343efac000 rw-p 00000000 00:00 0 
7f343efac000-7f343efcf000 r-xp 00000000 08:01 136229                     /lib/x86_64-linux-gnu/ld-2.19.so
7f343f000000-7f343f1b6000 rw-p 00000000 00:00 0 
7f343f1c5000-7f343f1cc000 r--s 00000000 08:01 918462                     /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache
7f343f1cc000-7f343f1ce000 rw-p 00000000 00:00 0 
7f343f1ce000-7f343f1cf000 r--p 00022000 08:01 136229                     /lib/x86_64-linux-gnu/ld-2.19.so
7f343f1cf000-7f343f1d0000 rw-p 00023000 08:01 136229                     /lib/x86_64-linux-gnu/ld-2.19.so
7f343f1d0000-7f343f1d1000 rw-p 00000000 00:00 0 
7ffccf1fd000-7ffccf21e000 rw-p 00000000 00:00 0                          [stack]
7ffccf23c000-7ffccf23e000 r--p 00000000 00:00 0                          [vvar]
7ffccf23e000-7ffccf240000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
julien@holberton:/usr/include/python3.4$ 
</code></pre>
<p>우리가 만든 객체는 <code>7f343f000000-7f343f1b6000 rw-p 00000000 00:00 0</code> 메모리 공간에 존재합니다, heap도 stack도 아니네요. 우리가 앞서 보았던 결과가 진짜라는 것을 확인해줬습니다. 하지만 이게 문자열 자체가 같은 메모리 영역에 들어가 있다는 것을 의미하지는 않습니다. 예로, <code>bytes</code> 객체는 문자열을 가리키는 포인터를 가질 수도 있습니다, 그